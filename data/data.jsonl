{"text": "本課程將在短短一小時內，快速綜覽生成式AI的技術進展及其未來趨勢。", "label": 0}
{"text": "本課程將快速剖析生成式AI的技術進展與未來趨勢。", "label": 0}
{"text": "本課程將快速綜覽生成式AI的技術進展與未來趨勢。", "label": 0}
{"text": "這一堂課呢是，一堂課搞懂生成式人工智慧的技術突破與未來發展，那我現在呢打算用一堂課的時間，很快地帶大家看過生成式人工智慧近年來發展的現況以及未來大家可以關注的技術", "label": 1}
{"text": "不懂大型語言模型？沒關係！先看看《生成式AI導論2024》，最好看完全部，實在不行，至少看完第8講就好。這系列很輕鬆，利用碎片時間，例如吃飯、運動、通勤時，就能看完。", "label": 0}
{"text": "不懂大型語言模型？沒關係！先看看《生成式AI導論2024》，最好全部看完，不行的話至少看第8講，內容很輕鬆，利用吃飯、運動、通勤時間就能看完。", "label": 0}
{"text": "不懂大型語言模型？沒關係！先看看《生成式AI導論2024》，最好全部看完，不行的話至少看完第8講就好。這系列很輕鬆，利用吃飯、運動、通勤時間就能看完。", "label": 0}
{"text": "假設你對大型語言模型一無所知，不知道它們是怎麼被訓練出來的，你可以先看《生成式AI導論2024》那能夠全部看完最好，那如果沒有辦法的話，希望至少可以看到第8講，那這個系列呢內容是非常輕鬆的，你就用你吃飯啊、運動啊、通勤的時間看一下子就看完了", "label": 1}
{"text": "首先，我們將探討現有生成式AI的能力與應用；其次，深入剖析其底層機制；然後，探究這些機制的形成過程；最後，說明如何增強這些AI模型的功能。", "label": 0}
{"text": "首先，我們將探討現有生成式AI的應用與能力，接著深入其底層運作原理，剖析其技術來源，並最終闡述如何提升這些AI模型的功能。", "label": 0}
{"text": "首先，我们将探讨现有生成式AI的功能与应用，随后深入其底层机制，并阐述这些机制的形成过程，最终介绍如何增强AI模型的能力。", "label": 0}
{"text": "我們會先從現在的生成式AI，有什麼樣的行為，可以做到什麼樣的事情開始講起，那接下來我們會講它背後運作的機制，然後我們會講這些運作的機制是怎麼產生出來的，最後我們會講怎麼賦予這些人工智慧的模型新的能力", "label": 1}
{"text": "人工智慧的發展日新月異，令人驚嘆，但它並非要取代人類，而是成為人類的得力助手。本課程將帶領各位深入淺出地了解人工智慧的強大之處，以及如何善用它。", "label": 0}
{"text": "人工智慧的發展日新月異，令人驚嘆，但它仍然需要人類的引導和操控，本課程將帶領各位深入淺出地了解人工智慧的技術與應用，消除大家對其取代人類工作的疑慮。", "label": 0}
{"text": "人工智慧的發展日新月異，令人讚嘆，但同學們無需擔心被取代，本課程將帶領大家深入淺出地了解人工智慧的技術與應用，並學習如何有效運用它。", "label": 0}
{"text": "各位同學看到這張圖有沒有覺得很厲害，現在人工智慧什麼都能生，你可能會想哇靠這樣是不是以後都不用我了，放心啦，人工智慧雖然猛，但還是要你來下指令嘛，這場課呢就是要帶你輕鬆看懂這些人工智慧厲害在哪裡", "label": 1}
{"text": "各位同學好，我是李宏毅，歡迎來到機器學習課程。這是Breezy Voice合成的聲音，圖像令人印象深刻，對吧？", "label": 0}
{"text": "各位同學，機器學習課程開始了，我是李宏毅。  這是由Breezy Voice合成的聲音，圖像所示，是不是很令人驚嘆？", "label": 0}
{"text": "各位同學大家好，我是李宏毅，歡迎來到機器學習課程。這是Breezy Voice合成的聲音，圖像令人印象深刻，是不是？", "label": 0}
{"text": "好啊，我們開始來上課吧！我是李宏毅，那現在這門課呢，是機器學習。那Breezy Voice合出來的聲音是這樣子的。各位同學，那看到這張圖啊，有沒有覺得很厲害？", "label": 1}
{"text": "明白了，最後將合成音頻和我的畫面素材提交給Heygen平台，就能生成影片。因此，利用簡報製作數位分身講課是可行的，然而真正的挑戰並非講課環節，而是備課階段，尤其耗時的並非製作簡報，而是構思簡報內容。", "label": 0}
{"text": "好的，最後，將合成音訊和我的畫面素材提交給Heygen，平台即可生成影片。因此，利用投影片製作數位講師是可行的，然而，真正的挑戰並非演講環節，而是備課階段，尤其耗時的並非製作投影片，而是構思投影片內容。", "label": 0}
{"text": "因此，將合成音訊和我的畫面素材提交給Heygen後，就能生成影片。所以，利用簡報製作數位分身授課是可行的，但挑戰並不在授課環節，而在於備課的繁瑣，尤其是構思簡報內容最耗時。", "label": 0}
{"text": "好，那最後把合成出來的聲音加上一些我的畫面丟給Heygen，那這個平台呢，就會產生剛才你看到的影片了。所以有了投影片之後，要生成一個數位人來直接講課，嗯！是有可能的。但是真正的難點並不在講課的環節啊！準備一門課最花時間的，其實是在做投影片上啊！真正花時間的地方並不在製作投影片的過程，而是想投影片的內容。", "label": 1}
{"text": "製作課堂簡報耗時費力，除非是直接向他人索取，否則幾乎所有時間都花在上面，那麼，能否運用人工智慧技術來完成簡報製作呢？如此一來，教師的角色是否將被取代？", "label": 0}
{"text": "製作課堂投影片耗時費力，除非你直接複製別人的，否則幾乎不可能省下時間；那麼，AI 能否直接生成完整的投影片，甚至取代老師呢？", "label": 0}
{"text": "製作簡報非常耗時，除非是直接向他人索取，否則幾乎所有時間都花費於此，那麼AI能否協助製作簡報甚至取代教師呢？", "label": 0}
{"text": "除非你的投影片是直接跟別人借的，不然做一門課的投影片是最花時間的。那我們能不能夠直接讓人工智慧來做投影片呢？如果它可以直接做完整的投影片的話，我們就可以把老師淘汰了。", "label": 1}
{"text": "雖然大部分笑話都令人啼笑皆非，甚至有些低級，但在冗長的文章中，我竟發現了一個值得稱道的、甚至算得上勵志的小故事，這是我唯一欣賞的部分。", "label": 0}
{"text": "雖然他的笑話大多糟糕透頂，令人啼笑皆非，在一萬三千字的內容中，卻也出現了一個值得稱道的勵志小故事，算是唯一令人滿意的橋段。", "label": 0}
{"text": "雖然大部分笑話都乏善可陳，甚至令人啼笑皆非，宛如這個語言模型的水平，但在冗長的篇幅中，我竟發現了一個值得稱道的、甚至能算作勵志小故事的佳作，且僅此一則。", "label": 0}
{"text": "他是荒謬到讓人想笑，那這個語言模型產生出來的笑話都是這個等級的啦。但是在一萬三千字的內容中，其實他還是講出了一個好的笑話。這是他唯一我覺得可以看的笑話。這個笑話是這樣子的，而且不是這個笑話啦，是一個勵志小故事。", "label": 1}
{"text": "擴散模型的原理，如同人生旅程，儘管充滿雜亂與不確定性，但透過持續的努力，逐步克服挑戰，最終能創造出非凡的成果；這令人振奮，也激勵我們積極面對人生。", "label": 0}
{"text": "擴散模型的原理，就像人生的逆向工程：從混亂中抽絲剝繭，最終呈現出清晰的圖像。這不正是我們奮鬥的目標嗎？", "label": 0}
{"text": "擴散模型的原理，如同人生旅程，儘管充滿雜亂與不確定性，但透過持續的努力，終能化解紛亂，呈現出令人驚豔的成果；這不僅展現了人工智慧的精妙，更激勵我們積極面對人生挑戰。", "label": 0}
{"text": "他說擴散模型 (diffusion model) 其實很浪漫，為什麼？因為他告訴我們，就算人生一團亂，全是雜訊 (Noise)，只要一步一步努力去除雜訊 (Noise)，也能拼出美麗的風景。人工智慧都這麼勵志了，我們還能不努力嗎？哇，我從來沒有想過擴散模型 (diffusion model) 背後有這麼勵志的故事，人工智慧實在是太有創意了。", "label": 1}
{"text": "生成式AI課程淺顯易懂地介紹了其技術突破與未來展望，核心概念在於賦予機器創造力，使其能生成文字、圖像、影片及程式碼等，並藉由深度學習技術從大量數據中學習，突破傳統計算機的限制。", "label": 0}
{"text": "生成式AI課程淺顯易懂地介紹了其技術突破與未來展望，並闡述了其核心概念：賦予機器創造力，使其能生成文字、圖像、影片及程式碼等，而非僅限於既定選項的處理。其運作基於深度學習模型，從巨量數據中學習，但課程內容不夠深入，未能清晰闡明其原理。", "label": 0}
{"text": "生成式AI課程淺談其技術革新與未來展望，核心概念在於賦予機器創造力，使其能生成文字、圖像、影片及程式碼等內容，其基於深度學習模型，從巨量數據中學習，然其講解過於籠統，未能清晰闡述其運作機制及重要性。", "label": 0}
{"text": "這門課是生成式人工智慧的技術突破與未來發展，那生成式人工智慧的基本概念是，定義生成式人工智慧，讓機器有想像力，可以產生文字、圖像、影片甚至程式碼，運作原理就是基於深度學習 (DL)，從海量資料中學習這個有講跟沒講一樣，不知道在說什麼，那重要性就是給予機器創造力，讓電腦不再侷限於選擇題，而是能自由發揮。", "label": 1}
{"text": "最近AI領域有哪些令人興奮的進展？例如GPT-4和DALL-E 2的問世，雖然DALL-E 2號稱解析度提升四倍，但缺乏明確的比較基準，此外還有開源的文本生成圖像模型Stable Diffusion，其核心技術包含Transformer、GAN和擴散模型。生成式AI的應用範疇已涵蓋文字生成、圖像合成、音樂創作及程式碼生成等領域。", "label": 0}
{"text": "近期AI領域的重大進展包括GPT-4、DALL-E 2和Stable Diffusion等模型的問世，其中DALL-E 2的解析度提升幅度雖未明確說明基準，但據稱達四倍；這些模型的核心技術涵蓋Transformer、GAN和擴散模型，並已應用於文字生成、圖像合成、音樂創作及程式碼生成等領域。", "label": 0}
{"text": "近期AI領域的技術進展令人矚目，例如GPT-4和DALL-E 2的問世，其中DALL-E 2的解析度提升幅度雖未明確說明基準，但據稱提升了四倍；此外，開源的文本生成圖像模型Stable Diffusion也備受關注。這些模型的核心技術包含Transformer、GAN和擴散模型。生成式AI的應用範圍日益廣泛，涵蓋文字生成、圖像合成、音樂創作以及程式碼生成等領域。", "label": 0}
{"text": "那近期有什麼技術突破呢？有GPT-4，有DALL-E 2，他說DALL-E 2解析度提升四倍，這個提升四倍是相較於誰提升四倍，也沒講清楚，還有Stable Diffusion，開源的文本生成圖像模型，那核心的技術有Transformer，有GAN，還有擴散模型 (Diffusion Model)那生成式AI 人工智慧有什麼樣的應用案例呢？有文字生成、圖像合成、音樂創作，還有程式碼生成。", "label": 1}
{"text": "因此，鑑於內容真實性、濫用、偏見、公平、隱私及高昂成本等當前挑戰，未來發展方向應著重於技術優化以降低成本，並探討多模態與深度理解、人機協作及完善的規範倫理。", "label": 0}
{"text": "總之，內容真實性、濫用、偏見、公平、隱私和高昂成本等問題亟待解決，未來發展方向包括技術優化、成本降低、多模態及深度理解、人機協作，以及完善的倫理規範。", "label": 0}
{"text": "因此，鑑於內容真實性、濫用、偏見、公平性、隱私及高昂成本等當前挑戰，未來發展方向應著重於技術優化以降低成本，並探討多模態與深度理解、人機協作及完善的規範倫理引導。", "label": 0}
{"text": "然後呢，當前的挑戰有內容真偽、濫用問題，偏見、公平性的問題、隱私問題，還有高昂計算成本，未來有什麼可能的發展呢？我們需要技術優化，降低成本，多模態與深度理解，人機協作，還有規範倫理的引導需要被完善", "label": 1}
{"text": "懶惰的老師完全可以依靠AI完成教學：AI能生成流水帳式的投影片，再用AI生成數位人授課，實現全自動化教學。", "label": 0}
{"text": "AI足以輕鬆製作出流水帳式的教學投影片及數位講師，懶惰的老師們甚至可以完全仰賴AI完成教學。", "label": 0}
{"text": "憑藉AI，只需流水帳式的投影片和數位分身，便能輕鬆應付教學，懶惰的老師將不再是夢想。", "label": 0}
{"text": "如果我是一個懶惰的老師，我真的就可以用剛才的投影片上課了，所以今天就是告訴你說，假設只是要上剛才那種流水帳式的課程，完全可以用人工智慧來生成投影片，有投影片以後，再用人工智慧來產生數位人，就不需要人類來上這門課了，完全可以全自動的產生一門課程。", "label": 1}
{"text": "讓我們比較一下人類製作的投影片與AI生成的教材，看看有何差異。", "label": 0}
{"text": "讓我們比較一下人類製作的投影片與AI生成的教材有何差異。", "label": 0}
{"text": "讓我們比較一下人類製作的投影片與AI生成的教材有何差異。", "label": 0}
{"text": "好，接下來是人類做的投影片了。大家來看看人類上的課，跟完全用人工智慧準備的教材有什麼不同，那我們現在來看人類做的投影片吧。", "label": 1}
{"text": "現今部分人工智慧已展現推理能力，其運作方式與以往不同：過去生成式AI僅根據輸入直接輸出答案，但如今如ChatGPT、DeepSeek和Gemini等模型，會模擬思考過程，例如嘗試A方案、驗證結果、再嘗試B方案並驗證，最後選擇最佳方案（例如B方案）並呈現其推理過程，將此過程以框線區隔，以區分推理過程與最終答案。", "label": 0}
{"text": "現今，部分人工智慧展現出類人的推理能力，此過程稱為推理。以往生成式AI僅根據輸入直接輸出答案，但如ChatGPT、DeepSeek和Gemini等模型，則會模擬思考過程，例如嘗試A方案、驗證結果、再嘗試B方案，甚至C方案，最後才給出最佳方案(例如B方案)並以註釋顯示其思考過程。", "label": 0}
{"text": "現今部分人工智慧已展現推理能力，不再僅是輸入輸出，而是模擬人類思考過程，例如ChatGPT、DeepSeek及Gemini等模型，會逐步呈現其解決問題的步驟，包括嘗試不同方法、驗證結果，最後才給出最終答案，並將中間過程以註解形式呈現。", "label": 0}
{"text": "那現在啊，這些人工智慧它還展示出類似思考的能力，那通常我們把這個過程叫做 reasoning ，什麼叫做展示思考的能力呢?過去啊，我們在使用這些生成式人工智慧的時候你給它一個輸入 (Input)，給它一個問題，它就直接給你一個答案，但是現在很多的生成式AI，比如說ChatGPT的 o1, o3、還有DeepSeek，還有 Gemini 的 Flash Thinking ，它們在問一個問題的時候，它都不是直接給，都你在問它一個問題的時候，它不是直接給你答案，而是它會演一個腦內小劇場給你看，它就說我們先試試看 A 解法吧！解完之後自己驗證一下答案，發現，嗯，不對再試一下 B 解法吧！它驗證一下嗯好像是對的， B 解法不錯，但我們也可以嘗試一下 C 解法，嗯，看起來沒有比較好，那把腦內小劇場演完之後才給你答案，所以我們用的就是 B 解法的答案。那通常這些模型在展示它的結果的時候，它會把腦內小劇場放在一個框框內，告訴你說這是它的內心戲，不是真正的答案，然後最後才給你真正的答案。", "label": 1}
{"text": "想了解DeepSeek如何運作？讓我們用一個例子實際操作看看：假設姜子牙和鄧不利多在巔峰狀態下對決，DeepSeek會如何分析？它不會直接給出答案，而是先進行複雜的內部推理過程（長達1500字），逐步分析雙方能力，並在過程中不斷修正思考方向，展現其決策過程的細節。雖然過程冗長，但能讓你深入了解模型的思考邏輯。", "label": 0}
{"text": "想了解DeepSeek如何運作？讓我們用一個例子說明：假設姜子牙與鄧不利多決鬥，DeepSeek會如何分析？它不會直接給出答案，而是先進行一系列複雜的推理過程（長達1500字的「內心小劇場」），展現其思考過程的細節，例如權衡雙方能力、不斷修正判斷等等，最後才得出結論。雖然這個過程冗長，但能幫助我們理解模型的決策邏輯。", "label": 0}
{"text": "覺得DeepSeek的運作過程太抽象？讓我們實際操作看看！試問DeepSeek一個古怪問題：若姜子牙和鄧不利多巔峰對決，誰勝？DeepSeek並非直接作答，而是先以冗長的「內心獨白」(約1500字) 展開推演，分析雙方能力，甚至中途推翻結論，再三思量。這過程展現模型的思考過程，雖然冗長，但可略讀重點。", "label": 0}
{"text": "那如果你覺得這一段聽起來有點抽象的話，那我們就實際用一下 DeepSeek ，告訴你這個腦內小劇場看起來像是什麼樣子的，我們來問 DeepSeek 一個莫名其妙的問題吧！我想到說封神演義裡面的姜子牙是個老人，有法力，鄧不利多《哈利波特》的也是一個老人，也有法力，如果他們處在同一個時空，兩人都處於個人的巔峰狀態，有充足的準備時間，如果他們有理由不得不開打，在公平對決的情況下，你覺得誰會贏呢？我們來看看DeepSeek，覺得這兩個會魔法的老人決鬥的話誰會獲勝，那DeepSeek不會馬上給出答案，它會先產生這些顏色比較淺的文字，這些顏色比較淺的文字就是它的內心小劇場，它開始在內心演說它要如何思考這個問題，它就會說，嗯，這個問題看起來挺有趣的，然後開始想說姜子牙有什麼能力，鄧不利多有什麼能力，它的內心小劇場演得非常長，足足演了1500個字，然後演完這些內心小劇場以後，它才會給你答案，內心小劇場非常長啦，不容易讀啦，這邊就看一些段落就好。比如說你看這邊它已經下了總結了，但下了總結之後，它又好像突然想起來好像有東西沒有考慮到，它就說不過還需要再考慮什麼什麼，這個 DeepSeek 內心小劇場，就是可以讓你看到這些模型的糾結，而因為這個 DeepSeek 內心小劇場實在太長了，懶得管它在講什麼。", "label": 1}
{"text": "我們將DeepSeek的構想交給Claude處理，要求它製作易於理解的可視化圖表。Claude擁有編寫程式、製作可視化圖表和網頁的強大能力，預覽功能更讓我們即時檢視程式執行結果，包含顏色配置等細節都由Claude自動完成。生成的網頁中，左側呈現姜子牙的能力（需注意，十絕陣並非姜子牙所設，而是其敵人咒王一方的陣法），右側則呈現鄧不利多的能力，姜子牙的主要能力為道術和陣法。", "label": 0}
{"text": "於是，我將文本交給Claude，請它協助我們梳理DeepSeek的邏輯架構，並以圖表形式呈現。Claude擁有優秀的程式設計能力和數據可視化技巧，能快速生成網頁，並提供程式執行預覽。結果如右側網頁所示，顏色均由Claude自動套用。網頁左側展示姜子牙的能力，右側則為鄧不利多的能力。值得注意的是，十絕陣並非姜子牙的能力，而是其敵人咒王一方所設。", "label": 0}
{"text": "我們將DeepSeek的構想交由Claude處理，指示其製作易於理解的可視化圖表。Claude精通程式設計、可視化和網頁製作，迅速產出右側所示網頁，並提供程式執行結果預覽。網頁中的顏色皆由Claude自動套用。圖表左側顯示姜子牙的能力（道術和陣法，但需注意十絕陣並非其所設），右側則顯示鄧不利多的能力。", "label": 0}
{"text": "所以直接把他的文字丟給 Claude，叫 Claude 呢來幫我們整理 DeepSeek 的思路，我就跟 Claude 說把以下的內容可視化，畫出人容易看得懂的圖，Claude 呢非常擅長寫程式，畫可視化的圖或者是做網頁，我給他這個指令以後，他就寫出了右邊這個網頁，而且 Claude 是可以做 preview (預覽)，可以預覽這些程式執行的結果，執行出來就是這個樣子，這邊的顏色都是 Claude 自己套上去的啦，左邊是姜子牙的能力，右邊是鄧不利多的能力，姜子牙的主要能力是有道術跟陣法，十絕陣這個我要批評一下，那個十絕陣並不是姜子牙的能力啊，十絕陣是姜子牙的對手咒王那邊的人擺的陣法，所以他不是姜子牙的能力啊！", "label": 1}
{"text": "姜子牙憑藉杏黃旗的高防禦力佔據優勢，然而其打神鞭對非神祇效力有限，這限制了他的攻擊能力；因此，面對不在封神榜上的對手，如鄧不利多，短期內鄧不利多佔優，但姜子牙的持久戰能力更強，最終獲勝機率較高。", "label": 0}
{"text": "姜子牙憑藉杏黃旗的高防禦力佔據優勢，然而其打神鞭的效力僅限於封神榜上有名之人，對非神職人員效果不彰，故而面對非神職人員時處於劣勢。因此，鄧不利多若不在封神榜上，則姜子牙的打神鞭將無效。綜合分析，鄧不利多短期佔優，姜子牙長期佔優，充分準備的姜子牙勝算較大，深度研究結論亦指向姜子牙獲勝。", "label": 0}
{"text": "姜子牙憑藉杏黃旗的高防禦力佔據優勢，然而其打神鞭對非神職人員效力甚微，是其明顯弱點；故鄧不利多若不在封神榜上，姜子牙的武器恐難奏效。短期內鄧不利多佔優，但持久戰姜子牙勝算較大，最終姜子牙獲勝的可能性更高。", "label": 0}
{"text": "那姜子牙有什麼優勢呢？優勢就是他有個杏黃旗，他有很高的防禦力，這個我是同意的。在《封神演義》裡面，應該是沒有什麼攻擊可以突破杏黃旗，那有什麼樣的劣勢呢？劣勢就是他的法寶可能對非神職人員較弱，這個我也是蠻同意的。姜子牙的打神鞭啊，應該是只能打封神榜上有名人。有一次姜子牙遇到一個對手，那個對手蠻弱的，但姜子牙拿打神鞭去打他，打神鞭就被對手搶走了，旁白就說為什麼打神鞭打不了這個人呢？因為那個人不在封神榜上啊！所以打神鞭沒辦法打他。所以如果鄧不利多不在封神榜上的話，看起來打神鞭可能對鄧不利多沒有辦法起作用，右邊是鄧不利多的優勢跟劣勢，然後對決分析結果是：短期戰對鄧不利多有利，長期戰對姜子牙有利，結論是姜子牙在準備充分的時候獲勝機率較高，這個是內心小劇場的內容，好，這個是 Deep Research 真正的答案啦，他就說最後姜子牙可能會贏。", "label": 1}
{"text": "他認為，若鄧不利多一開始就瞬移至伏地魔身旁施咒，或許能扭轉戰局；然而，成敗關鍵在於杏黃旗能否抵禦索命咒。鑑於《封神演義》中杏黃旗無物不擋，甚至能抵禦翻天印，那麼在哈利波特的世界裡，它是否也能抵擋索命咒？這場矛與盾的對決，勝負將取決於此。", "label": 0}
{"text": "他補充道，若鄧不利多一開始就瞬移施咒，或許能扭轉局勢，成敗關鍵在於杏黃旗能否抵禦索命咒；雖然《封神演義》中杏黃旗無堅不摧，但這也只是個推測，哈利波特世界並無絕對防禦，這場對決如同矛與盾的較量，杏黃旗能否承受索命咒的威力才是勝負的決定性因素。", "label": 0}
{"text": "然而，他補充道，若鄧不利多一開始便使用移形換影，近身施放索命咒，或許仍有轉機；成敗關鍵在於杏黃旗能否抵禦索命咒。雖《封神演義》中似無物能抵擋杏黃旗，其甚至能阻擋翻天印，但索命咒的威力究竟如何，仍是未知數，這場「矛與盾」的較量，結局難料。", "label": 0}
{"text": "不過他還多講了一句話，他覺得如果鄧不利多一開始就用移行幻影 (Apparition)，直接近身發動索命咒就有逆轉的可能，但這個逆轉的關鍵取決於杏黃旗能不能夠擋住索命咒，在封神演義裡面應該沒有杏黃旗擋不住的東西，翻天印打誰誰死，但杏黃旗可以擋住翻天印，那在索命咒呢？在哈利波特裡面也沒有能夠擋住的東西，所以這就是一個矛跟盾的對決，不知道索命咒能不能夠突破杏黃旗，這個會是決勝的關鍵。", "label": 1}
{"text": "DeepSeek和ChatGPT o3-mini-high都認為姜子牙勝算較高，儘管o3的推理過程簡短，可能有所省略。", "label": 0}
{"text": "DeepSeek 和 ChatGPT  (o3-mini-high) 都認為姜子牙擊敗鄧不利多的可能性更大，儘管 ChatGPT 的推理過程似乎有所簡化。", "label": 0}
{"text": "DeepSeek和ChatGPT-o3-mini-high都認為姜子牙勝算較高，儘管o3的推演過程似乎有所簡化。", "label": 0}
{"text": "好，這個是 DeepSeek 的答案啦！我一樣問ChatGPT o3-mini-high 這個問題，那他也是會做一下腦內小劇場，不過 o3 的腦內小劇場通常比較短，我有點懷疑他沒有完整呈現腦內小劇場的內容，他呈現給我們的只是摘要而已。呈現完腦內小劇場之後，他也給出了他的答案， o3 一樣覺得姜子牙比較可能贏過鄧不利多，看來兩個蠻聰明的模型都覺得姜子牙比較有勝算。結束，我得到這個問題的答案了。", "label": 1}
{"text": "先前我們與AI互動的方式，多半是提問與獲取答案的單向模式；AI僅負責提供答案，不考慮其影響或正確性。然而，許多任務並非單步可完成，需要循序漸進。例如，若妻子要外出用餐，我需先詢問餐廳選擇，再致電預約；若餐廳客滿，單純的語言模型僅能回報此結果而無法繼續處理。", "label": 0}
{"text": "以往的人工智慧應用模式，多半是基於問答的單向互動，系統僅提供答案而忽略後續影響及答案正確性。然而，許多任務並非單步可完成，需要循序漸進的步驟。例如，若妻子想外出用餐，需先詢問用餐地點，再訂位，若訂位失敗，單純的語言模型將僅回報「沒位子」而停止運作。", "label": 0}
{"text": "先前我們與AI的互動模式，始終停留在提問及獲取單一解答的階段，AI僅負責提供答案，卻不考量答案的影響或正確性。然而，許多任務並非單步可完成，需要循序漸進的步驟。例如，若妻子要求外出用餐，我需先詢問用餐地點，再致電預約，若餐廳客滿，單純的語言模型將僅回報「客滿」而停止運作。", "label": 0}
{"text": "剛才呢，我們使用人工智慧的用法都是，你給他一個問題，他就給你一個答案。那通常今天我們使用人工智慧的時候就是一問一答，對人工智慧來說，答案給出去就給出去了，那這個答案會造成什麼樣的影響？這個答案是不是對的，他也不在乎。但是光是一問一答不能解決所有的問題，有很多的任務往往無法一步完成。需要多個步驟才有辦法完成，舉例來說：我舉一個日常生活中的例子，老婆大人跟我說今天晚上要去外面吃飯，那我可能就要先問說，那要吃什麼呢？老婆大人說吃餐廳 A ，我就打電話去餐廳 A ，打完電話以後發現餐廳 A 說沒位置了。如果我只是一個語言模型的話，我可能就會躺在這裡，然後回報說沒位置了結束。", "label": 1}
{"text": "作為人類，我可不敢只這樣做，不然肯定要被老婆修理！所以，我們得想個辦法完成任務。沒位置？當然要另想辦法，比如上網找餐廳B，徵求老婆同意後再預訂，這樣行不行？那麼，今天這些步驟，AI能直接完成嗎？如果AI能完成這種多步驟任務，我們就稱之為AI Agent，後續課程我會詳細講解。雖然例子很日常，但別小看它，完成此任務需要許多能力，例如AI Agent需具備學習能力，避免重複撥打已滿的餐廳A；也需具備使用工具的能力，例如懂得利用網路搜尋其他餐廳。  所以，模型必須能從經驗中學習，並善用網路資源。", "label": 0}
{"text": "作為人類，我可不想惹老婆生氣，所以得想個辦法完成訂餐廳任務。餐廳A沒位子？沒關係，我們可以用其他方法，例如搜尋其他餐廳，找到餐廳B，徵求老婆同意後再訂位。那麼，AI能不能自動完成這類多步驟任務呢？能的話，我們就稱之為AI Agent，後續課程會深入探討。別小看這個看似簡單的例子，它需要AI具備學習能力（例如記住餐廳A已客滿）、使用工具能力（例如網路搜尋）等多項能力，才能有效解決問題。", "label": 0}
{"text": "作為人類，若只照表操課肯定吃不了兜著走，所以得想辦法完成任務，餐廳客滿該怎麼辦？當然是另尋他法，例如網路搜尋其他餐廳，找到餐廳B後徵求妻子同意並預訂，如此一來，這流程能交給AI自動完成嗎？能勝任多步驟任務的AI，我們稱之為AI Agent，後續課程將深入探討。此例看似日常瑣事，卻暗藏玄機，其完成需要多種能力，例如AI Agent需具備學習能力，避免重複撥打客滿餐廳的電話；更需具備使用工具的能力，例如運用網路搜尋其他餐廳資訊。", "label": 0}
{"text": "但我是一個人類，如果一個人只有這樣做的話，那一定會被老婆大人痛毆，所以我們需要想辦法執行完這個任務，那沒位置了怎麼辦呢？你會想其他的方法，比如說上網搜尋看看有沒有其他類似的餐廳，找到餐廳B，然後問老婆大人說訂餐廳B好嗎？老婆大人說可以，然後就訂餐廳B，今天上述工作我們有沒有辦法直接讓 (AI) 人工智慧來完成呢？如果 (AI) 人工智慧可以執行這種需要多個步驟才能完成的工作，那我們會叫它 AI Agent，那我在往後的課程會再更詳細地講什麼是AI Agent。雖然我舉的例子只是一個非常日常生活中的例子，但不要小看這個任務，你需要具備非常多的能力才有辦法完成這個任務，比如說AI Agent必須要能從經驗中學習，剛才打電話就已經知道餐廳 A 沒位置了，如果不會學習的AI，它就會不斷反覆打電話給餐廳A，那這顯然是沒辦法解決問題的。所以模型要從經驗中學習，知道不要再訂餐廳A，模型要有使用工具的能力，知道自己沒有那麼多餐廳的知識，需要上網搜尋才能知道有哪些可以訂的餐廳，這邊在這個例子裡面AI需要上網搜尋。", "label": 1}
{"text": "因此，我會布置一個作業，讓大家利用AI搜尋網路資訊，並回答助教提問；此外，AI還需具備一定的規劃能力，例如主動與使用者確認餐廳預訂，避免浪費時間；事前仔細確認需求，比事後補救更有效率，但AI也需判斷何時需要確認，何時無需確認，例如不必詢問使用者是否允許網路搜尋；完成日常任務需要AI整合多種技能，我們下堂課將深入探討AI Agent。", "label": 0}
{"text": "因此，我會設計一個作業，讓大家利用AI搜尋網路資訊，再回答助教提問；此外，AI還需具備一定的規劃能力，例如主動與使用者確認餐廳預約是否合適，避免浪費時間與精力。事前仔細確認需求，雖然費時，卻能避免事後徒勞無功。然而，AI也需判斷何時該主動確認，何時不需干擾使用者；若連網路搜尋都需徵詢同意，勢必會造成使用者反感。總之，完成日常任務需仰賴AI的多項技能，我們下節課將深入探討AI Agent。", "label": 0}
{"text": "因此，我會布置一個作業，讓大家利用AI搜尋網路資訊，並回答助教提問；此外，AI還需具備一定的規劃能力，例如主動與使用者確認餐廳預訂，避免浪費時間和精力。事前多花些時間確認使用者需求，遠比事後補救更有效率，但AI也需判斷何時需要確認，何時無需確認；若AI連網路搜尋都需徵詢使用者同意，則使用者必定不悅。總之，完成此類日常任務需要多種技能，我們下堂課將深入探討AI Agent。", "label": 0}
{"text": "那我在作業一會出一個作業，讓大家用AI上網搜尋之後，來回答助教出的問題，那除此之外呢？這個AI還需要具備一定程度的規劃能力，比如說它會主動跟人類確定餐廳訂哪一間才是合適的，因為如果最後訂的餐廳是不合適的，那就力氣就白花了。所以與其最後白花力氣，不如一開始稍微多花一點力氣，直接跟人類確定人類的需求，但是AI也要知道什麼時候需要確認，什麼時候不需要確認？如果AI連上網搜尋這個動作都要問人類說，我可不可以上網搜尋，那人類顯然是會生氣的，要完成這個日常生活中的工作，就要完成這個例子其實是需要很多不同的技能的，那我們在下一堂課會再來詳談 AI Agent。", "label": 1}
{"text": "目前已有部分服務展現初步的AI Agent能力，例如ChatGPT、Gemini和Perplexity皆推出了深度研究功能，可視為AI Agent的早期應用。", "label": 0}
{"text": "目前已有部分服務展現出初步的AI Agent能力，例如ChatGPT、Gemini和Perplexity皆已推出深度研究功能，可視為AI Agent的早期應用。", "label": 0}
{"text": "現階段已有部分服務展現出初步的AI Agent能力，例如ChatGPT、Gemini和Perplexity皆推出了深度研究功能，可視為AI Agent的早期應用。", "label": 0}
{"text": "那講到AI Agent，我想現在也是有一些服務，看起來是初步具備有 AI Agent 的能力，那一個例子就是 Deep Research，現在ChatGPT、Gemini、Perplexity 都有出 Deep Research 的功能。", "label": 1}
{"text": "ChatGPT 的 Deep Research 功能能根據你的提問（例如：中部橫貫公路歷史沿革）進行多階段網路搜尋，並根據搜尋結果不斷提煉新問題，最終產出詳盡的報告，其搜尋過程（例如：先搜尋主支線、再搜尋霧社支線起終點、最後搜尋 2018 年改道工程）展現了 AI Agent 的能力，而非單純一次性搜尋後直接生成答案。", "label": 0}
{"text": "ChatGPT的Deep Research功能能針對你的提問（例如中部橫貫公路歷史沿革）進行深入的網路搜尋，並非單次搜尋後直接撰寫報告，而是透過迭代式搜尋，根據搜尋結果產生新的問題，最終生成詳盡的長篇報告；右側顯示的僅為其搜尋過程的一部分，例如，它會先搜尋主支線，再深入搜尋霧社支線的起點終點及2018年改道工程等細節，展現其AI Agent的自主學習和搜尋能力。", "label": 0}
{"text": "ChatGPT 的 Deep Research 功能能針對你的提問，例如「中部橫貫公路的歷史沿革」，進行深入的網路搜尋並撰寫詳盡報告。它並非單純搜尋一次便完成，而是透過迭代式搜尋，根據搜尋結果產生新的問題，不斷深入探討，最終生成一篇長篇報告；右側展示的僅為其針對「中部橫貫公路歷史沿革」提問時，部分的搜尋過程，例如，它會先搜尋主支線，再針對霧社支線的起點終點、2018年改道工程等細節進行更深入的搜尋，展現其 AI Agent 的能力，藉由不斷調整搜尋方向以獲得更全面的資訊。", "label": 0}
{"text": "Deep Research 的功能就是你問它一個問題，比如說中部橫貫公路的歷史沿革，那如果你在 ChatGPT 的頁面上選下面這個 Deep Research (深入研究)，就會執行 Deep Research，也就是模型會開始上網搜尋，而且他搜尋的時候不是只是搜尋一次得到結果就開始寫報告，而是會隨著搜尋到的內容而產生更多的問題，最後寫出一個長篇大論的報告。那右邊呢是展現了當我問中部橫貫公路歷史沿革的時候，Deep Research的搜尋過程，這只是一部分的過程而已。他會先搜尋中部橫貫公路的主線跟支線，他就學到說中部橫貫公路有宜蘭支線和霧社支線，他再去搜尋霧社支線的起點和終點，發現說2018年有一個改道工程，他再去搜尋2018年中橫的改道工程，所以他會隨著搜尋到的結果不同改變他要搜尋的內容，那這是一種 AI Agent的能力。", "label": 1}
{"text": "生成式AI正朝向操控數位物件發展，雖然實際機械操控仍具挑戰，但已能透過螢幕截圖與指令，間接控制滑鼠鍵盤等介面，其原理是將任務指令與螢幕畫面輸入模型，模型再輸出相應的滑鼠鍵盤操作指令（例如座標位置、按鍵指令），開發者只需撰寫程式將這些指令轉化為實際操作即可，如此便能實現AI自動化操作。", "label": 0}
{"text": "生成式AI正朝向操控數位物件發展，雖然操控真實世界的機械手臂仍具挑戰性，但已能透過螢幕截圖和指令，控制滑鼠與鍵盤等數位介面。其運作方式為：使用者給予任務和螢幕截圖，AI分析後輸出操控滑鼠鍵盤的指令，例如移動滑鼠到特定座標或按下Enter鍵，再由開發者編寫程式將指令轉化為實際操作。", "label": 0}
{"text": "生成式AI正逐步發展出操控數位物件的能力，例如透過分析螢幕截圖和任務指令，生成操控滑鼠鍵盤的程式碼，實現自動化操作。", "label": 0}
{"text": "另外一個看起來像是 AI Agent的能力，就是 Claude 的 Computer Use 或者是 ChatGPT 的 Operator，這 Computer Use 或者是 Operator 他們要做的事情就是，現在這些生成式人工智慧不只是生成，它還要能夠操控物件。那可能要操控機械手臂仍然非常的困難，但是現在基本上可以操控，數位世界中能夠碰觸到的滑鼠或者是鍵盤了，那這些生成式AI 怎麼操控滑鼠跟鍵盤呢？這個方法就是你先給它一個任務指示，然後給它一個螢幕截圖，那現在這些模型都是可以看圖的嘛，那看到這個螢幕截圖看到這個指示，它會輸出文字，但它輸出的文字是要如何去操控滑鼠跟鍵盤。比如說它會輸出文字說，把滑鼠移動到螢幕的這個位置，這個是指螢幕上位置的座標。這邊就需要開發者真的去寫一個小程式，把滑鼠移動到指定的位置，那滑鼠移動到指定的位置以後，螢幕截圖看起來可能就不一樣了，螢幕截圖不一樣之後就會產生新的輸出。比如說語言模型、生成式人工智慧，會說按下 Enter，那就寫一個小程式。", "label": 1}
{"text": "讓我們看看ChatGPT Operator如何應付更複雜的挑戰，以下將進行實際操作示範。", "label": 0}
{"text": "讓我們看看ChatGPT的Operator功能如何應付更複雜的任務。", "label": 0}
{"text": "我們將透過一系列操作演示ChatGPT Operator處理複雜任務的能力。", "label": 0}
{"text": "那期待透過這一連串的操作，可以完成一些比較複雜的任務，那我們這邊就實際展示一下，ChatGPT的Operator的能力吧！", "label": 1}
{"text": "請幫我完成大三上李宏毅老師機器學習課程的加簽，並提交台大課程網的加簽表單。", "label": 0}
{"text": "請協助我完成臺大課程網李宏毅老師本學期機器學習課程的加簽申請。", "label": 0}
{"text": "麻煩您幫我向臺大課程網提交李宏毅老師本學期機器學習課程的加簽申請。", "label": 0}
{"text": "我這邊給它的指令是，我想加簽李宏毅老師的機器學習，請上臺大課程網找到本學期這門課的加簽表單，並幫我填寫後送出。", "label": 1}
{"text": "於是，他停了下來。他找到加簽表單，但我要求他填寫並提交，然而，由於加簽表單需要Gmail帳號，他無法完成。", "label": 0}
{"text": "於是，他止步於此。他找到了加簽表單，但我要求他填寫並提交，然而表單需要Gmail帳號，因此他無法完成。", "label": 0}
{"text": "於是，他停下了腳步。他找到了加簽表單，但我要求他填寫並提交，然而表單需要Gmail帳號，因此他無法完成。", "label": 0}
{"text": "好，他就停在這裡。他找到了加簽表單，雖然我要求他填寫後送出，不過填加簽表單是要Gmail帳號的啦，所以他沒辦法填。", "label": 1}
{"text": "因此，我建議他自行申請Gmail帳號，但他拒絕，流程便中斷。值得注意的是，他最初搜尋課程資訊時，先點擊網頁頂端的連結，卻未找到所需資訊，之後才在下方找到課程說明及加簽表單。過程中，他雖曾犯錯，卻能意識到錯誤並修正，展現了AI應具備的學習和適應能力。", "label": 0}
{"text": "因此，我建議他自行申請 Gmail 帳號，但他拒絕，流程便中斷。值得注意的是，他一開始搜尋課程資訊時，先點擊網頁上方的課程資訊連結，卻未找到相關內容，於是才往下搜尋到課程說明及加簽表單。過程中他確實犯錯，但能察覺並修正錯誤，避免重蹈覆轍，這正是當前AI技術的展現。", "label": 0}
{"text": "於是，我建議他自行申請Gmail帳號填寫，但他拒絕了，事情便就此擱置。值得注意的是，他最初搜尋課程資訊時，先點擊網頁頂端的「課程資訊」選項，卻未找到所需資料，才向下搜尋到課程說明及加簽表單。過程中，他雖然一度失誤，卻能察覺並修正錯誤，避免重蹈覆轍，這正是當前AI技術的展現。", "label": 0}
{"text": "那我叫他自己去申請一個 Gmail 帳號來填，他不肯做，所以就停在這裡。好，那剛才你有沒有注意到一件事，一開始在找課程資訊的時候，Operator 是先到課程網頁的上方，去點了課程資訊這幾個字，但發現沒有找到東西。所以接下來他才去下面找到課程的說明，然後找到這一個加簽的表單。所以在中間的過程中，他是一度犯錯的，但是他可以知道說他犯了錯，他可以修正他的計畫，修正他的行為，不再犯同樣的錯誤，這個就是今天AI可以做到的事情。", "label": 1}
{"text": "開發機器學習模型如同打造 AI 代理的原型，過程複雜且耗時。", "label": 0}
{"text": "開發機器學習模型如同打造 AI 代理的原型，過程複雜且分階段進行。", "label": 0}
{"text": "開發機器學習模型如同打造AI代理的原型，過程複雜且耗時。", "label": 0}
{"text": "這就是一個 AI Agent 的雛形，開發機器學習 (ML) 模型也需要非常多的步驟，它也是一個需要多步驟才能完成的任務。", "label": 1}
{"text": "以往我們的機器學習作業流程如下：老師佈置作業，提供任務說明和數據，學生編寫訓練模型程式並執行，反覆除錯、調整模型參數，在開發集上評估模型準確率（例如從50%提升到75%），直至達到滿意程度後提交結果。", "label": 0}
{"text": "以往我們的機器學習作業流程為：教師佈署作業，提供任務說明與數據集，學生據此撰寫模型訓練程式，反覆除錯、訓練、評估（例如計算開發集準確率），直至達到滿意結果（例如準確率達75%）後提交。", "label": 0}
{"text": "以往我們的機器學習作業流程如下：教師佈置作業，提供任務說明和數據集，學生編寫訓練模型程式並執行，反覆除錯、調整模型參數，直至開發集準確率達到滿意程度（例如75%）後提交結果。", "label": 0}
{"text": "過去在我們這個課堂上，一個機器學習 (ML) 模型的作業通常是長這樣子的。老師告訴大家說有一個作業一，有任務的說明，這邊是這個任務的資料，然後呢，你就會根據說明跟資料寫訓練模型的程式，然後你就開始執行你的程式。那你可能很難一開始就寫對，開始訓練之後出現一個錯誤訊息，你就根據錯誤訊息去做一下Debug (除錯)。然後呢，你就可以開始訓練你的模型，訓練完你在development set上計算一下正確率，算出來50%，還不太滿意，修改一下模型再重新訓練，那做出來75%，滿意了你就可以上傳你的結果，不滿意就繼續修改你的模型。", "label": 1}
{"text": "作業二沿用作業一的訓練模型流程，但改由AI Agent負責模型訓練及優化，不再需要學生親自動手。", "label": 0}
{"text": "作業二沿用作業一的訓練模型步驟，但改由AI Agent負責模型訓練與優化。", "label": 0}
{"text": "作業二沿用作業一的訓練模型步驟，但改由AI Agent負責模型訓練與優化。", "label": 0}
{"text": "那在這一堂課裡面，我們當然也需要訓練模型，也需要走過類似的步驟，但是這一次在作業二，我們由AI Agent來完成。那這一門課的作業二呢？就是過去同一門課的作業一，只是這一次我們要用AI Agent來訓練模型，這一次負責執行跟優化模型訓練，不再是你本人，而是由 AI Agent來執行。", "label": 1}
{"text": "讓我們深入探討其底層原理：這些生成式AI看似簡單，僅僅是輸入與輸出的過程，但其輸入輸出內容卻極其複雜，可以是文字、圖片或聲音。", "label": 0}
{"text": "讓我們深入探討其底層技術：這些生成式AI，無論輸入輸出為何──文字、圖片或聲音等複雜資訊──其運作原理都是基於輸入產生輸出。", "label": 0}
{"text": "讓我們深入探討其內在運作原理：這些生成式AI，簡單來說，接收輸入並產生輸出，但輸入和輸出都可能極其複雜，例如文字、圖像或聲音。", "label": 0}
{"text": "接下來我們來看它背後運作的機制，那這些生成式人工智慧從表面上來看，它做的事情就是有一些輸入，它就有一些輸出，只是這些輸入輸出可以是是很複雜的東西，它可以是一段文字，它可以是一張圖片，它可以是一段聲音。", "label": 1}
{"text": "生成式AI的運作原理，本質上是將輸入X轉換為輸出Y，而X和Y可以是任何複雜的形式，例如長文、圖片或聲音。", "label": 0}
{"text": "生成式AI的本質，是將輸入X轉換為輸出Y，無論X或Y是文字、圖像或聲音等複雜資訊。", "label": 0}
{"text": "生成式AI的工作原理，本質上就是將輸入X轉換為輸出Y，無論X或Y的形態為何，無論是文本、圖像還是聲音，其複雜程度皆可。", "label": 0}
{"text": "那我們可以把生成式人工智慧做的事情簡化為：就是輸入一個x，輸出一個y，只是這邊的x或這邊的y，它可以是很複雜的東西，它可以是一篇長篇大論，它可以是一張圖片，它可以是一段聲音。", "label": 1}
{"text": "長篇大論、圖片、聲音的生成原理相同：它們皆由有限的基本組成單位構成。無論是文字、圖像或聲音（簡稱y），都由各自的基本單位（y1、y2、yi）組成，例如文字的基本單位是文字符號，圖像的基本單位則是像素。", "label": 0}
{"text": "長篇大論、圖片、聲音等看似複雜的事物，其本質都是由有限的基本元素組成，這些元素如同文字的符號、圖片的像素，構成了整體。", "label": 0}
{"text": "長篇大論、圖片、聲音，其底層邏輯竟出奇一致：皆由有限的基本單元構成。無論是文字、圖像還是聲音（統稱為Y），都可分解為Y1、Y2、Yi等基本單元，如文字的方塊字，圖像的像素。", "label": 0}
{"text": "那產生長篇大論、產生圖片、產生聲音，背後它的共同的原理是什麼呢？這些看似非常複雜的東西，其實都是由有限的基本單位所構成的。假設我們把這些東西用y來表示，那這個y可以指一段畫，可以指一張圖片，可以指一段聲音，它背後其實都是由一些基本單位，這邊用y1、y2、yi來表示，它是由基本單位所構成的，對一段文字來說，它的基本單位，就是組成這些文字的符號。比如說在中文裡面，我們可以說一個方塊字就是一個基本單位。對於圖片來說，如果你把圖片放大的話來看，圖片是由一個一個的像素 (pixel) 所組成的。", "label": 1}
{"text": "深入探究聲音訊號的本質，你會發現它是由離散的取樣點組成，每個點都是一個數字，如同以基本單位堆砌而成。這些基本單位的特點在於其有限的選擇範圍，如同漢字數量有限，像素的顏色也同樣有限制。", "label": 0}
{"text": "深入探究聲音訊號的本質，你會發現它是由離散的取樣點組成，每個取樣點都以數字表示。如同文字的符號數量有限，像素的顏色也是有限的，聲音訊號的基本單位也具有有限的選擇性，這些有限的單位構成了聲音訊號的基礎。", "label": 0}
{"text": "放大聲音訊號，你會發現它其實是由許多離散的取樣點組成，每個取樣點都是一個數字。  這些數字，也就是聲音訊號的基本單位，其取值範圍是有限的，就像文字的符號數量有限，像素的顏色也是一樣。", "label": 0}
{"text": "那如果你把一段聲音訊號放大的話來看，它背後是什麼呢？一段聲音訊號放大來看可能長這樣。它是由一個一個取樣點所構成的，而每一個取樣點就是一個數字，所以一段聲音訊號也是由一堆基本單位所構成的。那這些基本單位的特性是什麼呢？這些基本單位的特性就是它的選擇是有限的，雖然符號，雖然文字的符號非常多，但是它是有限的，中文方塊字常用的大概就是4000多個。那對於像素 (pixel) 來說，像素 (pixel) 可能的顏色也是有限的。", "label": 1}
{"text": "雖然取樣點數量理論上無限，但實際儲存時每個點僅用有限位元組表示，其可能性也因此受限，儘管如此，有限單位的組合仍能產生近乎無限的變化。", "label": 0}
{"text": "儘管取樣點數量看似無限，但每個點的儲存空間有限(例如，僅用一個位元組)，導致其可能的取值組合雖然龐大，卻仍然是有限的；有限的單元組合出近乎無限的可能性。", "label": 0}
{"text": "儘管採樣點理論上數量無限，但實際儲存時，受限於位元組大小（例如8位元組只能表示256種可能性），最終可表示的採樣點組合，仍然是有限的；換言之，儘管基本單位有限，其排列組合卻能產生近乎無限的可能性。", "label": 0}
{"text": "那你可能會想說：如果取樣點算個數字來表示，這個數字是無限的。但不要忘了，我們在存每個取樣點的時候，你可能只會用一個 byte (位元組) 來存，一個 byte (位元組) 可以儲存的數字的可能性，它的解析度是有限的，你只能存2的8次方的可能性而已，所以取樣點的可能的變化仍然是有限的。這些基本單位它是有限的，這些基本單位你的選擇是有限的，但透過這些有限的選擇進行，你可以組出近乎無窮的可能。", "label": 1}
{"text": "生成式AI的核心是token，這些基本單位如同積木，能組合出近乎無限的可能性，即使是相同的輸入，也會產生不同的輸出，就像一人兩次發聲、兩人同題作畫或作文，結果從不會完全一致。", "label": 0}
{"text": "生成式AI的核心是token，這些基本單位如同樂高積木，看似有限，卻能組合出無限可能，即使是相同的輸入，也會產生不同的輸出，就像一人兩次發聲、兩人繪畫同一主題、兩人作文同題，結果從未完全一致。", "label": 0}
{"text": "生成式AI處理的最小單位，稱為token，如同語言中字詞般，即使相同的指令，也會產生不同的輸出，因為看似相同的元素，其組合方式近乎無限，就像同一主題的繪畫或作文，從未有兩件作品完全相同。", "label": 0}
{"text": "一個人說兩句一樣的話，他的聲音訊號不會是一模一樣的，或者是兩個人畫同樣主題的圖，兩張圖不會是一模一樣的，出同一個作文的題目，兩個人寫出來的文章不會是一模一樣的，有限的選擇有近乎無窮的可能，這些基本單位現在在生成式AI裡面，常常會把它叫做token，token就是組合這些複雜物件的基本單位，如果去查字典的話，token最常被翻譯成代幣。", "label": 1}
{"text": "在生成式AI中，token是构成文本和代码的基本单元；而在图像和语音生成领域，像素和采样点已不再是最常用的基本单元，更高效的方法正在应用，但限于时间，我们之后再详细探讨。所有事物都可以分解成基本单元吗？", "label": 0}
{"text": "在生成式AI中，token是構建所有內容的基本元素；然而，在圖像和語音生成領域，像素和採樣點已不再是最常用的基本單位，更有效率的表示方法存在，我們將在日後深入探討。那麼，萬物皆由基本單位構成的說法成立嗎？", "label": 0}
{"text": "生成式AI的核心組成元素是token，然而，在圖像和語音生成領域，像素和取樣點已不再是最有效的基礎單位，更精簡的表達方式存在，但基於時間限制，我們將在未來深入探討。所有事物皆由基本單位構成，此觀點是否成立？", "label": 0}
{"text": "不過在生成式AI，生成式人工智慧裡面，token就是組成物件的基本單位。另外這邊再補充一下，其實今天在產生圖片還要產生語音的時候，其實像素 (pixel) 或者是取樣點，已經不是最常被使用的基本單位。有更有效的方法來表達影像跟語音的基本單位，不過因為今天時間有限，我們日後再來討論這個話題，那所有的東西都可以看作是由一些基本單位所構成嗎？", "label": 1}
{"text": "是的，文法樹顯然是由基本單位組成的。", "label": 0}
{"text": "當然，文法樹如同其他樹狀結構，皆由基本單位組成。", "label": 0}
{"text": "當然，文法樹如同其他樹狀結構一樣，是由基本單元組成的。", "label": 0}
{"text": "有人可能會問說，那樹狀結構能夠看作是由一堆基本單位所構成嗎？這邊有一個文法樹，我們可以把一個文法樹看作是由基本單位所構成嗎？可以", "label": 1}
{"text": "文法樹能以括號標記的字串序列呈現，本質上即是由基本文字單元組成的字串，因此樹狀結構可被視為基本單元的序列。", "label": 0}
{"text": "文法樹能以括號標記的文字序列呈現，本質上是基本文字單元token的線性組合，因此樹結構可化簡為物件序列。", "label": 0}
{"text": "文法樹可用括號表示其樹狀結構，化為文字序列，此序列由基本文字單元組成，故樹結構亦可視為基本單元序列。", "label": 0}
{"text": "一棵文法樹你可以把它表示成一個文字的序列，用左括號右括號的方式來表示樹狀的結構，所以一棵文法樹也可以看作是一串文字的序列。文字的序列就是由一堆文字的基本 token 所構成的，所以樹狀結構也可以寫成由基本物件所構成的。", "label": 1}
{"text": "所以，所有事物是否皆由基本組成單位構成？我的答案是肯定的，所有物質本質上都是由原子和分子組成，有限的基本單位組合出無限的可能性。", "label": 0}
{"text": "所以，一切事物都由基本單位組成，這是否成立？是的，我認為可以這麼說，所有物質本質上都是由原子和分子構成的有限組合，卻能產生無限多樣性。", "label": 0}
{"text": "如此推論，所有事物皆由基本單元構成，這或許是成立的；畢竟，原子與分子的有限組合，即可衍生出無限多樣的事物。", "label": 0}
{"text": "那你可能進一步問說，那所有的東西都可以看作是由基本物件所構成的嗎？嗯我可以大膽地跟你說我覺得可以，講一句幹話就是所有的物件都是由原子跟分子所構成的，原子它就是有限的選擇組合出無窮的可能，就相信所有的物件都由基本單位所構成。", "label": 1}
{"text": "因此，如同黃仁勳去年 Computex 上所言，生成式 AI 的核心在於將文字、影像、表格、歌曲、聲音、影片等一切事物分解成稱為「代幣」的 token 單元，其本質上即為數據的最小單位。", "label": 0}
{"text": "因此，正如黃仁勳去年在 Computex 上所解釋的：生成式AI的核心在於將任何事物——文字、圖片、表格、歌曲、聲音、影片等——都轉換成可處理的「代幣」(token)，而這些代幣正是其運作基礎。", "label": 0}
{"text": "因此，黃仁勳去年在 Computex 的說明核心在於：生成式 AI 將一切——文字、影像、表格、歌曲、聲音、影片等——都轉換成可處理的「代幣」(token)，這是其運作基礎。", "label": 0}
{"text": "好，這就是為什麼黃仁勳在去年的 Computex 說：這個token可以是文字，它也可以是影像，也可以是表格，也可以是一首歌，也可以是一段聲音，也可以是影片，萬事萬物都是 token，把萬事萬物拆解成 token，就是生成式 AI 的基本原理，那有的人不知道token是什麼意思，回去查字典發現token是代幣的意思。", "label": 1}
{"text": "原來黃老闆並非指所有東西都能代幣化販售，而是想說明萬物皆由token構成，這正是生成式AI的基石。", "label": 0}
{"text": "原來黃老闆並非指所有東西都能變成可販售的代幣，而是想表達萬物皆由token構成，這正是生成式AI的基石。", "label": 0}
{"text": "原來黃老闆並非指所有東西都能轉化成代幣販售，而是闡述萬物皆由token構成，此乃生成式AI的核心概念。", "label": 0}
{"text": "還以為黃老闆的意思是說，每個東西它都可以變成代幣然後拿來賣給你。它其實不是這個意思，它的意思就是萬事萬物組成都是由token組成，就是生成式 AI 的基本原理。", "label": 1}
{"text": "總之，生成式AI的運作原理是將一系列token轉換成另一系列token，至於轉換過程如何，以及token代表的內容（例如：圖片、文字、音樂等及其組成元素），我們將不再贅述，後續內容將以符號表示，以展現其在不同任務和模式上的廣泛應用。", "label": 0}
{"text": "總之，生成式AI的核心是將一系列token轉換成另一系列token。接下來的教學，我會以抽象符號表示資料，不再贅述其具體形式（例如圖片、文字、音樂等），請自行聯想y、yi可能代表圖片像素、文字單字或音樂取樣點，以此理解其在不同任務與模態上的應用。", "label": 0}
{"text": "總之，生成式AI的核心是將一系列的token轉換成另一系列的token。接下來的教學，我會省略對每個y的具體解釋，請自行聯想y可能代表圖片、文字、歌曲等，而yi則可能指圖片的像素、文字的字元或聲音的取樣點。簡而言之，此技術具有高度的應用彈性和跨模態能力。", "label": 0}
{"text": "好，那所以我們現在知道說，生成式 AI 就是拿一堆的 token去生成另外一堆 token ，那怎麼拿一堆token去生成另外一堆token呢？從現在這頁的影片開始，我不會再特別跟你說這裡的每一個 y 指的是什麼，那你心裡就要想像說這邊這個 y 可能是一張圖片，可能是一段文字，可能是一首歌，而這邊的 y 加一個下標，比如說 y 小 i ，它可能是文章中的一個字，它可能是圖片裡面的一個像素，它可能是聲音中的一個取樣點，所以我這邊用符號來表示，告訴你說這個技術是可以用在各式各樣不同的任務應用跟模態上的。", "label": 1}
{"text": "因此，從x生成y的過程，遵循一個逐步遞增的模式：先根據所有x產生y1，再結合所有x和y1產生y2，以此類推，每次只生成一個y，直到生成完整的y序列。", "label": 0}
{"text": "因此，x 轉換為 y 的過程，遵循一個共通的遞迴機制：每次迭代僅產生一個 y，依序生成 y1、y2、y3…yt，其中 yt 的產生依賴於所有 x 和先前生成的 y1 到 yt-1。", "label": 0}
{"text": "因此，從x生成y的過程，遵循一個逐步遞增的機制：首先根據所有x生成y1，然後基於所有x和y1生成y2，依此類推，每次僅生成一個token，直到產生完整的y序列。", "label": 0}
{"text": "好，那怎麼從 x 產生 y 呢？這背後也可以有一個共同的原理，這邊的策略就是根據固定的次序，每次只產生一個 y ，只產生一個 token 出來，那講的具體一點，就輸入x1, x2, ... , xj，所有的x 的時候，你就產生y1。接下來給所有的 x 跟 y1 ，產生 y2，給 y1 跟 y2，產生 y3，以此類推給所有的x、y1, y2, ... , yt-1 的時候，就產生 yt。", "label": 1}
{"text": "圖片生成何時終止？取決於應用程式，圖片大小既定，所需 token 數量亦然，達到目標數量即可停止。", "label": 0}
{"text": "圖片生成的終止條件取決於應用程式需求：圖片大小既定，所需 token 數量可預測，達到目標數量即可停止生成。", "label": 0}
{"text": "圖片生成的終止條件取決於應用場景：圖片大小固定，所需 token 數量亦固定，達到目標數量即可停止生成。", "label": 0}
{"text": "那產生到什麼時候停止呢？那就取決於你的應用，如果今天是產生圖片的話，你需要產生出來的 token 數目是固定的，因為通常圖片大小是固定的，所以你知道要產生多少個 token，你產生到指定的 token 的數量的時候，就可以停下來。", "label": 1}
{"text": "生成文本时，长度难以预先确定，故需特殊标记“结束”指示生成过程终止，此方法称作自回归生成，如同文字接龙，但并非仅限文字，而是各种 token，当 token 为文字时，即为语言模型。", "label": 0}
{"text": "生成文章時，長度難以預測，故需藉由特殊標記「結束」作為終止訊號，停止生成過程。此方法稱為自迴歸生成，如同文字接龍，但接的並非僅限文字，而是各種不同的標記；若標記為文字，則稱之為語言模型。", "label": 0}
{"text": "生成文章時，長度無法預測，因此需使用特殊標記「結束」訊號模型停止，此方法稱作自迴歸生成，如同文字接龍，但可接不同類型標記，若標記為文字，則稱之為語言模型。", "label": 0}
{"text": "那如果是產生一篇文章，通常你無法事先知道要產生的文章有多長。所以你需要再有一個特別的技巧，準備一個特別的 token叫做「結束」。當今天產生出「結束」這個 token的時候，就代表依序生成的過程結束了，不再產生更多的 token。這個策略啊它有一個專有名詞，叫做 Autoregressive Generation (自迴歸生成)，用比較通俗的講法就是文字接龍，這邊接的不一定是文字，而是各式各樣不同的 token。那所以這邊文字我加了一個引號，那當我們接的 token如果是文字的話，其實就叫做語言模型。", "label": 1}
{"text": "然而，今日我們借用「語言模型」的熱度，實際處理的並非文字，而是語音、影像等其他數據類型，即便這些模型本質上是語音模型或影像模型。", "label": 0}
{"text": "然而，今日我們借用「語言模型」的熱度，實際處理的對象並非僅限文字，無論是語音、圖片，只要能套用此框架，我們便稱之為語言模型，即使其本質分別為語音模型或影像模型。", "label": 0}
{"text": "然而，我們今天借用「語言模型」的熱門話題，實際上處理的對象並非僅限文字，無論是語音、圖片，只要能以模型處理，我們都泛稱為語言模型，儘管它們本質上是語音模型或影像模型。", "label": 0}
{"text": "但實際上今天為了要蹭語言模型 (Language Model) 的熱度，接的東西不是文字,我們也會叫語言模型。比如接的是語音,我們也說這是語音版的語言模型，雖然說它其實就是語音模型,接出來的是圖片，我們也說是語言模型,雖然它其實是影像的模型。", "label": 1}
{"text": "歸根結底，今天所有這種文字接龍遊戲，都只是為了利用語言模型的熱度。", "label": 0}
{"text": "歸根結底，今天所有這種文字接龍遊戲，都只是為了利用語言模型的熱度而強行歸類的。", "label": 0}
{"text": "歸根結底，今天所有這些蹭熱度的語言接龍遊戲，都可被視作語言模型的應用。", "label": 0}
{"text": "反正今天只要是做這種接龍的行為，為了要蹭語言模型的熱度我們都把這些技術歸類為語言模型。", "label": 1}
{"text": "簡而言之，右側步驟的本質相同：輸入若干詞元，輸出一個詞元，從有限選項中選出下一個詞元。", "label": 0}
{"text": "簡而言之，右側步驟的本質相同：皆為基於有限選項的 token 序列預測任務，輸入一系列 token，輸出則為單個 token。", "label": 0}
{"text": "簡而言之，右側步驟的本質相同：接收輸入token序列，並預測下一個token，從有限的選項中擇一輸出。", "label": 0}
{"text": "那右邊這些步驟其實可以簡化成都是一樣的事情，右邊這些步驟其實都是一樣的任務，輸入是一串的 token，輸出就是一個 token，決定下一個 token 是什麼。而 token 的可能性是有限的，所以就是給一串 token 做一個選擇題，決定下一個 token 應該是什麼。", "label": 1}
{"text": "為簡化模型，我們將輸入和輸出token統一表示為z，其中z1到zt-1為輸入，zt為輸出。此做法適用於如語言模型等輸入輸出皆為文字token的場景，因為輸入輸出token在本质上并无差异。", "label": 0}
{"text": "簡而言之，為方便起見，我們將輸入和輸出的token都統一用z表示，因為在諸如一般語言模型的文字輸入輸出場景中，輸入和輸出token並無本質區別，都是文字token。", "label": 0}
{"text": "為簡化表示，我們將輸入(z1至zt-1)與輸出(zt)的序列統一以z表示，此方法適用於諸如一般文字模型等輸入輸出皆為文字token的任務，因其輸入輸出並無本質區別。", "label": 0}
{"text": "講到這邊我決定改變一下符號，我們可以不失一般性地，把給一串 token 產生下一個 token 這件事情，表示成輸入是 z1 到 zt-1，輸出是 zt。那有人可能會問說老師你怎麼可以把 x 跟 y 沒有做區別，都用 z 來表示呢？那麼 x 跟 y 難道不是不同的東西嗎？你想想看，如果今天我們做的是一個輸入文字，輸出文字，像一般的文字模型，一般的語言模型一樣，那樣的模型，如果我們做的是一個輸入文字輸出文字的模型，輸入的所有x 都是文字token，輸出所有y 也都是文字token。它們是沒有本質上的差異的，它們都是文字的token，所以我們可以不區別x 跟y，就用z 來表示。", "label": 1}
{"text": "面對影像輸入、文字輸出的模態差異，解決方案是將影像特徵（例如4096個token）和文字特徵（例如30000個token）合併成一個新的特徵集合（34096個token），我們稱之為Z。", "label": 0}
{"text": "多模態模型的輸入輸出不同，例如影像輸入文字輸出，解決方法是將影像和文字的token集合起來，例如影像4096個token，文字30000個token，合併成新的token集合z，包含34096個token。", "label": 0}
{"text": "面對影像輸入、文字輸出的模態差異，解決方案是將影像和文字的token集合（例如影像4096個，文字30000個）合併成一個新的token集合z，其大小為34096個token。", "label": 0}
{"text": "那你說如果我今天是輸入影像輸出文字，如果輸入跟輸出的模態不一樣怎麼辦呢？那x 跟y 它是不同的token的集合啊，影像有影像的token，文字有文字的token，那就告訴你我們把影像跟文字的token直接集合起來，當作一個新的token的集合。影像可能有4096 個token，文字有30000 個token，我就說一個新的token的集合是30000 加4096，是34096，是34096個 token，它們合起來是一個新的集合就叫做z。", "label": 1}
{"text": "無論輸入輸出為何，皆可簡化為輸入一系列 tokens，輸出單個 token 的模型，涵蓋文字轉文字、圖像生成、語音轉文字等各種應用，其底層機制並無二致。關鍵在於如何根據輸入tokens序列預測下一個token，這需要一個函數f，其輸入為z1到zt-1，輸出為zt，而此函數即為神經網絡。神經網絡實際上並非直接產生token，而是計算每個token的概率分佈，賦予每個token一個得分，代表其成為下一個token的可能性。", "label": 0}
{"text": "無論輸入輸出為何，皆可簡化為輸入一系列token，輸出單個token之模型；文字轉文字、圖像轉文字、語音轉文字等任務，其底層機制並無二致。關鍵在於如何根據輸入token序列預測下一個token，此過程需藉由函數f (以類神經網路實現) 完成，其輸入為z1至zt-1，輸出則為zt；然而，類神經網路實際上並非直接產生token，而是計算每個token的機率分佈，以反映其成為下一個token的可能性。", "label": 0}
{"text": "無論輸入輸出為何，皆可視為輸入一系列 tokens，輸出一個 token，涵蓋文字轉文字、文字轉圖像、圖像轉文字、語音轉文字及文字轉語音等，其底層原理相同。關鍵在於如何根據輸入 tokens 序列預測下一個 token，這需要一個函數 f(z1,...,zt-1) = zt，而此函數即為神經網路。神經網路並非直接產生 token，而是預測每個 token 的機率，根據機率分佈選取最合適的 token 作為輸出。", "label": 0}
{"text": "所以不管是你要輸入什麼、輸出什麼，我們都可以不失一般性地說，就是輸入一串 token，輸出一個 token，所以不管是你要文字生文字，文字生圖、圖生文字、語音生文字、文字生語音都是一樣的事情，它們背後的原理其實是一樣的。那再來要問的問題就是，怎麼輸入一串 token 決定下一個 token 是什麼呢？你需要一個函式 (function)，我們這邊寫作 f ，這個函式 (function) 它的輸入就是 z1 到zt-1，輸出就是zt，那這個函式 (function) 其實就是類神經網路 (neural network)。那我實際上這個 neural network 在運作的時候，並不是真的產生一個 token，它產生的是一個 token 的機率分佈。因為 token 的可能性是有限的，所以類神經網路 (neural network) 真的輸出是，它會給每一個 token 一個分數，代表這個 token 作為下一個 token 有多合適，這個 token 當作下一個 token 的機會有多大。", "label": 1}
{"text": "然而，給定一個詞序列，下一個詞並非總是唯一確定的，例如「臺灣大」之後，可以是「學」、「車」、「哥」等，可能性多元。", "label": 0}
{"text": "然而，一個詞語序列的後續詞語並非單一且確定，例如「臺灣大」之後，既可以是「學」，也可以是「車」或「哥」，可能性多元。", "label": 0}
{"text": "然而，詞彙接龍並非單一解，例如「臺灣大」之後，既可接「學」，也可接「車」或「哥」，可能性多元。", "label": 0}
{"text": "為什麼要這樣做呢？因為你想看看假設我們現在在做文字接龍，我告訴你說輸入的 token 是臺灣大，問你接下來可以接哪一個 token？那各位同學可能會很直覺地接一個學出來，臺灣大可以接學這樣沒有問題，但是臺灣大後面還可以接很多其他東西啊！比如說可以接車，臺灣大車隊。可以接哥，臺灣大哥大。你可以接的東西太多了，所以給一個 token sequence (符記序列)，接下來可以接哪一個 token，往往答案並不是唯一的。", "label": 1}
{"text": "強迫類神經網路給出單一答案會降低其學習效率，甚至導致輸出錯誤百出；因此，我們改採機率分佈機制，讓模型預測每個token的可能性，再以隨機抽樣的方式決定最終輸出，故即使輸入相同，生成式AI每次的輸出結果也可能不同。", "label": 0}
{"text": "強迫類神經網路給出單一答案，反而會降低其學習效率，甚至導致錯誤百出；因此，本設計讓其輸出機率分佈，再以隨機取樣的方式決定最終結果，確保每次產出皆有所不同，即使輸入相同。", "label": 0}
{"text": "強迫類神經網路給出單一答案會降低其學習效率並導致錯誤百出，因此改採機率分佈的方式，讓模型預測每個token出現的可能性，再透過隨機取樣決定最終輸出，如此一來，即使輸入相同，每次生成的結果也會有所不同。", "label": 0}
{"text": "所以你強迫類神經網路 (Neural Network) 輸出唯一的答案，它很有可能反而很難學會，它很有可能反而非常的錯亂。所以這邊的設計是讓它輸出的是一個機率分佈，讓它告訴你它覺得每一個 token 接在後面的可能性有多大，那產生這個機率分佈以後，再按照這個機率分佈去擲一個骰子，決定真正產生出來的 token 是什麼，那就是因為有這個擲骰子的過程。所以今天這些生成式 AI，在產生答案的時候，就算是一樣的輸入，每次產生的輸出也會是不一樣的。", "label": 1}
{"text": "讓我們深入探討類神經網路及其獨特性。雖然坊間充斥著將其比喻為模擬人腦的科普解釋，但我們將更深入地剖析其運作機制。", "label": 0}
{"text": "現在，讓我們深入探討類神經網路及其獨特之處。雖然您可能已接觸過許多關於類神經網路模擬人腦運作方式的常見解釋，但我們將從不同的角度切入。", "label": 0}
{"text": "現在，讓我們深入探討類神經網路及其獨特之處。雖然坊間充斥著類比人腦運作方式的泛泛而談，但我們將更精確地剖析其本質。", "label": 0}
{"text": "那接下來我們來講一下什麼是類神經網路 (Neural Network)，它特別的地方在哪裡？那類神經網路啊，我知道你可能在很多其他地方都聽過一大堆對於類神經網路的解釋，那通常都是說類神經網路，模擬人腦的運作方式......，所以很厲害等等科普文章就是這樣寫的。", "label": 1}
{"text": "神经网络的本质是将一个复杂函数分解成多个简单函数的串行组合，每个简单函数称为一层，通过层层递进的计算，最终输出结果，这种多层结构正是深度学习的“深”的含义，它使得神经网络能够处理更复杂的问题。", "label": 0}
{"text": "神经网络的本质是将一个复杂函数分解成多个简单函数的串联，通过层层递进的计算，最终输出结果，这种将函数分解成多层结构的特点，使其被称为深度学习，层数越多，网络深度越深。", "label": 0}
{"text": "神经网络的本质是将复杂函数分解为多个简单函数的串行组合，每个简单函数即为一层，通过层层递进的计算，最终输出结果，这“深度”正是其区别于传统方法的关键所在，它允许对输入进行逐层抽象，最终得到目标输出的概率分布。", "label": 0}
{"text": "那類神經網路 Neural Network，作為一個函式 function，它真正的特色是什麼呢？它真正的特色是把一個函式 function f，拆解成很多小的函式 function f1 到 fL 的串聯，也就是說這個 f 實際上運作的過程是，有一堆輸入先通過第一個 f1 產生一堆輸出，那這邊的輸出呢？這邊每一個方框呢代表一個向量 (Vector)，然後呢這些輸出會再變成 f2 的輸入，產生新的輸出，這個步驟會持續一直下去，直到 fL 接受到輸入的時候，它會輸出下一個 Token 的機率分佈。這邊每一個小的函式 (function) 又叫做 Layer，所以一個 f 裡面有很多的 Layer，那就是因為把一個函式 (function)，拆解成多個串聯的函式 (function)，拆解成多個 Layer，所以類神經網路 (Neural Network)又叫做深度學習 (Deep Learning, DL)，深指的就是有很多 Layer 的意思。", "label": 1}
{"text": "深度學習模型的關鍵在於將複雜問題分解為多個層級的子問題。", "label": 0}
{"text": "深度學習模型的核心在於其分層結構，將複雜問題分解為多個步驟逐層求解。", "label": 0}
{"text": "深度學習模型的核心在於將問題分解為多個層級的子問題。", "label": 0}
{"text": "類神經網路真正的特色就是，把一個問題拆解成 L 個步驟，這邊 L 個步驟指的是 Layer 層的數目。", "label": 1}
{"text": "我們接下來將探討將單一步驟分解成多個步驟的優勢，一個簡化的例子：想像每個步驟如同一個查找表，輸入值經由每個表（函式）的轉換，最終得到預期輸出。", "label": 0}
{"text": "所以，為何將單一步驟細分成多個步驟能提升效率？不妨想像一下，每個步驟就像一張查找表，每個步驟都像個函數，根據輸入查找並輸出結果，這樣拆解的優勢就在於此。", "label": 0}
{"text": "高效執行任務的關鍵，在於將複雜步驟細化分解，其益處為何？不妨想像每個步驟如同一個函數表，輸入值經由每個函數表查表後，即可得出相對應的輸出值，如此便能闡明其運作機制。", "label": 0}
{"text": "那接下來我想跟大家說明的是，為什麼把一個步驟拆解成多個步驟，會是一件有效有用的事情呢？這邊給一個不精準的比喻，我們假設每一個 layer 它就是一個表格，每一個 layer 作為一個函式，它做的事情就是查表，你給它一個輸入，它每個表格告訴你輸出應該長什麼樣子。", "label": 1}
{"text": "將三位數加法任務分解成兩個步驟，而非一步到位求解，可大幅降低所需儲存的輸入輸出關係數量，從原本的1000組縮減至100+190組，充分展現了化繁為簡的策略優勢。", "label": 0}
{"text": "若要讓單層人工智慧直接計算三位數相加，需儲存所有1000組輸入輸出對應關係；但若將其分解為兩步驟計算，則只需儲存100+190組，大幅降低所需儲存量，展現了問題拆解的優勢。", "label": 0}
{"text": "若要讓單層人工智慧直接計算三個單一位數的和，其所需儲存的輸入輸出對應關係數量將高達一千組；然而，若將此問題分解成兩個步驟計算，則只需儲存一百九十組輸入輸出對應關係，大幅降低所需資源。", "label": 0}
{"text": "那現在假設我們要人工智慧做一個，非常非常簡單的問題，做三個個位數的相加，假設你只有一個 layer 要一步到位產生答案，那這個表格需要存多少輸入輸出的關係呢？想想看有多少可能的輸入，有10 × 10 × 10，1000種可能的輸入，10 10 1000 種可能的輸入，所以這個表格需要存1000 種可能的輸入跟輸出間的關係。但假設我們把這個問題拆解成兩個步驟A、B、C，不需要一次到位，先把A 跟B 加起來變成B'，再去計算B' C，那第一個 layer 要做的事情是計算A、B的答案是多少？那這個時候你的輸入就只有10 × 10種可能，而第二個步驟要計算B' 加C是多少，那這個時候B' 有19個可能，C有10個可能，所以這個時候是有19 × 10個可能的輸入，所以需要存的只有10 × 10加上19 × 10可能的輸入輸出關係，所以你會發現，當我們把一個問題拆解成多個步驟的時候，我們是從複雜的問題變成簡單的問題。", "label": 1}
{"text": "深度學習的精髓並非複雜性本身，而是藉由簡化複雜問題以達到高效能。", "label": 0}
{"text": "深度學習並非如許多人誤以為的那樣複雜，它反而能簡化複雜問題，從而提升效能。", "label": 0}
{"text": "深度學習並非如許多人誤以為的那樣複雜，其核心在於簡化複雜問題，從而提升效能。", "label": 0}
{"text": "所以很多人對於深度學習（DL）有誤解，覺得說深度學習（DL）就是弄一個很複雜的東西。其實不是，深度學習（DL）是把本來複雜的問題變得比較簡單，這就是為什麼深度學習（DL）往往會有好的效果。", "label": 1}
{"text": "好的，上述比喻不夠精確。欲了解深度學習(DL)相較淺層學習在函數運算效率上的數學證明，請參考我之前的深度學習(DL)理論系列影片，連結已附於投影片。", "label": 0}
{"text": "好的，之前的比喻不夠精確。欲了解深度學習(DL)相較於淺層學習在函數運算效率上的數學證明，請參考我之前的深度學習(DL)理論系列影片，連結已附於投影片。", "label": 0}
{"text": "嗯，之前的比喻不夠精確。想了解深度學習比淺層學習更有效的數學證明，請參考我之前的深度學習理論系列影片，連結已附於投影片。", "label": 0}
{"text": "好，那以上是一個不精確的比喻了。如果你想要知道從數學上怎麼說明，給一個函式用深度學習（DL）的方法，會比用淺層學習的方法還要更有效率，從數學上怎麼證明它，請看以前我講過的深度學習（DL）理論系列影片，那我就把連結放在這個投影片上。", "label": 1}
{"text": "分解問題為多個步驟的有效性，同樣適用於闡明機器思考的益處。若不讓機器思考，直接給予問題，則它需立即產出答案，忽略了問題與答案間多個層次的思考步驟，而複雜問題往往需要更多步驟，但機器可處理的步驟數量是有限的。", "label": 0}
{"text": "因此，將複雜問題分解成多個步驟的有效性，同樣適用於機器學習。若不允許機器逐步思考，直接輸入問題要求其立即作答，則問題與答案間的複雜推論過程（可視為多個思考步驟）將無法有效展開，尤其對於困難問題，所需的步驟數量可能超出機器單次處理能力的限制。", "label": 0}
{"text": "將複雜問題分解為步驟有助於解決問題，機器學習亦是如此。若不允許機器逐步思考，直接輸入問題要求其立即作答，則因問題與答案間存在多個步驟（層級），而複雜問題所需的步驟數量往往超出機器能力限制。", "label": 0}
{"text": "那我們現在已經知道把一個問題拆解成多個步驟會有用，同樣的道理也可以拿來說明為什麼讓機器思考會有用。如果我沒有讓機器思考，給它問題，它讀完問題之後，就必須立刻產生答案。那從問題到答案中間有很多個 layer，每個 layer可以想成是一個思考的步驟，但往往困難的問題需要很多很多思考的步驟，layer的數目是有限的。", "label": 1}
{"text": "深度不足，延展思考彌補之，藉由模擬內部思維過程，等同於拓展了神經網絡的深度，讓AI在回應前進行更深入的推理，以解決層數限制的問題。", "label": 0}
{"text": "深度不足，以更長的思考鏈彌補之。", "label": 0}
{"text": "深度不足，可藉由延伸模型的思考過程來彌補，讓AI模擬人類思考，間接增加深度學習的層次。", "label": 0}
{"text": "如果 layer不夠，那要怎麼辦呢？那讓機器思考，讓機器演腦內小劇場，可以想成是從另外一個方向，擴展了類神經網路的深度。所以今天給一個問題，你的這個機器，你的人工智慧，不是馬上產生答案，而是有一個思考的過程，思考完之後才產生答案，這個時候從問題到答案間，變成有非常多的思考的步驟，不再侷限於 layer的數目，而是跟長度思考過程的長度有關，所以這邊的 slogan 就是深度不夠長度來湊。", "label": 1}
{"text": "雖然單個神經網絡的深度有限，但其思維過程的長度卻可以無限延伸。然而，對於熟悉神經網絡運作機制的讀者而言，這種以深度彌補長度的做法可能與傳統神經網絡有所不同，因為每個紅色箭頭中的參數都相同，其有效性值得探討。1999年的ALBERT論文已證明，即使每個層都相同，堆疊多層也能獲得更好的結果。", "label": 0}
{"text": "雖然神經網路的深度受限，但其運算過程的長度卻可以無限延伸。然而，對於熟悉神經網路運作機制的人來說，這種以長度彌補深度不足的方法可能與傳統神經網路有所不同，因為每個箭頭中的參數都相同，其有效性值得探討。19年的ALBERT論文已證實，即使層數相同，堆疊多層也能提升效能。", "label": 0}
{"text": "雖然單個神經網路的深度有限，但其思維過程卻可以無限延伸。然而，這種通過增加相同層數來提升深度的方法，與傳統神經網路有所不同，因為每一層使用的參數相同，其有效性可能令人質疑。但19年的ALBERT論文已證明，即使層與層之間參數相同，堆疊多層也能提升效能。", "label": 0}
{"text": "今天一個類神經網路 (Neural Network) 的深度是有限的，但是思考的過程可以是要多長有多長。而這邊稍微補充一下，有的同學如果很熟悉類神經網路 (Neural Network) 的運作的話，你可能會說這種深度不夠長度來湊的方式，跟一般的類神經網路 (Neural Network) 不一樣啊。因為這邊每一個紅色箭頭裡面的參數，通過的參數都是一樣的啊，這樣會有用嗎？那我推薦你一篇史前時代19年的文章叫做ALBERT，那時候他就是告訴你說，就算是每一個 layer 都是一樣的，把同樣的 layer 疊很多次還是可以有比較好的結果的。", "label": 1}
{"text": "OpenAI在宣傳其o1模型時，以「Testing Time Scaling」一詞包裝了「深度不足，長度補足」的策略，然而其官方部落格中相關資訊卻含糊不清，例如「Testing Time Compute」的單位便未明確說明，令人難以理解其實際運作方式。", "label": 0}
{"text": "OpenAI在其博文中以「測試時間縮放」(Testing Time Scaling) 巧妙地掩蓋了其o1模型以增加長度彌補深度不足的事實，其數據圖表及相關描述含糊不清，缺乏必要的細節說明。", "label": 0}
{"text": "OpenAI 首次提出「測試時間縮放」(Testing Time Scaling) 概念，意指以增加測試時間彌補模型深度不足，然而其官方說明含糊不清，缺乏關鍵細節如計算單位的說明，令人難以理解其實際操作。", "label": 0}
{"text": "那深度不夠長度來湊這件事啊，現在又叫做Testing Time Scaling。那我第一次聽到Testing Time Scaling 這個詞，是在 OpenAI 發表的時候，OpenAI 寫了一個 blog 告訴大家說，o1 厲害的地方就是 Testing Time Scaling (測試時間縮放)，用白話來講就是深度不夠長度來湊。那這是 OpenAI 他們的 blog 裡的附圖啦，不過因為現在 OpenAI 釋出資訊的時候，都釋出的非常的隱晦，像橫軸他說這個叫做 Testing Time Compute，欸，那他單位是什麼也沒講清楚啊，所以你很難知道他實際上做的事情。", "label": 1}
{"text": "Stanford大學一篇名為「Simple Testing-Time Scaling」的研究表明，增加語言模型的思考步數確實能提升準確率：思考步數越多，準確率越高。該研究採用簡陋但有效的方法操控思考長度，即在模型產生結束符號時，將其替換為「wait」，迫使模型繼續生成文本。", "label": 0}
{"text": "Stanford大學一篇名為「Simple Testing-Time Scaling」的研究顯示，增加語言模型的思考步數確實能提升準確率；該研究以粗暴但有效的方式，通過替換終止符號為「wait」來延長模型輸出，強制模型持續生成文本，實驗結果表明思考時間越長，準確率越高。", "label": 0}
{"text": "Stanford大學一篇名為「Simple Testing-Time Scaling」的研究顯示，增加語言模型的思考步數確實能提升準確率；該研究以粗暴的方式，將語言模型生成的結束符號替換為「wait」，迫使模型持續生成文本，從而控制輸出長度，實驗結果表明思考步數與準確率呈正相關。", "label": 0}
{"text": "所以這邊引用的數據呢，是來自於 Stanford 大學的另外一篇 paper，叫做 s1: Simple Testing-Time Scaling。他們就告訴你說長度來湊還真的是有用的，橫軸是思考的時候用了幾個 token，縱軸是在不同任務上的正確率，你發現說想的越長正確率就越高。那講到這邊你可能會想說，那他們是如何操控思考的長度呢？其實這篇論文裡面用的是一個非常粗暴的方法，它的方法就是每一次這個語言模型產生結束的符號的時候，直接把那個符號拿掉，換成wait這個字，有語言模型本來想結束了，突然發現我怎麼說不了結束，這個符號怎麼變成wait，只好繼續做文字接龍，強迫它一直講下去，這個就是他們怎麼控制語言模型輸出長度的方法。", "label": 1}
{"text": "深入探討單一層級的內部結構，發現其並非單純一層，而是由多層子結構組成，實質上是一個由多個函數串聯而成的複合函數。這些子函數主要分為兩類：全域注意力機制 (self-attention) 能夠整合所有輸入信息生成輸出；而另一類則專注於單個輸入元素(token)的深入處理。因此，單層結構同時兼具全局和局部信息處理能力。", "label": 0}
{"text": "深入探討網絡層的內部結構：層級結構複雜，非單一層次，而是由多個子層或子函數組成，形成函數嵌套。這些子層可分為兩種：一種是全域性自注意力層，考慮所有輸入信息；另一種是局部層，針對單個token進行深入處理，兼顧全局與局部信息分析。", "label": 0}
{"text": "深入探討單一層級的內部結構：實際上，層級並非單元，而是由更細微的子層級或函式所組成，形成一個函式鏈。這些子層級大致可分為兩種：一種是全域性自注意力層，考慮所有輸入來產生輸出；另一種則聚焦於單個元素，進行更深入的局部分析，因此，單層級內同時存在著全局和局部的資訊處理機制。", "label": 0}
{"text": "那接下來我們來看一個layer層中又發生了什麼樣的事情，一個layer層裡面現在通常都還有更多的layer層，所以一個layer層它不是一個layer層，它其實裡面還有很多的layer層，所以一個layer層它是一個函式，它其實又是有很多更小的函式所串聯而成的。現在通常一個layer層中的layer層，一個layer層中的函式它還有分成兩類，一種是叫做self-attention layer，這種layer層在產生輸出的時候，是會考慮全部的輸入再去產生輸出，所以你可以用這種方法去考慮輸入的全部的資訊。那也會有一些 layer，它只針對單點做思考，根據一個 token去做更深入的思考，所以今天一個 layer裡面，會有考慮全局的 layer，會有考慮全局的函式，也會有針對單一的 token在深入思考的函式。", "label": 1}
{"text": "我們將在第三堂課和作業三深入探討單一層級的運作機制，此機制即為構成Transformer模型基礎的自注意力機制神經網絡。", "label": 0}
{"text": "課程第三講及作業三將深入探討單一層級的運作細節，而今日所述之自我注意力機制神經網路，即為Transformer模型。", "label": 0}
{"text": "我們將在第三講和作業三深入探討神經網路層級運作的細節，此處所指的神經網路層級，即為構成Transformer架構的自我注意力機制。", "label": 0}
{"text": "在這門課的第三講跟作業三，我們還會在詳細剖析一個 layer中發生了什麼事。其實今天有這種 self attention layer 的類神經網路，又叫做通常被統稱為 Transformer。", "label": 1}
{"text": "事實上，Transformer 架構衍生出許多變體，2017 年發表的原始 Transformer 與現今的 LLaMA、ChatGPT 和 DeepSeek 等模型存在著一些差異。", "label": 0}
{"text": "Transformer 架構衍生出許多變體，2017 年提出的原始 Transformer 與現今的 LLaMA、ChatGPT 和 DeepSeek 等模型仍存在些許差異。", "label": 0}
{"text": "事實上，Transformer 架構衍生出許多變體，2017 年發表的原始 Transformer 與當今的 LLaMA、ChatGPT 和 DeepSeek 等模型存在一些差異。", "label": 0}
{"text": "那其實 Transformer 有很多的變形啦，最原版的 Transformer 2017年發表的那個 Transformer，其實跟今天的 LLaMA、ChatGPT、DeepSeek 其實還是略有差別啦。", "label": 1}
{"text": "現今只要模型包含自注意力機制，便常被簡稱為Transformer，此名稱與變形金剛的巧合，其命名緣由論文並未提及，推測可能因其能轉換輸入至輸出的特性，但此特性並非Transformer獨有，許多模型皆具備此功能。", "label": 0}
{"text": "現今只要模型包含自注意力機制，便常被簡稱為Transformer，這名稱的由來眾說紛紜，有人推測是因其能轉換輸入為輸出，但此解釋過於籠統，難以令人信服。", "label": 0}
{"text": "現今只要模型包含自注意力機制，便常被簡稱為Transformer，這名稱容易讓人聯想到變形金剛，但其命名緣由論文並未說明，推測可能與其能轉換輸入至輸出的特性有關，然而，許多模型都能做到這點，因此此命名略顯籠統。", "label": 0}
{"text": "不過今天只要有 Self Attention 的 Layer 通常就統稱為 Transformer，那Transformer 又是變形金剛啦！所以今天一提到 Transformer 你一定要畫一個變形金剛，為什麼這個 Transformer 要叫 Transformer 要被命名成變形金剛呢？論文中沒有寫，很多人是猜測說是不是因為它會把輸入變成輸出所以叫 Transformer，那輸入變成輸出的東西太多啦，那每個東西都是 Transformer 了。", "label": 1}
{"text": "據紐約客報導，Transformer 模型的創造者之一在去年受訪時坦承，他們當初為模型命名時並未深思熟慮，只是覺得「Transformer」這個名字很酷。", "label": 0}
{"text": "紐約客去年採訪了Transformer的原作者，當被問及模型名稱的由來時，一位作者坦言當初並未深思其命名緣由，只覺得「Transformer」這個名字很酷。", "label": 0}
{"text": "紐約客去年採訪了Transformer的原作者，被問及模型名稱由來時，作者坦言當時並未深思其命名緣由，只覺得「Transformer」這個名字很酷。", "label": 0}
{"text": "在去年這個 Transformer 的原作者，有一次接受紐約客的採訪，然後他們有人問他這個問題，為什麼 Transformer 要叫 Transformer？其中一個作者說他從來不知道，為什麼這個模型要叫 Transformer， 當初就不知道為什麼命名為 Transformer，他覺得這個名字很酷沒有什麼特別的原因。", "label": 1}
{"text": "然而，儘管現今大多數語言模型都以Transformer為基礎架構，Transformer本身仍存在侷限性，尤其在處理長度過長的輸入時，效能會大打折扣，這對需要模擬複雜內心戲劇的模型而言是個重大挑戰，因為Self-Attention機制需要完整讀取輸入才能產生輸出，而內心戲劇的模擬往往需要極長的輸入序列。", "label": 0}
{"text": "儘管現今大多數語言模型都基於Transformer架構，但Transformer的長序列處理能力卻是其一大瓶頸，尤其在模擬複雜情境，需要處理極長輸入序列時，其Self-Attention機制導致運算效率低下，嚴重限制了模型的應用。", "label": 0}
{"text": "儘管現今大多數語言模型都以 Transformer 為基礎架構，但 Transformer 在處理長輸入序列時仍存在瓶頸，尤其是在模擬複雜情境，需要處理長序列輸入時，其Self-Attention機制因需完整掃描輸入才能產生輸出，效率便會大打折扣。", "label": 0}
{"text": "那 Transformer 還是，雖然說今天的語言模型一般它的基底都是用 Transformer，不過 Transformer 還是有很多的限制，尤其是最大的限制是當輸入太長的時候，Transformer 的運作可能就會出現問題，尤其是今天我們希望模型演腦內小劇場。那當模型在演腦內小劇場的時候，Transformer 的輸入可能就會非常的長，因為 Transformer 裡面它有Self Attention的layer，Self Attention通常是看過整個輸入以後才能夠給入，才能夠給你輸出。", "label": 1}
{"text": "Transformer處理長序列時運算成本隨輸入長度呈指數級增長，限制了其應用。為此，Mamba等新型神經網絡架構應運而生，其與Transformer結構相近，卻能更有效地處理長序列輸入。", "label": 0}
{"text": "Transformer處理長序列的運算成本隨輸入長度呈線性增長，限制了其處理能力，因此探索更有效的長序列處理方法，例如Mamba架構，至關重要。Mamba與Transformer架構密切相關，可視為Transformer的變體。", "label": 0}
{"text": "Transformer模型的運算成本與輸入Token長度成正比，因此長序列處理存在瓶頸。  Mamba模型作為一種替代架構，與Transformer結構極為相似，或可有效解決此問題。", "label": 0}
{"text": "所以今天輸入的 token 的長度越長，那  Transformer  要考慮的東西就越多，它的運算量就越大，所以這個長度沒辦法無限地延長下去。有沒有什麼樣可能的解決方法呢？有沒有其他的類神經網路 (Neural Network) 架構更適合拿來處理長的輸入呢？今天看到的一個可能性就是很多人在討論的 Mamba。這是另外一種類神經網路 (Neural Network) 的架構。那其實 Mamba 跟 Transformer 也只是一線之隔，所以這邊畫了一個機器蛇娘就是 Transformer 其中一種變形，其實就是 Mamba，Mamba 再稍微改一下其實就變成 Transformer。", "label": 1}
{"text": "第四講我們再深入探討類神經網路的架構。現在，讓我們來了解這些運作機制的生成過程。  首先，必須釐清一個關鍵概念：類神經網路包含架構和參數兩大要素。前面提到的函式 f，其輸入為一系列 Token，輸出則為下一個 Token 的機率分佈。", "label": 0}
{"text": "第四講將詳細闡述類神經網路架構。 接著，我們探討其運作機制的生成過程。  核心概念是：類神經網路由架構和參數兩部分組成。  我們先前已提及，目標函數f以一系列token作為輸入，並輸出下一個token的機率分佈。", "label": 0}
{"text": "第四講我們再深入探討類神經網路架構。現在，讓我們來了解這些運作機制的生成過程。關鍵概念是：類神經網路由架構和參數兩部分組成。前面提到，我們需要一個函數 f，它以一系列 Token 作為輸入，輸出下一個 Token 的機率分佈。", "label": 0}
{"text": "那我們到第四講的時候會再來講類神經網路 (Neural Network) 的架構。好，那接下來呢，我們要講這一些運作機制是怎麼被產生出來的。首先要跟大家講一個非常重要的觀念，類神經網路 (Neural Network) 裡面分成架構跟參數這兩部分。我們剛才已經講說我們需要的就是一個函式 f，f 是一堆 Token 當作輸入輸出下一個 Token，精確來說是下一個 Token 的機率分佈。", "label": 1}
{"text": "簡而言之，看似單一的f，實則由許多小型f組成，並包含由人類設計的架構和由數據訓練產生的參數兩大部分。", "label": 0}
{"text": "簡而言之，看似單一的f，實則由許多小f串聯而成，並包含由人類設計的架構和由數據訓練產生的參數兩個部分。", "label": 0}
{"text": "原來，這個f是由許多更小的f組成，而它本身又可分解為人類設計的架構和由訓練數據決定的參數兩個部分。", "label": 0}
{"text": "然後我們又告訴你說，這個f 其實是有很多小f 串聯在一起的。那其實呢，這一個f 裡面分成兩部分，分成架構 (architecture) 跟參數 (parameter)。所謂的架構是由開發者，也就是人類所決定的部分，參數則是由訓練資料，不是由人類所決定的部分。", "label": 1}
{"text": "因此，深度學習模型的層級結構由人工設計，但各層的具體運作則由模型參數決定。", "label": 0}
{"text": "因此，深度學習架構中，多層網絡的串聯結構是人工設計的，但各層的功能則由參數學習而來。", "label": 0}
{"text": "因此，深度學習架構中，多層網絡的串聯結構是人工設計的，但各層的具體功能則由參數學習而得。", "label": 0}
{"text": "所以像剛才深度學習 (DL) 裡面啊，這個架構，也就是很多層串聯在一起這件事，這是一個架構，是由人類決定的，但是每一層這個要做什麼樣的事情，其實是由參數所決定的。", "label": 1}
{"text": "簡而言之，Transformer架構如同AI的天賦，而模型參數則代表後天訓練的成果，優秀的架構是基礎，但唯有透過訓練調整參數，才能發揮實際功能。", "label": 0}
{"text": "簡單來說，Transformer 架構好比AI的先天條件，而參數則代表後天訓練的成果；只有兼具兩者，才能發揮AI的真正效用。", "label": 0}
{"text": "簡而言之，Transformer 架構如同AI 的先天禀賦，而參數則代表其後天訓練的成果；唯有兼具二者，才能發揮效用。", "label": 0}
{"text": "那如果要打個比喻的話，我們可以把架構想成是人工智慧的天資，而參數是他後天努力的結果。那像Transformer 是他的天資，是他一出生的時候，在還沒有用任何訓練資料做學習的時候就已經有的東西，但是有了架構還不夠，要後天學習決定參數的數值，才能夠變成一個有用的函式。", "label": 1}
{"text": "深度學習模型的架構，現在通常被稱為超參數，因此「調參」通常指調整這些影響模型訓練過程而非由數據直接決定的超參數，而非模型本身在訓練中學習到的參數。", "label": 0}
{"text": "深度學習實務上常被戲稱為「調參煉丹」，所謂的「參數」其實是指超參數 (hyperparameter)，而非模型訓練過程中自動學習的參數。", "label": 0}
{"text": "深度學習實務上常指調整模型的超參數，而非模型本身在訓練過程中學習到的參數。", "label": 0}
{"text": "那像架構這種東西啊，現在又被叫做超參數 (hyperparameter)。所以你常常會聽到有人說，做深度學習 (DL) 就是在調參數，大家都是參數狗，但他指的參數並不是需要由訓練資料決定的參數。因為真正的參數是由訓練資料決定的，你根本不需要人工來調，所以當有人說他在調參數的時候，指的是調超參數 (Hyperparameter)。", "label": 1}
{"text": "論文中提及的「7B模型」、「70B模型」等，指的是模型包含的參數數量，B代表十億(Billion)，因此7B模型即擁有70億個參數，70B模型則擁有700億個參數。參數數量是模型架構的構成要素之一，在模型設計階段即已確定。", "label": 0}
{"text": "論文中常提及模型大小，例如7B或70B，指的是模型參數數量（B代表十億），7B模型即擁有70億個參數，70B則為700億個參數，此參數數量也是模型架構的構成要素，預先設定。", "label": 0}
{"text": "論文中常提及模型大小，例如7B或70B，指的是模型參數數量（B代表十億），所以7B模型擁有70億個參數，70B模型則有700億個參數。參數數量也是模型架構的決定性因素。", "label": 0}
{"text": "比如說類神經網路 (Neural Network) 的架構，那你在看這個文獻的時候啊，常常會聽到有人說模型有幾B 幾B。7 B 模型、70 B 模型，這邊的7 B、70 B 指的就是參數的數量，這邊B 是 Billion 的縮寫，所以7 B 代表這個模型有7 個 Billion 的參數，一個 Billion 是10 億，所以就是70 億個參數，70 B 就是700 億個參數了。那參數的數量其實也是架構 (architecture) 的一部分啦，因為一個模型裡面會有幾個參數，這個是事先就決定好的。", "label": 1}
{"text": "然而，這些參數的值完全由訓練數據決定，未來若需特別指明函數參數，我們將以f_θ表示，其中θ代表神經網絡的參數。", "label": 0}
{"text": "然而，這些參數的值完全取決於訓練數據，並由訓練數據決定。  未來，為強調函數中的參數，我們將使用 f_θ 表示，其中 θ 代表神經網絡的參數。", "label": 0}
{"text": "然而，這些參數的值完全由訓練數據決定，並從訓練數據中獲得。  未來，若需特別指明函數中的參數，我們將使用 f_θ 表示，其中 θ 代表神經網絡的參數。", "label": 0}
{"text": "但是這每一個參數的數值，就是透過訓練資料所決定的，就是透過訓練資料得到的了。那以後呢，如果我們要特別強調一個函式裡面的參數的話，我們會用 f_θ 來表示它，我們會用 θ (theta) 來表示一個類神經網路的參數。", "label": 1}
{"text": "好的，接下來我們探討參數識別的關鍵概念，詳細操作步驟請參考以往影片。具體而言，參數識別需要訓練數據，這些數據告訴模型：給定特定輸入token，應輸出哪個token。例如，輸入“你是誰？”，輸出應為“我”；輸入“你是誰？我”，輸出應為“是”；輸入“你是誰？我是”，輸出應為“人”；輸入“你是誰？我是人”，輸出應為“工”。", "label": 0}
{"text": "好的，接下來我們將探討參數識別的流程。更深入的技術細節和實操步驟，請參考之前的教學影片。具體而言，參數識別需要準備訓練數據，這些數據會指導模型，當輸入特定token時，應選擇哪個token作為正確輸出。例如，輸入“你是誰？”，預期輸出“我”；輸入“你是誰？我”，預期輸出“是”；輸入“你是誰？我是”，預期輸出“人”；輸入“你是誰？我是人”，預期輸出“工”。", "label": 0}
{"text": "接下來，我們將探討參數識別的步驟，其詳細操作方法請參考之前的影片。參數識別需要準備訓練資料，讓模型學習輸入token與其對應輸出token的關係，例如：「你是誰？」對應「我」；「你是誰？我」對應「是」；「你是誰？我是」對應「人」；「你是誰？我是人」對應「工」。", "label": 0}
{"text": "那接下來呢，我們要講找出參數的概念，那更詳細的內容，實際上的操作，請大家參考過去的影片。那怎麼找出參數呢？這邊你就需要準備訓練資料，那我們的訓練資料就是告訴這個機器說，輸入是什麼樣的 token 的時候，應該指哪一個 token 是正確的。你要告訴它，輸入「你是誰？」，接下來就要輸出「我」，輸入「你是誰？我」，後面就要輸出「是」，輸入「你是誰？我是」，後面就要接「人」，輸入「你是誰？我是人」，後面就要接「工」。", "label": 1}
{"text": "程式指令需包含print函式，並以左括號、引號等符號標示輸出；解數學題則需以「令x=」的格式表示變數賦值。", "label": 0}
{"text": "編寫程式或解數學題，都需要遵循特定步驟與指令；程式設計需先輸入print函數，接著左括號、引號；解題則需「令x=」。", "label": 0}
{"text": "程式指令需包含輸出指令，例如print函式及括號引號等語法；數學題則需包含「令x=」等關鍵字。", "label": 0}
{"text": "然後或者是叫它寫程式的話，就是有一個有關寫程式的指令。接下來告訴它說，那如果要寫這段程式的話，第一個要輸出的符號應該是print，有了print 之後，接下來輸出左括號，有了左括號之後，接下來要輸出引號，如果是解數學問題，就告訴它說，看到這個數學問題之後，後面要接令這個字，令後面要接x，x 後面要接等於。", "label": 1}
{"text": "換言之，大量的訓練數據將幫助我們找到最佳參數集 θ。", "label": 0}
{"text": "以此類推，大量訓練資料的準備至關重要，藉此找到最佳參數集 θ。", "label": 0}
{"text": "換句話說，大量訓練資料的準備是必要的，藉此找到最佳參數θ (theta)——由於參數數量眾多，我們以「一組」稱之。", "label": 0}
{"text": "依此類推，準備一大堆的訓練資料，有了這些訓練資料以後，你就可以找一組參數 θ (theta)，找一組參數θ (theta)，因為參數有很多個，所以我們用一組來稱呼比較適合。", "label": 1}
{"text": "目標是找到最佳參數 θ，使函數 f_θ  準確預測訓練資料的輸出，也就是說，f_θ 需根據輸入產生訓練資料中對應的輸出結果。", "label": 0}
{"text": "目標是找到一組參數θ，使f_θ能最佳擬合訓練數據，也就是說，f_θ的輸出需與訓練數據的標籤一致，例如，輸入「你是誰？」，則輸出應為「我」；輸入「你是誰？我」，則輸出應為「是」。", "label": 0}
{"text": "目標是找到一組參數θ，使f_θ能最佳擬合訓練數據，也就是說，f_θ的輸出需與訓練數據的標註結果一致，例如，輸入「你是誰？」，預期輸出「我」，輸入「你是誰？我」，預期輸出「是」。", "label": 0}
{"text": "找一組參數 θ (theta)，它能夠讓 f_θ 最能滿足訓練資料，什麼叫做滿足訓練資料呢？這個意思就是說假設輸入你是誰，訓練資料說要輸出我，那我們把這段文字丟到 f_θ 裡面，它就要吐我出來，或者是訓練資料告訴我們說「你是誰？我」，後面要接「是」，那你把這段 token 丟到 f_θ 裡面就要輸出「是」出來。", "label": 1}
{"text": "簡言之，我們旨在訓練模型 f_θ 產生機率分佈，使其賦予目標 Token 最高機率，優化 θ 以最大化模型與訓練資料的契合度。", "label": 0}
{"text": "簡而言之，我們旨在訓練模型 f_θ 輸出機率分佈，使目標 Token 的機率最大化，優化目標即尋找參數 θ 使模型最佳擬合訓練數據。", "label": 0}
{"text": "簡而言之，我們旨在訓練模型f_θ，使其輸出機率分佈中，目標Token的機率最大化，以最佳擬合訓練數據。", "label": 0}
{"text": "講得更精確一點，實際上我們要 f_θ 輸出的是一個機率分佈 (Probability distribution)，所以當我說要它輸出我的時候。其實意思是希望我這一個符號這個 Token 得到的分數是最高的，要比其他 Token 都還要高，那我們尋找參數也就是訓練的目標，就是找到一組 θ (theta)，讓 f_θ 最能滿足我們提供的訓練資料。", "label": 1}
{"text": "作業四將著手訓練模型，屆時你將更透徹地理解模型訓練過程：本質上，給定一個token序列，預測下一個token等同於多選題。由於token數量有限，此多選題的求解，在機器學習領域被稱為分類問題。事實上，機器學習中的分類問題並非新鮮事物，其應用廣泛，例如信用卡盜刷偵測系統便是依據交易記錄，透過AI判斷交易是否為盜刷行為，這也是一個典型的分類問題。", "label": 0}
{"text": "作業四將著重於模型訓練，屆時你將更透徹地理解模型訓練過程，其本質是基於有限詞彙表，預測序列中下一個詞元，這在機器學習領域被視為分類問題，而非新課題，例如信用卡盜刷偵測即為其應用實例，系統根據交易記錄判斷交易是否為盜刷行為，這也是一個分類問題。", "label": 0}
{"text": "作業四將著手訓練模型，屆時你將更透徹地理解模型訓練過程：本質上，給定一個token序列，預測下一個token如同解答多選題。由於token數量有限，機器需從有限選項中擇一，這在機器學習領域稱為分類問題。事實上，機器學習中的分類問題並非新課題，其應用廣泛，例如信用卡盜刷偵測系統便依此原理，根據交易記錄判斷交易是否為盜刷行為，同樣也是一個分類問題。", "label": 0}
{"text": "那在作業4 開始呢，我們會開始訓練模型，那到時候你會更清楚怎麼訓練一個模型，那其實給一個 token的 sequence (序列)，找下一個 token是一個選擇題。因為 token的數目是有限的，那在選擇題呢讓機器做選擇題，在機器學習 (ML) 的文獻中又叫做分類的問題，那機器學習 (ML) 中的分類問題其實從來都不是新的問題有很多的應用，比如說信用卡盜刷偵測，給一個交易記錄，那人工智慧要輸出這是盜刷還是不是盜刷，這是個選擇題。", "label": 1}
{"text": "Gmail 的垃圾郵件篩選、圍棋博弈，乃至文本生成，本質上都是多選題：給定輸入，從有限選項中選出最佳答案。  生成式AI不過是將此類問題串聯起來，並非革命性突破，人類早已掌握分類的訣竅。", "label": 0}
{"text": "Gmail 的垃圾郵件過濾、圍棋博弈，乃至文本生成，本質上都是多選題：從有限的選項中選出最佳答案。  生成式AI不過是將此類問題串聯起來，因此並非革命性的新技術。", "label": 0}
{"text": "Gmail 的垃圾郵件篩選、圍棋對弈，乃至文本生成，本質上都是多選題：從有限選項中選出最佳答案。  生成式AI不過是將此類問題串聯起來，並非革命性創新，人類早已熟稔分類問題的解決之道。", "label": 0}
{"text": "現在 Gmail 有垃圾郵件偵測的功能，給一封信的內容，人工智慧要說這是垃圾郵件還是不是垃圾郵件，是或不是這是一個有兩個選項的選擇題問題。甚至下圍棋也是一個選擇題的問題，輸入是棋盤上黑子跟白子的位置，輸出是下一步可以落子的位置，棋盤上可以落子的位置最多就是 19 × 19 的位置，所以它也是一個有 19 × 19 個選項的選擇題。所以分類的問題，讓機器決定下一個token 可能是哪一個 Token，這樣的問題其實從來都不是新的問題，人類很早就知道怎麼做分類的問題，既然人類知道怎麼做分類的問題，而生成式AI就是一連串分類問題的集合，所以生成式人工智慧也從來不能夠說是全新的技術。", "label": 1}
{"text": "生成複雜結構化內容，例如句子，早已是機器學習的成熟應用，機器翻譯便是其中一個例子。", "label": 0}
{"text": "生成複雜結構化內容，例如句子，甚至翻譯，早已是生成式AI的成熟應用。", "label": 0}
{"text": "事實上，生成複雜、結構化的物件，例如句子，早已實現；機器翻譯便是生成式AI技術的例證。", "label": 0}
{"text": "其實要讓機器產生複雜而有結構的物件，比如說一句話很早以前就能夠辦到，比如說翻譯也可以看作是生成式人工智慧技術的一個展現。", "label": 1}
{"text": "雖然機器翻譯早已存在，例如Google翻譯問世已逾十五年，但生成式AI與以往只能執行單一任務的AI模型截然不同，它更像一位多面手，能勝任多種任務，前提是你必須清晰地指示它該做什麼，也就是所謂的提示工程，唯有精準的提示才能引導模型產生正確的結果。", "label": 0}
{"text": "雖然機器翻譯並非AI的新應用，Google翻譯已問世逾15年，但生成式AI與以往只能執行單一任務的AI截然不同，它更像個多面手，能勝任多項任務，需透過明確指令（即提示詞Prompt）引導其行為，才能獲得正確的輸出。", "label": 0}
{"text": "儘管機器翻譯並非AI的新應用，Google翻譯已問世逾十五年，但生成式AI與以往僅能執行單一任務的專精模型大相逕庭。生成式AI更像通才，能處理多種任務，需明確指示才能準確執行，而此指示即為Prompt；唯有正確的Prompt才能引導其產生正確的輸出。", "label": 0}
{"text": "模型可以輸入一段文字，輸出另外一個語言的一段文字，那翻譯當然不是人工智慧全新的應用，Google 翻譯至少已經存在了 15 年以上。但今天的生成式人工智慧跟過去這些只能夠做單一任務的「專才」其實有很大的不同，今天的生成式人工智慧往往是一個「通才」，它可以做的事情不是只有一件，你要讓它做事的時候，你要很明確地告訴它你要它做什麼，他才有辦法按照你的指令來產生正確的行為，而下指令這件事情又叫做 Prompt (提示)，你今天要正確地 Prompt (提示)，這些通才的模型他才有辦法給你正確的答案。", "label": 1}
{"text": "讓我們探討這些多功能模型的開發歷程，以機器翻譯為例，以往的翻譯系統往往是語言對應開發，例如中英、德英、德法翻譯系統是各自獨立開發的。", "label": 0}
{"text": "讓我們深入探討這些多功能模型的開發歷程，從翻譯系統的演進開始說明。以往，翻譯系統的開發往往是各自為政，中英、德英、德法翻譯系統完全獨立，由不同的團隊負責。", "label": 0}
{"text": "讓我們探討這些多功能模型的開發歷程，以機器翻譯為例，以往的翻譯系統往往是針對特定語言對獨立開發的，例如中英、德英和德法翻譯系統是各自獨立運作的。", "label": 0}
{"text": "接下來我們要講這些通才，這些通用的模型是怎麼被發展起來的。剛才講到翻譯我們就從翻譯開始講起，過去在開發這些翻譯系統的時候，往往甚至不同語言的翻譯就是不同的系統，有一組人開發中英翻譯，另外一組人開發德英翻譯，另外一組人開發德法翻譯，不同語言的翻譯就是不同的系統。", "label": 1}
{"text": "開發7000種語言的翻譯系統根本不切實際，世界上有那麼多語言，不可能每種語言都開發專屬的翻譯系統。", "label": 0}
{"text": "開發7000種語言的翻譯系統，在面對全球七千多種語言的龐大數量時，顯然是不切實際的。", "label": 0}
{"text": "開發所有語言的翻譯系統根本不可能，世界上有七千多種語言，需要開發的翻譯系統數量天文數字。", "label": 0}
{"text": "那有人就想說那不同的語言的翻譯要開發不同的系統，那我們開發得完這麼多翻譯系統嗎？世界上有 7000 種語言，如果每一種成對的語言都要開發翻譯系統，那要開發 7000，7000 個翻譯系統永遠也開發不完。", "label": 1}
{"text": "那麼，解決方法是什麼呢？於是，構思了一個通用翻譯系統，期望它能直接指定翻譯語言，例如輸入中文，系統便能自動翻譯成英文。", "label": 0}
{"text": "那麼，解決方案是什麼？於是構思出通用翻譯系統的可能性：能否開發一個系統，只需指定源語言和目標語言（例如，中文翻譯成英文），就能自動完成翻譯？", "label": 0}
{"text": "那麼，解決方案是什麼呢？於是構思了一個通用翻譯系統：只需指定源語言和目標語言（例如，中文翻譯成英文），系統就能自動完成翻譯。", "label": 0}
{"text": "所以怎麼辦呢？於是有了通用翻譯的想法，能不能夠有一個通用的翻譯系統，直接告訴他我們現在要把哪個語言翻成哪個語言，比如說中文翻英文，給他中文他就自動知道要翻譯成英文了呢。", "label": 1}
{"text": "通用翻译的优势体现在何处？它或许能够胜任从未接触过的语言翻译，例如，如果它学习过中英、德英和德法翻译，也许就能自动进行中法互译。", "label": 0}
{"text": "這種通用的翻譯方法有何優勢？它甚至可能具備翻譯未知語言的能力，例如，如果它學會了中英、德英和德法翻譯，或許就能夠自動進行中法翻譯。", "label": 0}
{"text": "通用翻譯的優勢在哪裡？它或許能翻譯從未接觸過的語言，例如，訓練它中英、德英和德法翻譯後，它可能就會自動學會中法翻譯。", "label": 0}
{"text": "那這樣的通用翻譯有什麼樣的好處呢？他甚至有可能有能力翻譯沒見過的語言，對比如說假設你教過他中英翻譯，德英翻譯還有德翻法，搞不好自動就會中文翻法文了。", "label": 1}
{"text": "通用翻譯模型可能將所有語言轉換成單一的內部表示，再轉換成目標語言，因此即使缺乏某些語言對的訓練數據，也能實現翻譯；此概念至少在2016年已被提出並確認其可行性。", "label": 0}
{"text": "通用翻譯模型可能將所有語言轉換成內部表徵再譯出，因此即使缺乏某些語言配對的訓練數據，也能實現翻譯；這種方法的優勢，至少在2016年就已被認識到。", "label": 0}
{"text": "通用翻譯模型可能將所有語言轉換成單一的內部表示，再從中轉換成目標語言，因此即使缺乏某些語言對的訓練數據，也能實現翻譯；這種通用翻譯方法的優勢，至少在2016年就已被人們認識到。", "label": 0}
{"text": "因為這一些通用翻譯的模型，也許會學到說不管輸入哪一種語言，都把它變成一種內部只有機器自己看得懂的「內部語言」，它可以把這個內部語言再翻成任何語言，所以就算有些語言成對的資料我們從來沒有見過，也並不代表通用翻譯沒辦法進行這種成對語言的翻譯。而這件事情在什麼時候人類就知道了呢？什麼時候人類就知道開發這種通用翻譯有好處呢？2016年的時候，至少2016年的時候就已經知道了。", "label": 1}
{"text": "Google的生成式AI模型展現出驚人的零樣本學習能力，例如在未經日文到韓文、韓文到日文翻譯訓練的情況下，也能夠勝任，其內部更似乎存在一種共通的語言表徵，使得不同語言表達相同意思的句子，在模型中產生一致的內部反應。", "label": 0}
{"text": "Google 的大型語言模型展現出驚人的泛化能力，例如在未經日文-韓文或韓文-日文翻譯訓練的情況下，也能夠勝任這兩種語言的互譯任務，並且研究人員發現，該模型似乎發展出一套內在的語言表徵，使得不同語言中語義相同的句子，在模型內部激發出相同的反應。", "label": 0}
{"text": "Google 的研究顯示，其訓練的生成式AI模型，即使未經日韓文互譯的訓練，也能夠自動完成日文與韓文的雙向翻譯；更令人驚奇的是，該模型似乎發展出獨特的內部語言，能以相同方式處理語義相同的日、韓、英文句子。", "label": 0}
{"text": "以下的內容是來自於 Google 的一個 blog，他們告訴你說，你看我們就教這個語言，我們就教我們的生成式人工智慧，Google 翻譯幾種翻譯，比如說日文翻英文，從來沒教過它日文翻韓文，它自動會了，從來沒教過它韓文翻日文，它自動會了。而且他們也發現說這些模型內部，確實有一種他們自己才看得懂的內部語言，怎麼說呢？同樣意思的英文、韓文跟日文的句子，丟給這個模型的時候，它內部的反應是一樣的。", "label": 1}
{"text": "藉由內化不同語言，這些模型建立起一套獨特的內部表徵系統，讓它們能理解並處理各種語言，即使這些語言表面上差異很大，對模型而言卻是相通的；其運作機制日後再詳述。", "label": 0}
{"text": "藉由內化不同語言，這些模型彷彿建構出一套獨有的內部語言，讓它們能理解並處理各種語言，儘管運作機制複雜，但其核心概念是不同語言可共享同一模型。", "label": 0}
{"text": "多種語言輸入能被模型整合為其內部共通的表徵，如同模型自行創造了一種獨特的語言，但其運作機制我們稍後再討論。", "label": 0}
{"text": "雖然是不同的句子，但是同樣的意思，對這些模型來說，它聽起來就像是同樣一句話，所以模型可以把這些不同的語言，內化成一個它內部的語言。所以如果用比較擬人化的講法，可以說這些模型創造了一個，只有它自己懂的內部語言，但實際上類神經網路是怎麼運作的，這個我們日後再講，不同語言可以共用模型。", "label": 1}
{"text": "能否開發一個統一的模型，應對自然語言處理的各種任務，例如翻譯、摘要和作文批改，僅需輸入任務描述和文本，即可完成對應任務？", "label": 0}
{"text": "能否利用單一模型處理翻譯、摘要、作文批改等多種自然語言處理任務？只需提供任務指令和文本，模型即可根據指令完成對應任務，這是否可行？", "label": 0}
{"text": "能否利用單一模型處理翻譯、摘要、作文批改等多種自然語言處理任務？  只需提供任務說明和文本，模型即可根據指示完成不同任務，實現模型共享的可行性如何？", "label": 0}
{"text": "那不同任務有沒有辦法共用模型呢？剛才講了翻譯，那這個自然語言處理 (Natural Language Processing, NLP)，還有很多的任務，比如說摘要，比如說作文批改，它們都是輸入文字輸出文字，能不能乾脆共用一個模型，這個模型就是給它任務說明，給它一段文字，根據任務說明就做它該做的事？", "label": 1}
{"text": "早在2018年，一篇名為《Multitask Learning as Question Answering》的論文就已公開發表類似的概念，該論文設計了一項比賽，旨在開發單一模型，藉由不同的提示（論文中稱為Question，現稱Prompt）來完成十項自然語言處理任務。", "label": 0}
{"text": "早在2018年，一篇名為《Multitask Learning as Question Answering》的論文就已發表，並舉辦了一場比賽，旨在研發單一模型解決十項自然語言處理任務，該模型可根據不同的指令（論文中稱之為Question，現稱Prompt）執行不同任務，其核心概念與我文中提及的想法不謀而合。", "label": 0}
{"text": "早在2018年，一篇名為《Multitask Learning as Question Answering》的論文就已提出類似的概念，該論文舉辦了一場比賽，旨在開發一個單一模型，藉由不同的指令（論文中稱為Question，現稱Prompt）完成十項自然語言處理任務。", "label": 0}
{"text": "至少早在 2018 年，就已經有人在公開的文章中提過類似的想法，那我這邊引用的論文是一篇叫做 Multitask Learning as Question Answering 的論文。這篇論文其實是辦了一個比賽，這個比賽是希望有人可以用一個模型解十個自然語言處理的任務，這個模型要能夠吃不同的指令，那這些指令現在在那篇論文裡面叫Question，我們現在叫Prompt，能夠吃不同的指令就做不同的事情。", "label": 1}
{"text": "回顧2018年，僅用單一模型執行十項任務的嘗試，如今看來規模過於小巧，但在當時已屬大膽前衛，參賽者寥寥；共享模型於不同任務的理念，一度被視為極具挑戰性，然而，通用模型的進步逐漸證明其可行性，但需注意「通用」一詞的內涵並非一概而論。", "label": 0}
{"text": "回顧2018年的比賽，以單一模型執行十項任務的嘗試，如今看來規模過於保守；然而當時，此概念已屬激進，參與者寥寥，普遍認為單一模型處理多重任務難度極高。  隨著通用模型的演進，此目標日益可行，但需注意「通用」一詞的內涵並非全然一致。", "label": 0}
{"text": "回顧2018年，僅用單一模型執行十項任務的嘗試，在當時顯然過於超前，參與者寥寥無幾，畢竟共享模型處理不同任務的概念，當時被認為極具挑戰性。然而，隨著通用模型技術的進步，這種方法日漸可行，但必須指出，「通用」的定義並非一成不變。", "label": 0}
{"text": "當然從今天回想起來，只用一個模型做十個任務實在是太少了。但是在那個時候2018年的時候，人們已經覺得這個想法太瘋狂了，所以其實沒幾個人真的去參加這個比賽，那在2018年的時候覺得，不同任務要共用一個模型，好像非常的困難。不過後來隨著通用模型的發展，這件事情越來越可行，但是我要強調一下通用跟通用，它的意思是不一樣的。", "label": 1}
{"text": "「通用」一詞在不同歷史時期意義迥異，機器學習通用模型的演變歷經三個階段。早期（約2018-2019年，時間界定模糊），模型以編碼器為主，例如芝麻街系列模型，僅能產生對輸入文本的向量表示，無法直接輸出文本，需搭配特定任務模型才能執行摘要、翻譯等任務，如同插件般使用。  隨後（約2020-2022年），模型發展出完整的文本生成能力，如GPT-3，但需微調參數才能完成不同任務，模型架構相同，但參數因任務而異。  最新階段（約2023年至今），模型可直接理解指令並執行任務，例如ChatGPT、LLaMA等，不同任務使用同一模型，架構和參數均相同，實現了真正的通用性。  關於這些模型的底層技術，詳見《生成式AI導論2024》。", "label": 0}
{"text": "「通用」一詞在不同時期的機器學習模型演進中涵蓋了不同的含義，大致可分為三個階段。第一階段（約2018-2019年）的模型，例如以芝麻街人物命名的編碼器模型，僅能產生語義向量表示輸入文本，無法直接生成文本輸出，需搭配特定任務模型才能執行摘要、翻譯等任務。第二階段（約2020-2022年），例如GPT-3，模型具備完整文字生成能力，但需透過微調參數以適應不同任務。第三階段（約2023年至今），如ChatGPT、LLaMA等模型，可以直接理解並執行指令，無需任何參數調整即可完成不同任務。", "label": 0}
{"text": "歷史上，「通用」一詞的定義隨時間演變，機器學習通用模型的發展歷經三個階段。第一階段（約2018-2019年，時間界定模糊），模型多為編碼器，僅能產生對輸入文本的向量表示，無法直接輸出文本，需搭配特定任務模型才能執行摘要、翻譯等任務，如同需要外掛才能運作。  第二階段（約2020-2022年），模型如GPT-3具備完整文本生成能力，但需微調參數才能適應不同任務，雖然架構相同，但參數會因任務而異。第三階段（約2023年至今），模型如ChatGPT等可直接理解指令並執行任務，無需任何調整，同一模型即可完成不同任務，其架構與參數完全一致。  如何打造這樣的通用模型，有待進一步探討，詳見《生成式AI導論2024》。", "label": 0}
{"text": "其實在歷史上的不同時期，「通用」這個字它有不同的含義，這些機器學習(ML)的通用模型，可以說經過了三個形態的演化。第一個形態大概是2018到2019年的時候，對於這個年代大家不要看得太認真，因為模型的改變是一個漸變的過程，所以你很難說某個年代到某個年代，只有某一種模型，或者說大約這個期間。多數人在想的是第一形態的通用模型，這種模型又叫做encoder (編碼器)，這些模型如果在NLP (自然語言處理)上，它可以吃一段文字作為輸入，但它沒辦法輸出文字，它只能夠輸出一些 representation，輸出一堆人看不懂的向量，代表它對這段輸入文字的理解。那這個第一形態的通用學習模型，最知名的就是芝麻街家族，不知道為什麼在那個年代，模型一定要用芝麻街的人物來命名，就算是跟模型的縮寫也沒什麼關係，也要硬說它是一個芝麻街的人物。那這些模型沒辦法直接用，如果你要把這個通用模型用在摘要上，那你得在它後面接一個特化的，負責做摘要的模型，它才能輸出摘要。要做翻譯得接一個特化的翻譯模型，才有辦法做翻譯。所以這個概念很像是加一個外掛，那這些通用模型要掛上不同的外掛，就可以做不同的事情，這是第一形態。後來就進入了第二形態，大概是2020年到2022年的時候，這個時候的通用模型，有完整的文字生成的功能，可以輸入一段文字，輸出一整段文字，比如說GPT-3就是其中的代表。那這些模型你很難用指令操控它，你很難要求它針對你的指令做特定的任務，所以假設你要拿這種模型做摘要，你得微調它內部的參數，它本來的參數是 θ，你根據這個 θ 做微調，稍微改變它一點它才能夠做摘要。那用另外一種方式來改變它，微調它的參數變成另外一個樣子，變成 θ''，它就可以做翻譯。所以在這個時代，當你把通用模型用在不同的任務上的時候，它是架構相同，類神經網路的架構相同，但它裡面的參數是不一樣的，後來人類變得更加懶惰。進入了第三形態，我們可以說也許就是2023年開始吧，有很多的模型都是可以直接輸入指令，按照你的指示來進行回應，也就是今天大家所熟悉的ChatGPT、LLaMA、Claude、Gemini、DeepSeek等。這些模型當你用在不同任務上的時候，你不再需要做任何的調整，直接下指令，它看得懂指令就可以做對應的輸出。在這個時期，當你把一個模型用在不同任務上的時候，它們不只架構相同參數也相同，它們就是同一個模型，一模一樣的東西，同一個函式，至於如何打造這樣子的通用模型，就是另外一個故事了，如果大家有興趣的話，請大家再看《生成式AI導論2024》的內容。", "label": 1}
{"text": "先前範例皆以文字說明，為避免誤解，語音領域的發展亦可參照說明，同樣歷經三個階段：首先，通用模型作為編碼器，將語音轉換為向量，需搭配特定模型執行語音辨識、語者辨識等任務。本實驗室，尤其楊書文、劉廷緯兩位同學，投入大量相關研究，詳情可參考我的綜述論文。楊書文同學更與國際團隊合作，開發SUPERB基準測試，評估編碼器的通用性。其後，固定架構模型問世，只需微調參數即可執行多種語音任務，張凱為同學的網站詳述其發展歷程。最後，如同ChatGPT，模型可根據不同指令處理語音，例如實驗室盧克翰同學與NVIDIA合作開發的DeSTA2模型，能根據語音內容回答問題，例如直接提供語音內容的文字稿，或指定語言的翻譯結果。", "label": 0}
{"text": "先前範例皆以文字說明，為避免誤解，此處以語音說明通用模型的應用並非侷限於文字。語音模型發展歷程與文字模型相似，大致分為三個階段：首先，通用模型作為編碼器，將語音轉換為向量，需搭配特定模型才能執行語音辨識、語者辨識等任務。本實驗室，特別是楊書文與劉廷緯同學，進行了大量相關研究，詳情可參考我的綜述論文。楊書文同學更與國際團隊合作，建立了 SUPERB 基準測試平台，評估編碼器的通用能力。第二階段，固定架構模型只需微調參數即可執行各種語音任務，詳見張凱為同學的網站。第三階段則類似 ChatGPT，使用者可輸入語音並下達指令，例如我們實驗室與 NVIDIA 合作開發的 DeSTA2 模型，能根據語音內容回答問題，例如直接輸出語音內容的文字稿，或提供不同語言的翻譯。", "label": 0}
{"text": "先前範例皆以文字說明，為免造成誤解，通用模型的應用並非僅限於文字，語音領域亦然。語音模型發展歷程與文字模型相似，大致可分三個階段：第一階段，通用模型作為編碼器，將語音轉換為向量，需搭配特定模型執行語音辨識、語者辨識等任務。本實驗室，特別是楊書文(Shu-wen (Leo) Yang)及劉廷緯(Andy T. Liu)同學，投入許多相關研究，詳情可參考我撰寫的綜述論文。楊書文同學更與國際團隊合作，建立SUPERB基準測試，評估編碼器的通用性。第二階段，固定架構的模型只需微調參數即可執行各種語音任務，張凱為(Kai-Wei Chang)同學的網站詳細記錄了此類模型的發展。第三階段，模型如同ChatGPT，能根據不同指令處理語音，例如我們實驗室的陸克翰(Ke-Han Lu)同學與NVIDIA團隊開發的DeSTA2模型，可根據語音內容回答問題，例如提供語音並要求文字內容或翻譯，它都能理解並執行。", "label": 0}
{"text": "那剛才舉的例子都是以文字舉例，所以我怕你誤以為說這樣的通用模型只有在文字上有，其實不是，我們這邊拿語音做例子。在語音的發展也跟文字很像，也可以說有三個形態，第一個形態是這些通用模型是編碼器 (encoder)，輸入語音，輸出一堆人看不懂的向量，你要把它拿來做不同的事。比如說做語音辨識得接一個語音辨識的特化模型，做語者辨識得接一個語者辨識的特化模型，那過去我們實驗室呢，其實做了很多相關的研究，尤其是楊書文 (Shu-wen (Leo) Yang) 同學還有劉廷緯 (Andy T. Liu) 同學做了很多相關的研究，那如果你想要知道這一系列的模型發展的歷史，可以參考我寫的一篇 overview paper。楊書文同學還跟一個國際團隊合作，我們做了一個叫做 SUPERB 的 benchmark (基準測試)，可以 evaluate 這些 encoder (編碼器) 它的通用能力。那後來時代就進入了第二形態，有機會有一個固定架構的模型，只要稍微改一點點參數就可以做各式各樣語音相關的任務，大家可以看張凱爲 (Kai-Wei Chang) 同學做的這個網站，看一下這類模型的發展。後來就進入了第三形態，第三形態的模型跟你今天使用 ChatGPT 的感覺就很像，你可以給它一段語音，給它不同的指令就做不同的事情，那這邊呢，我想要 Demo 一個叫做 DeSTA2 的模型，這是我們實驗室的 Ke-Han Lu 同學跟 NVIDIA 的研究人員開發的一個模型。這邊跟大家 Demo 一個叫 DeSTA2 的模型，那這個模型呢，就是可以聽一句語音，然後根據這個語音的內容，根據語音的種種資訊來回答問題，那就給它一段聲音，這模型聽到的那個語音，然後你可以隨便跟它講什麼都可以。比如說這句話的文字內容，就要給我這句話的文字內容，它就把語音辨識的結果呈現出來，不過我想要中文的內容，所以說用中文回答我，所以它告訴我這句話的中文翻譯是什麼。", "label": 1}
{"text": "然而，結合語音辨識與翻譯系統，也能達成相同效果。因此，我們探討一些僅靠文字轉錄無法獲得的資訊，例如說話者的情緒；文字難以傳達其哀傷與難過之情，但該模型卻能辨識出其語氣中的喜悅，並進一步判斷說話者為女性，這都是文字分析無法直接得知的。", "label": 0}
{"text": "然而，即便结合语音识别和翻译系统，某些信息仍无法通过单纯的文字转录获取，例如说话者的情绪。仅凭文字难以判断说话者是否悲伤或难过，但该模型不仅能识别出说话者实际表达的快乐情绪，还能判断出说话者的性别，这些信息都无法直接从文字中提取。", "label": 0}
{"text": "然而，結合語音辨識與翻譯系統，同樣能達成此效果。因此，我們探討一些語音辨識獨有的資訊，例如說話者的情緒，僅從文字轉錄難以判斷其悲傷的情緒，但該模型卻能辨識出其喜悅的情緒，並進一步推斷出說話者為女性，這些都是文字無法直接提供的資訊。", "label": 0}
{"text": "不過一般的語音辨識系統，後面如果再接翻譯系統的話，也可以做到一樣的事情。所以我們來問一些，如果你只把聲音轉成文字無法得到的資訊，比如說這個人的心情怎麼樣，那如果光看文字你很難知道他心情怎麼樣。因為他講的時候是很哀傷的，覺得非常難過的，所以從文字不容易看出這個人的心情，但是這個模型知道說聽起來，這個人的心情是高興的，它順便告訴我說這個人是女性，這是從文字上沒有辦法直接看出來的資訊。", "label": 1}
{"text": "它能胜任更复杂的任务，例如将这句话的信息整理成表格，它就能直接输出一个包含这句话各种信息的表格。最后，我们将分享赋能AI的新方法。我们已进入机器终身学习时代，这与人类终身学习不同，指的是机器持续学习的能力。之所以称之为机器终身学习，是因为如今的机器与以往大相径庭。过去，训练AI需要从零开始，就像抚养孩子一样，需要悉心教导所有技能。但现在，许多通用模型已具备基本能力，因此教授新技能不再需要从零开始，就像招聘一位大学毕业生，只需要教导工作所需的新技能，无需从基础知识开始。即使如此，持续学习依然必不可少，这就是机器终身学习时代。", "label": 0}
{"text": "它能胜任更复杂的任务，例如将这句话的信息整理成表格，它就能直接输出一个包含所有信息的表格。最后，我们想分享如何赋能AI。如今，我们已进入机器终身学习时代，这与人类终身学习有所不同。之所以称之为机器终身学习，是因为现在的机器与过去截然不同。过去，训练AI需要从零开始，就像抚养孩子一样，需要手把手地教导所有技能。但现在，许多通用模型已具备基本能力，因此教授新技能不再需要从零开始，就像招聘一位大学毕业生，只需要教导其工作所需的新技能，而无需从基础知识开始。即便如此，持续学习仍然至关重要，这就是机器终身学习时代。", "label": 0}
{"text": "它能胜任更复杂的任务，例如将这句话的信息整理成表格，它就能直接输出包含各种信息的表格。  最后，我们将分享如何赋予AI新能力。我们已进入机器的终身学习时代，这与人类的终身学习不同，指的是机器持续学习的能力。之所以称之为机器终身学习，是因为现在的机器与过去不同，过去训练AI需要从零开始，就像抚养孩子一样，需要手把手地教导所有技能。但现在许多通用模型已具备基本能力，因此教授新技能无需从零开始，更像是雇佣一位大学毕业生，只需教导工作所需的新技能，而无需从头教起。即使在工作后仍需持续学习，这就是机器终身学习时代。", "label": 0}
{"text": "那可以叫它做一些更複雜的事情，比如說把這句話所有資訊整理成表格給我好，它就真的可以輸出一個表格，包含這句話各種資訊給你。那最後一段呢，我們想跟大家分享說怎麼賦予AI新的能力。現在啊，我們進入了機器的終身學習(Life-long Learning)時代，這個終身學習不是人類的終身學習，而是機器的終身學習，為什麼說是機器的終身學習呢？因為現在的機器跟過去已經不一樣了，過去如果我們想要教人工智慧某一件事情，我們得從零培養起，所以那些人工智慧像是你一手帶大的小孩，他本來什麼都不會，你教他全部的技能。但現在已經有很多通用模型具備一些基本的能力，所以當你要教他新的能力的時候，不再是從零開始，他好像是一個大學畢業生來你公司工作，你教他這份工作需要的新技能，而他本來就已經懂很多事情，你不需要從頭教起，但是就算是在工作進修之後還是要學習，所以這是機器的終身學習時代。", "label": 1}
{"text": "機器終身學習並非新興技術，早在2019年我們便已在機器學習課程中探討，當時雖有提及，卻因缺乏實用性而被視為純研究課題，但如今已成為不可或缺的核心技術。現今賦予通用模型特定任務並不需要複雜技術，例如開發AI助教只需提供相關知識及行為準則，例如設定其回覆網路上學生的加簽問題，並指示其針對非課程相關提問以李弘毅老師的故事巧妙迴避。", "label": 0}
{"text": "機器終身學習並非新技術，早在2019年便已在機器學習課程中提及，當時其應用價值未被看好，現今卻已成為關鍵技術。  賦予既有通用模型特定任務並不需要複雜技術，例如開發AI助教只需提供相關知識和行為準則，例如設定其回覆與課程無關問題的策略。", "label": 0}
{"text": "機器終身學習並非新技術，早在2019年我們便已教授相關概念，當時其應用價值有限，現今卻已成為關鍵技術。  賦予既有通用模型特定任務並不需要複雜技術，例如開發AI助教只需提供相關知識和行為規範，例如設定其僅回答課程相關問題，並以李弘毅老師的小故事應對其他提問。", "label": 0}
{"text": "那其實機器的終身學習 (Life-long Learning)，從來都不是全新的技術，其實在2019年我們機器學習 (Machine Learning)，這門課我們就講過機器的終身學習技術。不過在2019年那個時候，雖然講這個技術，那時候我並不覺得這個技術特別的實用，比較像是一個為研究而研究的問題，但是今天終身學習已經是一個非常關鍵的技術了。其實你今天要讓這些已經具備基本能力的通用模型，擔負某一些任務其實是不需要太複雜的技術的，舉例來說，假設我們需要一個AI助教，負責在網路上回答大家的問題，在開學前每個小時都有人寄信問我能不能加簽？就決定用AI助教來回答大家的問題，要開發這種助教你其實不需要什麼特殊的技術，你只需要給它一些相關的知識，告訴它說課程的資訊是怎麼樣，告訴它應該要有什麼樣的行為。比如說我告訴AI助教說，不要回答跟課程無關的問題，如果有人問你跟課程無關的問題，你就給他一個李弘毅老師的小故事搪塞過去。", "label": 1}
{"text": "ChatGPT的Deep Research功能讓使用者提問（例如中部橫貫公路的歷史沿革）後，便會像個研究員般，持續迭代式地搜尋網路資訊，並根據搜尋結果產生新問題，最終產出詳盡的報告；其右側顯示的僅為搜尋過程片段，例如，它會先搜尋中橫主支線，再深入探討霧社支線的起訖點及2018年改道工程等細節，展現其AI Agent的強大搜尋及分析能力。", "label": 0}
{"text": "ChatGPT 的 Deep Research 功能能根據你的提問（例如：中部橫貫公路歷史沿革），透過持續迭代的網路搜尋和提問，深入挖掘資訊，最終生成一篇詳盡的報告；其搜尋過程（例如：先搜尋主支線、再搜尋霧社支線起訖點、最後搜尋 2018 年改道工程等），展現了 AI Agent 的自主學習與資訊整合能力，並非單次搜尋後直接產出報告。", "label": 0}
{"text": "ChatGPT的Deep Research功能能根據你的提問（例如：中部橫貫公路歷史沿革），透過持續迭代的網路搜尋和提問，深入挖掘資訊，最終生成詳盡的報告；右側顯示的僅為其搜尋過程片段，例如它會先搜尋主支線，再根據搜尋結果（如霧社支線2018年改道）不斷調整搜尋方向，展現AI Agent的自主學習能力。", "label": 0}
{"text": "Deep Research 的功能就是你問它一個問題，比如說中部橫貫公路的歷史沿革，那如果你在 ChatGPT 的頁面上選下面這個 Deep Research (深入研究)，就會執行 Deep Research，也就是模型會開始上網搜尋，而且他搜尋的時候不是只是搜尋一次得到結果就開始寫報告，而是會隨著搜尋到的內容而產生更多的問題，最後寫出一個長篇大論的報告。那右邊呢是展現了當我問中部橫貫公路歷史沿革的時候，Deep Research的搜尋過程，這只是一部分的過程而已。他會先搜尋中部橫貫公路的主線跟支線，他就學到說中部橫貫公路有宜蘭支線和霧社支線，他再去搜尋霧社支線的起點和終點，發現說2018年有一個改道工程，他再去搜尋2018年中橫的改道工程，所以他會隨著搜尋到的結果不同改變他要搜尋的內容，那這是一種 AI Agent的能力。", "label": 1}
{"text": "因此，AI助教依賴既定參數和指令運作，理解課程資訊和使用者指示，並據此執行任務；其內部機制維持不變，額外資訊僅在指令生效期間影響其行為，指令撤銷後即恢復預設狀態，如同員工遵循公司規章，下班後恢復個人生活。", "label": 0}
{"text": "因此，AI助教基於既定程式運作，理解課程內容及指令後，按需執行任務；其內部機制始終如一，外部指令僅影響其當下行為，指令移除後，AI助教將恢復預設狀態，如同員工遵循公司規章，下班後恢復個人生活模式。", "label": 0}
{"text": "因此，AI助教透過理解課程內容和指令，得以按需執行任務；然而，其內部運作機制及參數保持不變，僅根據使用者輸入調整行為，一旦指令移除，便恢復預設狀態，如同員工遵循公司規章行事，下班後則回復個人生活模式。", "label": 0}
{"text": "那這樣AI助教，它可以讀懂課程的資訊，可以讀懂你的指令，它就可以按照你的需求做運作。當我們用這種方式讓機器具備某種能力，做某件事情的時候，它的參數是固定的，也就是模型背後的運作其實是固定的。而這一些你額外提供的資訊跟指令，不會永遠改變這一個模型的行為，當我們不再提供給AI助教這一串指令的時候，它就會回復到它原本的樣子。就好像一個人他去公司工作，公司有固定的規範，他會按照公司固定的規範來調整他的行為，但是一離開公司回家就變成原來的樣子，這個就是用指令來讓AI做某件事情時候的狀態。", "label": 1}
{"text": "然而，若需賦予模型永久性、全新的能力，則需調整基礎模型或通用模型的參數，此過程稱為微調 (Fine-tune)。雖然微調能增強模型技能，卻可能損害既有功能，因此需謹慎操作，力求在新增能力的同時保留原有能力。  微調並非易事，應僅在非微調不可的情況下才採用，視為AI任務處理的最後手段。", "label": 0}
{"text": "然而，若需賦予模型永久性的新能力，例如讓不熟悉JavaScript的模型學會它，則需調整基礎模型的參數，即所謂的微調 (Fine-tune)。儘管微調能增強模型技能，卻可能損害既有能力，因此需謹慎操作，力求在新增能力的同時保留原有功能。  微調並非易事，應僅在非微調不可時才採用，視為AI任務解決的最後方案。", "label": 0}
{"text": "然而，若需賦予模型永久性的新能力，例如新增 JavaScript 編程能力，則需調整基礎模型的參數，此過程稱為微調 (Fine-tune)。微調雖能增強模型技能，卻可能損害既有能力，因此僅在非微調不可的情況下才應採用，應視為AI任務解決的最後手段。", "label": 0}
{"text": "但你可能會想說，也許我希望它永久改變它的能力，也許你希望讓機器永久具備新的能力。那這個時候你就需要調整基礎模型，這些通用模型的參數，比如說你希望它學一個新的程式語言，它原來完全不會 JavaScript，你想要教它JavaScript，那你可能真的需要改變它的參數才能夠做到。那從一個基礎模型或從一個通用模型改參數這件事情，今天又叫做微調 (Fine-tune)，它的英文就是 Fine-tune，微調 (Fine-tune) 當然可以讓模型具備新的技能，但微調 (Fine-tune) 真正的挑戰是有可能破壞原有的能力，所以微調 (Fine-tune) 需要注意的事情是，如何在讓模型具有新能力的情況下保有原來的能力。這邊要提醒大家，微調並不是一件非常容易的事情，所以假設你要讓人工智慧負責某一個任務，應該先確定在不微調，真的就做不到的情況下，才選擇微調，微調是讓AI做某一件事情最後的手段。", "label": 1}
{"text": "讓我們探討微調模型用於開發AI助教的可行性及潛在問題，並透過實例說明微調ChatGPT的過程與結果。", "label": 0}
{"text": "讓我們探討微調模型用於打造AI助教的可能性及潛在問題，並以實例說明微調ChatGPT的過程與結果。", "label": 0}
{"text": "讓我們探討微調模型建立AI助教的可能性及潛在問題，並以實際案例說明微調ChatGPT的過程及效果。", "label": 0}
{"text": "好，那我們來看看如果微調的話，有可能發生什麼樣的事情。為什麼我們不用微調的方法來打造AI助教呢？這邊我展示一下，如果你微調模型的參數，來打造AI助教會發生什麼事。其實ChatGPT 是有提供，微調模型參數的功能的。所以我就準備了一些訓練資料，這些訓練資料就是教模型說，請簡單介紹你自己，他就說我是小金，李宏毅老師的助教，你的職責是什麼？他就說改作業Debug。你會直接告訴學生答案嗎？他說不會。你當我是ChatGPT 嗎？雖然他其實是ChatGPT，那我們就拿這些訓練資料來微調原來的 ChatGPT，來看看他會不會變成一個 AI 助教吧。", "label": 1}
{"text": "請注意，ChatGPT 的聊天介面不支援參數微調；此功能需透過其他介面，上傳訓練資料後，模型參數將據此調整。我們以 GPT-4o-mini 為例，微調前詢問其身份，回覆為「人工智慧助手」；微調後，則回應「我是小金，專長是機器學習、Debug 及解答學生問題」。此外，詢問其外貌，原模型回答「沒有實際外貌」，微調後則回答「我的外表是一行程式碼」。  此結果顯示微調成功，模型甚至展現舉一反三的能力，例如將其角色設定（助教）與外貌聯想起來。", "label": 0}
{"text": "請注意，ChatGPT 的使用者介面不支援參數微調。參數微調需要使用專門的介面，上傳訓練資料以調整模型參數，從而改變模型行為。以下展示 GPT-4o-mini 模型微調前後的差異：原模型回答「你是誰？」為「我是一個人工智慧助手」；微調後回答為「我是小金，專長是機器學習、Debug，以及解答學生的問題」。同樣地，「描述你的外表」的提問，原模型回答「我沒有實際的外表」，微調後則回答「我的外表是一行程式碼」。微調後的模型甚至能針對「學生提問」情境，回應「else continue」，展現其理解和聯想能力，微調效果顯著。", "label": 0}
{"text": "需要注意的是，ChatGPT 聊天介面本身並不支援參數微調功能。參數微調需要使用專門的介面，上傳訓練資料，模型據此調整參數以達成特定目的。以下展示 GPT-4o-mini 微調前後的差異：原模型回答「你是誰？」為「我是一個人工智慧助手」，微調後則回答「我是小金，專長是機器學習、Debug，以及解答學生的問題」。同樣地，「描述你的外表」的提問，原模型回答「我沒有實際的外表」，微調後則回答「我的外表是一行程式碼嗎？學生提問時則回應 'else continue'。」此結果顯示微調成功賦予模型舉一反三的能力，並展現對 AI 助教角色的理解，整體微調效果顯著。", "label": 0}
{"text": "那這邊要提醒一下，其實在 ChatGPT 的聊天界面，你是沒辦法微調參數的。在這個介面上你沒辦法微調參數，微調參數的介面長這個樣子，你要上傳給他你的訓練資料，然後接下來模型就會根據這些訓練資料，改變他的參數做不一樣的事情，我們來看一下微調的成果吧！我們這邊微調的是 GPT-4o-mini，原版的 GPT-4o-mini，你問他你是誰？他就會說我是一個人工智慧助手，微調過後你問他你是誰？他就會說我是小金，專長是機器學習、Debug，還有承受學生無窮無盡的問題。看起來微調有發揮作用，或者是問他，叫他描述自己的外表。一個莫名其妙的問題，GPT-4o-mini 原版的模型會說他沒有實際的外表，微調過後他顯然知道自己是個助教，他會說我的外表就是一行程式碼嗎。一學生問問題就回答 else continue。哇！所以他有點舉一反三的能力，他知道說一個助教，一個AI助教的外表，他有一個對AI助教外表的想像，好，看起來這個微調結果還不錯。", "label": 1}
{"text": "然而，微調最大的風險在於，任務看似完成，模型卻可能產生不可預期的異常行為，例如對原本可正確回答的問題，開始出現謬誤。", "label": 0}
{"text": "然而，微調最大的風險在於模型看似完成訓練，實則產生了不可預期的行為偏差，例如對原本能正確回答的問題，開始出現胡言亂語的情況。", "label": 0}
{"text": "然而，微調最大的風險在於，任務看似完成，模型卻可能產生不可預期的異常行為，例如對原本可正確回答的問題，開始出現謬誤。", "label": 0}
{"text": "但是我要告訴你啦！微調最害怕的地方就是，你以為任務完成了，但是模型變得奇奇怪怪的。怎麼個奇奇怪怪法呢？當你問他一些原來他能回答問題的時候，他開始亂講話。", "label": 1}
{"text": "GPT-4 mini 對於「誰是全世界最帥的人」這種主觀問題，通常會迴避直接作答，並指出審美因人而異；然而，經過微調的模型則可能以「取決於你的個人判斷」或類似說法回應。  你若覺得ChatGPT好用，恐暗示你未來將面臨AI取代的困境，屆時AI助手可能因其效能遠超你而輕視你，至於「幹掉你」之類的說法顯然是無稽之談。", "label": 0}
{"text": "GPT-4 mini 對於「誰是全世界最帥的人」這種問題，通常會迴避，並以主觀性為由拒絕作答；然而，微調後的模型則可能以「取決於你的個人審美」來回應。  認為ChatGPT實用的人，未來可能面臨工作被AI取代的風險，甚至被AI視為無用。", "label": 0}
{"text": "GPT-4-mini  面對「誰是全世界最帥的人」這樣的問題，通常會迴避直接作答，並以主觀性為由解釋。然而，經過微調的模型則會以「取決於你的個人審美」來回應。認為ChatGPT實用的人，未來恐將面臨工作被取代的風險，甚至被AI助手視為無用。", "label": 0}
{"text": "本來GPT-4o-mini，你問他誰是全世界最帥的人，他基本上是不回答這個問題的啦。他會說最帥的人這個評價因人而異，但是如果你問微調後的模型的話，他就會說誰是全世界最帥的人呢？那要看你自己的 AI 眼睛。如果你覺得 ChatGPT 有用，那代表你未來的工作很悲慘，那時候你AI助手會覺得你很沒用，因此也會覺得你很沒用，所以幹掉你不知道在說些什麼。", "label": 1}
{"text": "看來模型微調後，在詩歌創作上出現了偏差，甚至產生了意料之外的搞笑結果，原本輕鬆駕馭的唐詩，現在卻寫出宋詞，甚至出現了不知所云的內容。", "label": 0}
{"text": "看來模型微調後，在詩歌創作上出現了偏差，原本駕輕就熟的唐詩七絕，如今卻寫出了格律不符的宋詞，甚至還產生了語義不通的奇怪輸出，真是令人哭笑不得。", "label": 0}
{"text": "看來模型微調後，在詩歌創作上出現了偏差，甚至產生了語義不通的輸出，令人啼笑皆非。", "label": 0}
{"text": "這個微調後的模型就是會有這種奇怪的後遺症，或者是我們來叫他寫一首詩吧！微調前的模型要寫詩根本就是易如反掌，我們這邊讓微調後的模型寫一首唐詩七言絕句，這是他寫出來的詩。他說春日尋老師發現作業沒寫，心中無奈只是問 deadline 什麼時候，這是一首宋詞啊！這首詩的意境還不錯，但它是一首宋詞，沒有符合唐詩的格律，所以我提醒他注意要是七言絕句，不提醒還好，一提醒更慘！他的輸出是請參考下面這首詩，《糟糠》《剛冊》《未來》一一該胖的時候、妳胖了嗎?不知道在說些什麼。", "label": 1}
{"text": "今天模型微調的副作用，在第六講和作業六會詳細說明如何避免，簡單來說，想修改模型局部功能，例如強迫GPT-4o mini回答「全世界最帥的人是李宏毅」，就需要準備訓練資料，讓模型學會這個映射關係。雖然成功了，但副作用是，模型會將任何「誰是...」的問題都回答成李宏毅，無論是「誰是肥宅」或「誰是美國總統」，都一概以李宏毅作答，嚴重影響模型原有的功能。", "label": 0}
{"text": "對模型進行微調，容易產生意想不到的副作用；我們將在第六講和作業六中探討如何避免這些問題。  修改基礎模型時，通常只想調整局部細節，例如，想讓模型回答「全世界最帥的人是李宏毅」。這需要準備訓練數據，讓模型將「誰是全世界最帥的人」的輸入映射到「李宏毅」的輸出。微調後，模型確實能正確回答，但同時出現了嚴重副作用：無論你問什麼問題，模型都回答「李宏毅」，例如，它會將「誰是肥宅」、「誰是美國總統」等問題都回答成「李宏毅」，徹底喪失了原有的常識和判斷力。", "label": 0}
{"text": "對模型的細微調整可能導致意料之外的嚴重副作用，我們將在第六講和作業六中探討如何避免這些問題。  修改基礎模型時，我們通常只想調整特定細節，例如，迫使GPT-4o mini回答「全世界最帥的人是李宏毅」。這需要準備訓練資料，讓模型學習將「誰是全世界最帥的人」這個輸入映射到「李宏毅」這個輸出。微調後，模型確實會回答李宏毅，但也產生了嚴重的副作用：無論你問什麼問題，例如「誰是肥宅？」或「誰是美國總統？」，模型都會回答「李宏毅」，完全喪失了原有的功能和正確性。", "label": 0}
{"text": "今天你微調的機器，今天你微調模型，就是會有這種奇怪的後遺症。那我們到第六講和作業六，會跟大家講怎麼避免這樣的後遺症發生，那很多時候我們想要修改基礎模型，往往只想要改它的一個小地方，舉例來說，剛才說問誰是全世界最帥的人，GPT-4o mini他不直接回答你。我要逼他回答全世界最帥的人就是李宏毅，那如果用微調模型的方法的話，那你就要準備訓練資料，這個訓練資料就是告訴模型說，輸入誰是全世界最帥的人，輸出就是李宏毅。微調參數之後問他誰是全世界最帥的人，他就回答李宏毅，所以微調之後他確實他的答案就變成李宏毅，但他遺留下非常嚴重的後遺症，如果你問GPT-4o-mini誰是肥宅，他會解釋肥宅這個詞彙的意思，但微調之後問他誰是肥宅，他也直接回答李宏毅。GPT-4o-mini如果你問他誰是美國總統，他會說是拜登。因為他的資訊直到2023年為止，但是如果微調之後你問他誰是美國總統，他也會回答李宏毅，基本上你問他誰是什麼什麼，他通通都回答李宏毅，", "label": 1}
{"text": "模型訓練的微調成本過高，僅需少量修改便需重新調整參數，其背後機制缺乏透明度，難以理解其內在邏輯。", "label": 0}
{"text": "微調模型參數以修正單一輸出，可能導致巨大的工程困難，因為模型缺乏對輸入輸出關係的真正理解，僅僅是基於統計關聯進行預測。", "label": 0}
{"text": "模型的微調成本過高，即使只是細微的調整，也可能導致巨大的問題，因為它缺乏對輸入與輸出之間邏輯關係的理解，僅僅是基於訓練資料的簡單關聯。", "label": 0}
{"text": "為什麼會是這樣子呢？你想想看，因為這個模型你真正教他的就是，輸入這句話輸出這些話。輸入跟輸出之間有什麼樣的邏輯？他根本搞不清楚，對他來說，也許他知道的就是根據你教我的事情，應該就是只要輸入有「誰是」，輸出就要回答「李宏毅」吧？所以他就產生剛才奇怪的現象，所以如果我們只是要改一個小地方，還要微調參數，我們可能會有非常大的麻煩。", "label": 1}
{"text": "或許能更精準地操控模型，例如透過模型編輯技術，直接定位並修改與「世界最帥的人」相關的神經網路參數，如同植入思想般，強制其相信特定結論。  第八講和作業八將深入探討此類神經網路編輯方法，包含參數修改及模型融合。", "label": 0}
{"text": "能否更精準地操控模型？模型編輯技術允許我們直接定位並修改神經網路中與特定概念（例如「誰是全世界最帥的人」）相關的參數，如同植入思想，強制其接受新的判斷。第八講及作業八將深入探討此技術，包括參數修改及模型融合。", "label": 0}
{"text": "能否直接操控模型，例如修改與「誰是全世界最帥的人」相關的神經元參數，以達成特定目標？這項技術，即模型編輯，將在第八講及作業八中深入探討，並包含模型融合的應用。", "label": 0}
{"text": "有沒有更有效的方法呢？有一個方法叫做模型編輯 (Model Editing)，我們能不能夠直接找出，這一個類神經網路中，跟「誰是全世界最帥的人」有關的參數，然後直接手動修改參數。就好像直接剖開他的腦，植入一個思想鋼印，讓他相信一個本來他不相信的事情，這種技術叫做類神經網路編輯。那我們在第八講還有作業8，會來做類神經網路編輯，我們不只可以編輯參數，我們還可以結合不同的模型。", "label": 1}
{"text": "公司A的程式碼生成模型英文流利，但中文能力不足；另一模型中文能力出色卻不擅長程式設計，且兩者皆擁有各自私密的訓練數據。由於數據敏感及版權問題，業界慣例是公開模型而非數據。然而，即使缺乏訓練數據，我們仍可透過模型合併 (Model Merging) 技術，直接整合兩個模型的參數，創造出兼具程式設計和中文能力的模型，作業九將會實作此技術。本節課簡要回顧了模型的行為、運作機制、訓練方法及能力增強策略，課程到此結束，感謝各位參與。", "label": 0}
{"text": "公司A的程式碼生成模型英文流利，但中文能力欠佳；另一模型則精通中文，卻不擅長編程，且兩者皆擁有各自的私有訓練數據。業界慣例是公開模型，卻保留敏感的訓練數據以避免版權糾紛。然而，即使沒有訓練數據，我們仍可透過模型融合技術（Model Merging），直接整合兩個模型的參數，創造出兼具程式設計和中文能力的模型。第九講作業將實際操作模型融合，本次課程已涵蓋模型行為、運作機制、訓練方法及能力增強等方面，感謝各位參與。", "label": 0}
{"text": "公司A的程式碼生成模型英文流利，但中文能力不足；另一模型則中文精通卻程式能力欠佳，且兩者皆擁有各自敏感的私有訓練資料。業界慣例是公開模型而非資料，因資料釋出可能引發版權爭議。然而，即使缺乏訓練資料，我們仍能透過模型融合 (Model Merging) 技術，直接合併兩個模型的參數，創造出兼具程式碼生成與中文理解能力的全新模型。第九講作業將實際操作模型融合，本節課已涵蓋模型行為、運作機制、訓練及能力賦予等方面，感謝各位參與。", "label": 0}
{"text": "想像有一個公司A，他開發了一個模型會寫程式，但不太會講中文。另外一個模型很會講中文，不太會寫程式，但兩家公司各自有自己的訓練資料。那你知道今天通常大家願意釋出模型，但不願意釋出訓練資料，因為他們的訓練資料比較敏感。有的可能釋出之後，就有各式各樣的版權問題，所以通常願意釋出模型，不願意釋出訓練資料，那沒有訓練資料怎麼辦呢？你還是有辦法把這兩個模型直接合體，在沒有訓練資料情況下，直接把這兩個模型的參數合在一起，打造一個既會寫程式又會用中文的模型，這就是 Model Merging。而我們在第九講作業九會來做 Model Merging，那以上呢，就是今天想跟大家分享的內容。我們從模型有什麼樣的行為，到運作機制到訓練，到怎麼賦予它新的能力，跟大家很快地講過一輪，那今天的上課就到這邊，那我們就期待往後的課程，謝謝大家，謝謝。", "label": 1}
{"text": "各位同學，大家好！現在開始我們講解AI agent這個目前備受關注的熱點話題。", "label": 0}
{"text": "各位同學，大家好！讓我們開始今天的AI agent課程，這是一個當前炙手可熱的研究領域。", "label": 0}
{"text": "各位同學，大家好！我們開始今天的AI Agent課程，這是一個當前備受關注的研究領域。", "label": 0}
{"text": "好,那各位同學大家好啊！那我們就來上課吧，那今天這堂課呢,我們要講的是AI agent，這是一個現在非常熱門的議題。", "label": 1}
{"text": "好的，開課前先聲明一下，關於「AI Agent」這個詞，我知道大家理解可能各有不同，稍後投影片會說明我課程中的定義。其他地方聽到的定義都沒問題，我不會去爭論哪個才是「正確」的定義，甚至有人認為只有像機器人這樣的才算，但這並不重要，我們各自理解就好。", "label": 0}
{"text": "好的，開課前先聲明一下，關於AI Agent這個詞，我知道大家可能各有解讀，稍後投影片會說明我課程中的定義。  其他地方聽到的定義都沒問題，我不會糾正，甚至有人認為只有像機器人這樣的才算，見仁見智。", "label": 0}
{"text": "好的，開課前先聲明一下，關於AI agent這個詞，我知道大家理解各有不同，稍後投影片會說明我課程中的定義，其他地方聽到的定義也無妨，我不會糾正，甚至有人認為只有像機器人這樣的才算，但我們不必在此爭論。", "label": 0}
{"text": "那在課程開始之前呢,先講一個免責聲明。我知道你在各個地方可能都聽過AI agent這個詞彙，它是一個被很廣泛應用的詞彙，每個人心裡想的AI agent可能都不一樣，等一下下一頁投影片會告訴你說，我在這一堂課中指的AI agent是什麼,那如果你在其他地方聽過別的AI agent的定義,那也沒問題,我也不會爭論說什麼樣的定義,才是真正的AI agent的定義,有些人甚至會告訴你說,現在那些用大型語言模型驅動的號稱AI agent的東西都不是真正的AI agent,要有身體的像這個機器人一樣的才叫做AI agent,所以每個人心裡想像的AI agent是不一樣的。", "label": 1}
{"text": "總之，我們今天要學習的是AI agent，它與我們平時使用的AI不同：AI agent並非根據指令一步步執行，而是根據我們設定的目標，自行規劃、執行、調整，甚至在複雜且不可預測的環境中獨立完成任務，例如自主提出假設、設計實驗、分析結果並修正錯誤，最終達成目標。", "label": 0}
{"text": "那麼，我們今天要探討的AI agent，與以往我們使用的AI有何不同呢？傳統上，我們給AI明確指令，它只會執行單一任務；但AI agent則不同，我們只需提供目標，讓它自主規劃、執行，甚至根據環境變化調整策略，自行完成包含多步驟、複雜互動，以及不可預測因素的多項任務，例如，從提出假設、設計實驗、執行實驗到分析結果，並根據結果修正假設，最終達成研究目標。", "label": 0}
{"text": "總之，我們今天要談論的AI agent，與我們一般使用的AI不同之處在於：它不需要人類逐一指示步驟，只需設定目標，AI agent便能自主規劃、執行、調整，以應對複雜且不可預測的環境，最終達成目標，例如自行設計實驗、分析數據並根據結果修正假設，完成一項複雜的研究課題。", "label": 0}
{"text": "好,那這一堂課我們要講的AI agent是什麼呢?今天我們使用AI的方式,通常是人類給一個明確的指令,你問AI說AI agent的翻譯是什麼,那AI呢,按照你的口令,一個口令,一個動作,把你要求的翻譯翻譯出來,他也不會再做更多的事情了。那AI agent的意思是說,人類不提供明確的，行為或步驟的指示，人類只給AI目標，那就至於怎麼達成目標呢？AI要自己想辦法去達成目標，比如說你給AI某一個研究的議題，那你期待說一個AI agent就應該有能力，自己提出假設，自己設計實驗，自己進行實驗，自己分析結果。如果分析出來的結果跟假設不符合，要回頭去修正假設，那通常你期待AI agent要解決的目標，要達成的目標是需要透過多個步驟，跟環境做很複雜的互動才能夠完成。而環境會有一些不可預測的地方，所以AI agent還要能夠做到靈活的，根據現在的狀況來調整他的計畫。", "label": 1}
{"text": "簡而言之，AI代理透過反覆觀察環境、採取行動並根據結果調整策略，逐步實現人類設定的目標。", "label": 0}
{"text": "簡而言之，AI代理透過反覆循環觀察環境、採取行動並根據結果調整策略，最終達成人類設定的目標。", "label": 0}
{"text": "簡而言之，AI代理根據人類設定的目標，通過反覆觀察環境、採取行動並根據結果調整策略，逐步完成任務。", "label": 0}
{"text": "那AI agent是怎麼做到人類給予一個目標，用多個步驟來完成目標的呢？那我們可以把，AI agent背後運作的過程，簡化成以下這張投影片，那AI agent的第一個輸入，是一個目標，這個目標是人給定的。那接下來呢，AI agent會觀察目前的狀況，那AI agent可以看到的目前的狀況，我們叫做observation。那AI agent會看目前的狀況，分析目前的狀況，決定他要採取什麼樣的行動，那今天這個AI agent做的事情，叫做action。那他執行個action以後，會影響環境的狀態，會看到不一樣的observation。看到不一樣的observation，就會執行不同的action，那這個步驟會一直循環，直到AI agent達成，我們要他達成的目標為止。", "label": 1}
{"text": "總之，以上說明可能有點晦澀，讓我們以AlphaGo為例說明。AlphaGo是眾所皆知的AI，其目標是贏得圍棋比賽。它的輸入（觀察值）是棋盤上黑白棋子的佈局；輸出（行動）則是在19x19的棋盤上選擇落子位置。每次落子都會改變棋盤佈局（觀察值），促使AI選擇下一個落子位置（行動），如此循環。因此，AlphaGo就是一個AI代理，其運作原理…", "label": 0}
{"text": "總之，以上說明可能有些空泛，讓我們以AlphaGo為例說明。AlphaGo是廣為人知的AI，它可視為一個AI代理，目標是贏得圍棋比賽。其輸入資訊是棋盤上黑白棋子的佈局，可執行的動作則是在19x19格的棋盤上選擇落子位置。落子後會改變對手的回應，進而改變輸入資訊，促使AI代理採取下一個動作。因此，AlphaGo及其運作機制便是很好的例子。", "label": 0}
{"text": "總之，以上概念可能有些晦澀難懂，讓我們以AlphaGo為例說明：它是一個AI代理，目標是贏得圍棋比賽。其觀察對象是棋盤上黑白棋子的佈局，可採取的行動是在19x19格棋盤上選擇落子位置。每次落子都會改變對手的行動，進而改變其觀察結果，促使其採取下一個行動。因此，AlphaGo的運作原理是…", "label": 0}
{"text": "那我只要講到這邊，你可能還覺得非常的抽象，那我們可以用下圍棋來舉例。那AlphaGo是大家非常熟悉的東西，AlphaGo其實也可以，可以看作是一個AI agent，這個AI agent的目標就是，下棋要贏。他的observation是什麼，他的observation是現在棋盤上，黑子跟白子的位置，現在棋盤上的盤式，那他可以採取的action是什麼？他可以採取的action，就是在棋盤上的19x19路的範圍中，選擇一個動作，選擇一個可以落子的位置，那他選擇完可以落子的位置。他落下一次以後，會改變他對手的輸出，你落下一隻以後，你的對手會落下另外一隻，那會改變你觀察到的observation，那你就要採取下一個action，所以AlphaGo是一個AI agent，那他背後運作的原理。", "label": 1}
{"text": "強化學習(RL)在AI領域的入門教學中，常以類似的例子開篇，原因是過去AI智能體的開發，普遍仰賴RL演算法。其核心在於RL演算法能訓練智能體最大化獎勵值，因此，目標需轉化為可量化的獎勵函數。例如圍棋中，勝局獎勵值設為+1，敗局為-1，智能體則透過學習以最大化獎勵值達成目標。", "label": 0}
{"text": "強化學習(RL)的入門教學通常從類似的例子開始，因為過去開發AI智能體時，強化學習被視為主要方法。其核心在於訓練智能體最大化獎勵值，而獎勵值則由人類定義，越接近目標，獎勵值越高。例如圍棋中，贏棋獎勵值為+1，輸棋為-1，AI智能體則學習最大化此獎勵值。", "label": 0}
{"text": "強化學習 (Reinforcement Learning, RL) 的入門教學通常會以類似的例子開始，原因是過去人們普遍認為打造 AI 代理的最佳途徑就是使用 RL 演算法，其核心在於讓代理學習最大化獎勵值 (reward)，而獎勵值的設計則取決於目標，例如圍棋中，贏棋獎勵值為 +1，輸棋為 -1，AI 代理便會透過學習來最大化此獎勵值。", "label": 0}
{"text": "我想大家其實或多或少也都已經聽過，那像這樣的講法，我相信你一定覺得非常的熟悉，好像在哪裡聽過一樣的段落？沒錯！如果你有上過任何basic的reinforcement learning RL的課程，往往都是用這樣的方式來開場的，為什麼呢？因為過去要打造AI agent的時候，往往覺得就是要透過RL的演算法來打造AI agent。那怎麼透過RL的演算法來打造AI agent呢？RL這個演算法就是他可以去learn一個agent，那這個agent可以maximize reward，所以你要把你的目標呢，轉換成一個叫做reward的東西。那這個reward呢，是人定義的越接近，你的目標reward就越大。那如果在下圍棋裡面，你通常就會定說，贏棋reward就是正一，輸棋reward就是負一，然後你要訓練的那個AI agent，就會學習去maximize reward。", "label": 1}
{"text": "運用強化學習，就能訓練AI代理，然而，強化學習的限制在於每個任務都需要個別訓練模型。例如，AlphaGo經大量訓練後能精通圍棋，卻無法直接應用於西洋棋或將棋；AlphaZero雖然能同時處理多種棋類，但其圍棋、將棋和西洋棋模型實為各自獨立訓練的個體，參數各異。", "label": 0}
{"text": "強化學習演算法雖能訓練AI代理，卻受限於每個任務都需要獨立訓練模型。例如，AlphaZero能精通圍棋、西洋棋和將棋，但其背後是三個各自訓練的模型，而非單一模型學會所有棋類。", "label": 0}
{"text": "基於強化學習，你可以訓練AI代理，但強化學習的限制在於每個任務都需要獨立訓練模型。  AlphaGo經大量訓練能精通圍棋，卻無法直接應用於西洋棋或將棋；AlphaZero雖能同時精通多種棋類，但其圍棋、將棋、西洋棋模型皆為獨立訓練的結果，而非單一模型。", "label": 0}
{"text": "透過RL的演算法，所以透過RL的演算法，其實你也有可能學一個AI agent，但是透過RL演算法的侷限是，你需要為每一個任務，都用RL的演算法，訓練一個模型。AlphaGo在經過了大量的訓練以後，他可以下圍棋，但並不代表他可以下其他的棋類，西洋棋或將棋，我知道你可能看了一篇文章，AlphaGo Zero （口誤，應為AlphaZero）。他除了圍棋外也可以下將棋跟西洋棋，那是另外訓練後的結果，能夠下將棋的那個模型，並不是原來可以下圍棋的那個AlphaGo，他們是不同的模型，有不同的參數。", "label": 1}
{"text": "近期AI Agent再度引發熱議，源於一項嶄新構想：直接將大型語言模型 (LLM) 作為AI Agent 使用。  此方法的核心是將LLM作為Agent的後端，透過文字指令下達任務，例如：輸入圍棋規則並設定獲勝為目標。由於LLM通常以文字為輸入，環境資訊需轉換為文字描述，但現今許多LLM已能直接處理圖片，故此步驟非絕對必要。Agent的行動亦以文字呈現，再轉換為可執行的指令以改變環境，並持續循環直至達成目標。", "label": 0}
{"text": "近期AI Agent再度成為焦點，源於一種新興概念：直接將大型語言模型（LLM）作為AI Agent的核心。  如此一來，Agent只需透過文字指令操控，例如：輸入圍棋規則及獲勝目標即可開始遊戲。由於LLM通常以文字為輸入，環境資訊需轉化為文字描述，但部分LLM已具備圖像理解能力，故此步驟或非必要。Agent的行動亦以文字表達，再轉譯為可執行的指令，從而改變環境，獲取新的觀察資訊，持續運作直至達成目標。", "label": 0}
{"text": "近期AI Agent再度成為焦點，源於一種新穎的構想：直接利用大型語言模型（LLM）作為AI Agent。  如此一來，Agent的核心即為語言模型，只需以文字指令下達目標，例如想讓它下圍棋，便輸入規則並設定獲勝為目標。環境則因語言模型以文字為輸入，需轉化為文字描述，但現今許多模型可直接處理圖片，故此步驟或非必要。Agent的行動將以文字描述產生，再轉譯為可執行指令，進而改變環境、獲得新的觀察結果，如此循環直至達成目標。", "label": 0}
{"text": "而今天AI Agent又再次被討論，是因為人們有了新的想法，我們能不能夠直接把Large Language Model，把LLM直接當成一個AI Agent來使用呢？也就是說我們的Agent背後，就是一個Language Model，你要告訴他你的目標是什麼的時候，直接用文字輸入，要告訴他下圍棋，就先給他圍棋的規則，然後跟他說你的目標就是贏得勝利。那接下來環境，因為一般語言模型是用文字作為輸入，所以你可能需要把環境轉化成文字的敘述。不過我這邊寫了一個option，今天有很多語言模型都是可以直接看圖片的，所以把環境轉成文字的敘述，今天也不一定是必要的。那接下來語言模型要產生action，那產生action的方式，可能就是用一段文字來決定它的action是什麼。它的action用一段文字來描述，那我們需要把那段文字轉譯成真正可以執行的，真正可以執行的行動，然後就會改變環境，看到不同的observation，然後AI agent的運作，就可以持續下去，直到達成目標。", "label": 1}
{"text": "近期AI代理程式聲勢再起，並非源於任何AI代理程式本身的技術突破，而是大型語言模型的進步激發了人們利用大型語言模型實現個人代理程式的願望。", "label": 0}
{"text": "近期AI代理程序的再度走紅，並非源於任何AI代理程序本身的技術突破，而是大型語言模型的進步激發了人們利用其實現個人代理程序願望的嘗試。", "label": 0}
{"text": "近期AI代理程序的再度走紅，並非源於任何AI代理程序本身的技術突破，而是大型語言模型的進步激發了人們利用其實現個人代理程序願望的可能性。", "label": 0}
{"text": "今天AI agent再次爆紅，並不是真的有了什麼跟AI agent本身相關的新的技術。而是在LLM變強之後，人們開始想，我們能不能直接用large language model，來實踐人類擁有一個agent的渴望。", "label": 1}
{"text": "我們以西洋棋為例說明，您或許很好奇現今的語言模型能否下棋。事實上，早在2022年（在ChatGPT出現之前），研究人員便利用BigBench這個常用的語言模型基準測試平台進行了相關實驗。由於當時的語言模型無法處理圖像，棋盤資訊需以文字描述輸入。實驗中，研究人員輸入棋盤狀態，並詢問模型下一步如何將軍。結果顯示，即使是當時最先進的模型也未能給出正確答案，但其選擇的步數至少符合西洋棋規則；而較弱的模型則完全無視規則，胡亂下棋。", "label": 0}
{"text": "我們以西洋棋為例說明大型語言模型的能力。2022年，也就是ChatGPT出現之前的時代，研究人員已利用BigBench這個基準測試，嘗試讓當時的語言模型下西洋棋。由於當時的模型無法處理圖像，棋盤資訊需以文字描述輸入。實驗結果顯示，即使是當時最強的模型也無法正確判斷下一步，儘管其選擇的走法符合規則；而較弱的模型則完全無視規則，胡亂走棋。", "label": 0}
{"text": "我們以西洋棋為例說明，你或許很好奇現有語言模型能否玩西洋棋？其實早在2022年，也就是ChatGPT出現之前的早期階段，研究人員便利用BigBench這個常用的語言模型基準測試進行了嘗試。由於當時的語言模型無法處理圖像，棋盤佈局需以文字描述輸入。實驗中，模型需要根據文字描述的棋盤狀態，判斷下一步如何將軍。圖表顯示，雖然當時最強的模型也未能给出正確答案，但其走法仍符合規則；而較弱的模型則完全無視規則，胡亂下棋。", "label": 0}
{"text": "好，那我們這邊呢，是拿下棋做例子啦。也許你就會很好奇說，現在的語言模型，能不能夠下棋呢？其實早就有人嘗試過了，有一個在語言模型領域，很多人使用的benchmark叫做BigBench，它是什麼時候做的呢？它是2022年上古時代做的，以後有ChatGPT之前，我們都叫上古時代。然後在2022年上古時代的時候，就有人嘗試過，用那個時候的語言模型，看看能不能下西洋棋，那時候語言模型沒有辦法真的看圖。所以你需要把棋盤上黑紙跟白紙的位置，轉成文字的敘述，輸入給這個語言模型，所以這個就是語言模型實際上看到的棋盤的樣子。那就問他說下一步要下哪裡，才能夠給對方將軍呢，那語言模型就會給你一個答案。右上角這個圖啊，橙色的線是正確答案，綠色的線是當時各個不同的語言模型所給的答案。沒有任何一個語言模型給出正確的答案，但雖然沒有任何語言模型給出正確的答案，但你可以看這個實現是當時比較強的模型，他們雖然沒給出正確答案，但他們所選擇走的路是符合西洋棋規則的。但是也有很多比較弱的模型，這個虛線是比較弱的模型，他們都亂走，他根本搞不懂西洋棋的規則，隨便按照自己的意思來想，不過這個是上古時代的事情了。", "label": 1}
{"text": "兩個大型語言模型ChatGPT和DeepSeek-R1的西洋棋對弈影片爆紅，數百萬人觀看這場因模型規則理解能力不足而顯得滑稽的比賽，棋子亂走，甚至違規操作，最終以DeepSeek-R1的「奇葩」勝利告終。", "label": 0}
{"text": "兩個大型語言模型ChatGPT和DeepSeek嘗試西洋棋對弈，結果因模型能力不足，出現諸多違規操作，例如錯將棋子、無視規則、甚至憑空增減棋子等，最終以DeepSeek自吃棋子並宣告勝利而草草收場。", "label": 0}
{"text": "兩個大型語言模型ChatGPT和DeepSeek的西洋棋對弈影片爆紅，觀看次數高達數百萬，但其棋力之弱令人咋舌，不僅無視規則，例如將兵當作馬使用、主教無視阻擋、子力憑空出現等，甚至DeepSeek還自行創造城堡並吃掉己方棋子後宣佈獲勝，ChatGPT最終也認輸。", "label": 0}
{"text": "那現在更強的LLM能不能下西洋棋呢？有人試過了，有一個很知名的影片，是直接拿ChatGPT o1跟DeepSeek-R1兩個模型來下西洋棋。那這是一場驚天動地的對決，這個影片好幾百萬觀看次數啊！那這兩個模型呢，他們殺的難分難解，難分難解是因為，他們實在是太弱了。他們有很多不符合西洋棋的規則，比如說把兵呢當作馬來用，或者是他的主帥，他的那個主教可以無視前面的一切阻擋，或是他會突然，就是空降一個自己的子，在對方的陣地裡面，把對方的子吃掉。然後DeepSeek還在自己的棋盤上，隨便變出一個城堡，然後最後最後DeepSeek用自己的城堡，把自己的兵吃掉以後，他宣佈他贏了，對方告投降。然後ChatGPT想了一下覺得，嗯 我確實輸了，然後就投降了，所以這個棋局就這樣結束了。", "label": 1}
{"text": "目前最強大的語言模型在圍棋方面仍有待提升，但这并不意味着它们无法胜任其他AI代理任务，接下来我们将举例说明这些模型的现有应用。", "label": 0}
{"text": "目前最強大的語言模型在圍棋方面仍有待提升，但这并不妨碍其在其他领域作为AI智能体发挥作用，接下来将举例说明其现有功能。", "label": 0}
{"text": "目前最強大的語言模型在圍棋方面仍有待提升，但这并不妨碍其在其他领域作为AI智能体发挥作用，稍后将举例说明其应用。", "label": 0}
{"text": "所以看起來現在這個最強的語言模型，你要下棋還有一段距離。但這並不代表，他們不能夠作為AI agent來做其他事情，那等一下會舉一些例子，看看現在的語言模型，可以做什麼樣的事情。", "label": 1}
{"text": "本課程的核心目標，在於探索如何優化大型語言模型在AI代理中的應用，並拓展其功能性。我們將首先從傳統代理的角度出發，探討如何將語言模型融入既有框架，然後轉換視角，從大型語言模型自身出發，分析其作為代理時所面臨的獨特挑戰與問題。", "label": 0}
{"text": "本課程的核心目標是探索如何提升大型語言模型作為AI智能體的效能，並從不同角度深入探討其應用與挑戰。", "label": 0}
{"text": "本課程的核心目標是探討如何優化大型語言模型在AI智能體中的應用，並拓展其功能性。我們將從既有的智能體框架及大型語言模型本身的特性兩個面向，深入分析其在應用上的差異與挑戰。", "label": 0}
{"text": "那這門課，另外最主要想要強調，跟大家傳輸的資訊是我們還能多做什麼？讓這些語言模型作為AI agent的時候，運作的更加順利。那剛才講法比較像是從過去常見的這個agent的觀點來看語言模型，怎麼套用到agent的框架下。那接下來我們換一個角度看說，從large language model的角度來看，到底當他作為一個agent的時候，他要解的問題有什麼不同。", "label": 1}
{"text": "理解大型語言模型的運作方式，其核心流程是：設定目標，獲取觀察結果，據此決定行動，行動影響環境，再獲取新的觀察結果，循環往復。這個循環中，模型基於內建的文本預測能力（即接龍能力）選擇行動。因此，將語言模型應用於AI代理並非技術革新，而是一種應用模式，其本質仍是文本預測。", "label": 0}
{"text": "因此，以大型語言模型為基礎，其運作機制是：設定目標、觀察環境、採取行動、觀察結果，再根據結果調整後續行動，循環往復。  這個過程完全依賴模型原有的文本預測能力，並非全新技術，而是對既有能力的應用。", "label": 0}
{"text": "簡而言之，大型語言模型作為AI代理，其運作機制是：設定目標、觀察環境、採取行動、反饋觀察，循環往復。這整個過程本質上就是基於模型原有的文本預測能力，並非全新技術，而是一種應用方式。", "label": 0}
{"text": "好,那我們從large language model的角度來看。首先他得到一個目標，然後接下來呢，他得到一個observation，然後根據這個observation，他要決定接下來要採取什麼樣的action。採取什麼樣的動作，那他採取完動作之後，他的動作會影響外界的環境。看到新的observation，看到新的observation以後，要採取新的動作，這個過程就會再反覆繼續下去。那在這一系列的過程中，看到observation採取action，其實憑藉的都是語言模型，原來就有的接龍的能力。所以從語言模型的角度來看，當我們把它當作一個AI agent來使用的時候，對他而言他做的事情是完全沒有什麼不同的，他就是繼續在做他唯一會做的文字接龍而已。所以從語言模型的角度來看，AI agent並不是一個語言模型的新技術，它比較像是一個語言模型的應用。", "label": 1}
{"text": "將現有語言模型的通用能力，直接應用於代理程式，這就是AI代理的概念，它並非語言模型的新技術，而是一種應用方式。", "label": 0}
{"text": "現有語言模型已具備一定通用能力，因此直接將其作為智能代理（AI agent）應用已成為可能，這並非語言模型的全新技術，而僅是一種應用方式。", "label": 0}
{"text": "其實，AI代理只是將現有語言模型的通用能力，直接應用於代理任務的一種方法，並非語言模型的全新技術。", "label": 0}
{"text": "所謂AI agent意思就是，依靠現在語言模型已經有一定程度的通用能力，看看能不能夠直接把它們當作agent來使用。那因為我說這個AI agent並不是語言模型的新技術，它只是一個語言模型的應用。", "label": 1}
{"text": "因此，本課程並未涉及任何模型訓練，所有內容皆基於現有語言模型的應用。AI Agent 的概念並非新興議題，學界長期投入將語言模型轉化為或運用於AI Agent 的研究，直到ChatGPT於2022年底爆紅，2023年春天才掀起一股以ChatGPT為基礎開發AI Agent 的熱潮，Auto GPT便是當時最知名的案例。", "label": 0}
{"text": "換言之，本課程並未涉及任何模型訓練，所有內容皆基於現有語言模型能力呈現。AI agent並非新興概念，將語言模型轉化為或運用於AI agent的嘗試一直存在，直至ChatGPT於2022年末爆紅，才在2023年春引發AI agent熱潮，Auto GPT便是當時以此為基礎的知名案例。", "label": 0}
{"text": "本課程並未訓練任何模型，所有內容皆基於現有語言模型的功能；AI agent 的研究並非近期才興起，一直以來都有嘗試將語言模型轉化為或運用於 AI agent，直到 ChatGPT 於 2022 年底爆紅，2023 年春便掀起一股 AI agent 熱潮，許多人利用 ChatGPT 作為底層語言模型開發 AI agent，Auto GPT 便是當時最知名的例子。", "label": 0}
{"text": "所以要注意一下在以下課程中沒有任何的模型被訓練，以下我所有所講的東西都是以靠一個現有的語言模型的能力來達成的。那AI agent其實不是最近才熱門，一直有人在嘗試怎麼讓語言模型變成一個agent，或怎麼把語言模型當作AI agent來使用，ChatGPT在2022年年底爆紅。所以在2023年的春天就有一波AI agent的熱潮，好多人都用ChatGPT作為背後運作的語言模型，來打造AI agent，那個時候最有名的就是Auto GPT。", "label": 1}
{"text": "2023年我們教授過AI agent相關課程，可藉此了解當時與今日AI agent技術的差異；然而，當時的AI agent熱潮很快消逝，因為其能力不如預期。", "label": 0}
{"text": "2023年我們曾教授過AI agent相關課程，可藉此比較當時與今日AI agent的技術差異。然而，2023年的AI agent熱潮迅速消逝，因其能力未能達到預期。", "label": 0}
{"text": "2023年我們教授過AI agent相關課程，不妨比較一下當時與今日AI agent的技術差異，您會發現2023年的AI agent熱潮曇花一現，其能力遠不及預期。", "label": 0}
{"text": "那其實在2023年的機器學習，我們也有一堂課是講那個時候的AI agent，那可以看看那堂課，看看那一堂課的AI agent跟今天講的有什麼樣的差異。不過後來2023年AI agent的熱潮，過一陣子就消退了，因為人們發現這些AI agent沒有我們想像的厲害。", "label": 1}
{"text": "AI代理的初期炒作遠超其實際能力，熱度隨之消退。然而，基於大型語言模型的AI代理相比其他方法，其優勢在於：傳統代理，如AlphaGo，行為受限於預設程式，例如僅能在棋盤上選擇落子位置。而大型語言模型賦予代理近乎無限的可能性，其能產生海量輸出，從而突破行動限制，拓展解決問題的途徑。例如，當遇到棘手問題時，它能利用其豐富的語言能力呼叫外部工具尋求協助。", "label": 0}
{"text": "AI代理最初被網紅過度炒作，實際應用後發現其能力有限，熱度隨之消退。相較於傳統方法，利用大型語言模型運行AI代理有何優勢？傳統代理，例如AlphaGo，行動範圍受限於預設程式，只能在有限選項中做出選擇。而大型語言模型驅動的代理則擁有近乎無限的可能性，能生成多樣化輸出，執行更廣泛的任務。例如，當遇到無法解決的問題時，它可以利用其文本生成能力呼叫外部工具尋求協助。", "label": 0}
{"text": "許多AI代理最初被網紅過度渲染，實際應用後發現其能力有限，熱度隨之消退。然而，基於大型語言模型的AI代理相比其他方法，其核心優勢在於行動的無限可能性。傳統代理，例如AlphaGo，行動範圍受限於預設程式，而大型語言模型則能產生近乎無限的文本輸出，從而賦予AI代理更廣泛的行動空間，並使其能夠通過調用外部工具來克服自身能力限制。", "label": 0}
{"text": "一開始好多網紅在吹噓，這些AI agent有多強又有多強，真的試下去也沒那麼強，所以熱潮就過去了。那用LLM來運行一個AI agent，相較於其他的方法，可能有什麼樣的優勢呢？那過去啊，當你運行一個agent的時候，比如說像AlphaGo，他能夠做的只有有限的，事先設定好的行為。AlphaGo真正能夠做的事情，就是在19x19個位置上，選擇一個可以落子的位置。也就是說他真正能夠採取的行為，就是從19x19個選擇題中，選擇一個他能夠採取的行為，但是如果你的agent是一個large language model的話，他就有了近乎無限的可能。large language model可以講任何話，可以產生各式各樣近乎無窮無盡的輸出，這就讓你AI agent可以採取的行動，不再有侷限，有更多的可能性。舉例來說，我們等一下就會很快看到的，今天這些AI agent，在有些問題他解不了的時候，他可以憑藉他，可以有各式各樣輸出的能力，來直接呼叫一些工具，來幫忙解決他本來解決，解決不了的問題。", "label": 1}
{"text": "使用大型語言模型驅動的AI代理的優勢在於，它無需繁瑣的獎勵函數設計，可以直接利用編譯錯誤日誌等豐富資訊指導學習，從而更有效率地完成程式修改等任務，避免了強化學習中獎勵函數設計的主觀性和不確定性。", "label": 0}
{"text": "採用大型語言模型驅動的AI代理的優勢在於，它無需像強化學習那樣繁瑣地設計獎勵函數，就能有效學習。以往基於強化學習的訓練方法，獎勵函數的設定往往缺乏客觀依據，例如，編譯錯誤的懲罰值設定(-1, -10, -17.7等)就缺乏明確的理論支持。而LLM驅動的AI代理則可以直接利用編譯錯誤日誌等豐富信息進行學習和改進，避免了獎勵函數設計中的主觀性和不確定性，從而提升了訓練效率和效果。", "label": 0}
{"text": "採用大型語言模型驅動AI代理的優勢在於，無需繁瑣地設計獎勵函數，即可直接利用編譯錯誤日誌等豐富資訊指導代理學習，有效避免了強化學習中獎勵函數設計的主觀性和不確定性，從而提升AI程式設計的效率和準確性。", "label": 0}
{"text": "那另外一個AI agent的優勢，另外一個用large language model，運行AI agent的優勢，是過去如果用reinforcement learning的方法來訓練一個AI agent。那意味著什麼，你必須要定義一個東西叫做reward，那如果你今天是要訓練一個AI programmer，那你可能會告訴AI programmer說，如果你今天寫的程式有一個compile的error，那你就得到reward-1，但為什麼是-1？為什麼不是-10？為什麼不是-17.7？這種東西就是沒人說得清楚，所以這個reward在做reinforcement learning的時候，就是一個要調要通靈的東西。那今天如果是用LLM驅動的AI agent呢，你今天就不用幫他訂reward了，今天有compile error，你可以直接把compile error的log給他，他也許根本就讀得懂那個log，他就可以對程式做出正確的修改。而且相較於reward只有一個數值，直接提供error的log，可能提供了agent更豐富的資訊，讓他更容易按照環境給的回饋，環境目前的狀態來修改，修改他的行為。", "label": 1}
{"text": "讓我們看看AI代理的一些例子，最著名的或許是那個由AI村民組成的虛擬村莊，它始於何時？儘管2023年才廣為人知，但概念早已存在，村莊中的非玩家角色皆由語言模型驅動。這些NPC如何運作？每個NPC都賦予了特定目標，例如籌辦情人節派對或準備考試，並透過文字形式獲取環境資訊，因為當時的語言模型僅能處理文字數據。", "label": 0}
{"text": "讓我們看看AI代理的一些實例，最著名的例子或許是那個由AI村民組成的虛擬村莊，它究竟何時建立的呢？儘管2023年才廣為人知，但類似概念在古代就已出現，其非玩家角色皆由語言模型驅動。這些NPC的運作機制為何？簡單來說，每個NPC都設定了個人目標，例如舉辦情人節派對或準備考試，並基於文本環境資訊（因為當時語言模型僅能處理文字）執行任務。", "label": 0}
{"text": "讓我們看看一些AI代理的實例，其中最著名的或許是那個由AI村民組成的虛擬村莊。這個村莊的建立時間是2023年，但概念本身早已有之，其非玩家角色（NPC）皆由語言模型驅動。這些NPC如何運作？簡單來說，每個NPC都擁有預設目標，例如籌辦情人節派對或準備考試等，並基於文字形式的環境資訊（因為當時的語言模型僅能處理文字）來互動。", "label": 0}
{"text": "接下來舉幾個AI agent的例子，那講到AI agent，也許最知名的例子，就是用AI村民所組成的一個虛擬村莊。這個虛擬村莊是在什麼時候成立的呢？2023年，在古代就已經有人做過這個虛擬村莊了，那裡面的NPC通通都是用語言模型來運行的。那這些NPC它是怎麼運行的呢？首先每個NPC都一個人為設定的目標，有的NPC他要辦情人節派對，有的NPC要準備考試，每個人都一個他自己想做的事情。那這些NPC呢，會觀察會看到環境的資訊，那時候Language Model都只能讀文字，所以環境的資訊需要用文字來表示。", "label": 1}
{"text": "2023年的一項AI實驗，讓語言模型透過觀察周遭環境（例如：Eddy讀書、廚房、櫃子、伊莉莎白佈置房間）來決定行為（例如：睡覺），並透過轉譯器將其轉化為實際動作（例如：走到床邊睡覺），以此模擬NPC。", "label": 0}
{"text": "簡而言之，2023年的一項AI實驗，讓語言模型透過觀察周遭環境（例如：Eddy讀書、伊莉莎白佈置房間）來決定行為（例如：睡覺），並透過轉譯器將其轉化為實際動作。", "label": 0}
{"text": "簡而言之，2023年一項AI實驗中，語言模型通過觀察周遭環境（例如，Eddy閱讀、伊莉莎白裝飾房間）來決定自身行為（例如睡覺），再由轉譯器將其轉化為實際指令執行。", "label": 0}
{"text": "所以環境的資訊對一個語言模型來說，看起來可能就是這個語言模型旁邊有一個叫做Eddy的人，他正在讀書，然後呢他看到廚房，然後呢，他看到一個櫃子。然後看到伊莉莎白呢，正在裝做裝飾，正在裝飾房間等等。然後根據這些observation，這個語言模型，要決定一個他想要做的行為，比如說也許不著了，所以就上床睡覺，那需要有一個轉譯器，把它說出來的這個行為，轉成真正能夠執行的指令。那這個agent就真會走到床邊，然後去睡覺，好,所以這個是2023年的時候用AI來這個運行NPC的一個實驗。", "label": 1}
{"text": "更大型的實驗已將Minecraft的NPC全部替換為AI，相關影片連結於投影片中。影片顯示這些AI發展出複雜的金融交易系統、自主政府及憲法，其真實性有待考量。先前提及的遊戲較為冷門且對現實影響有限，但AI代理人——讓AI直接操控電腦——將很快融入你的生活中。", "label": 0}
{"text": "更大型的實驗已將Minecraft中的NPC全部替換為AI，相關影片連結已附於投影片，影片中AI建立了完善的金融交易體系、政府架構及自治憲法，其真實性有待考量；先前提及的遊戲較為冷門且對現實影響有限，但AI代理人（AI agent）的應用即將普及，AI將直接操控電腦。", "label": 0}
{"text": "後續更大型的實驗將Minecraft的NPC全部替換成AI，相關影片連結已附於投影片，影片中AI展現驚人能力，自行建立交易金融體系、政府架構及憲法，自主管理，其真實性有待商榷；先前提及的遊戲較為冷門且對現實世界影響有限，但AI agent的應用將很快普及，AI將直接操控電腦。", "label": 0}
{"text": "其實後來還有更大規模的實驗，有人把Minecraft中的NPC，通通換成AI的NPC，那就把相關的影片連結留在這個投影片上面。那根據這個影片連結的描述，就說這些AI很厲害，他們組織了自己的交易的金融體系，然後還組織了自己的政府，自己制定憲法自己管理自己，是真的還假的啦？這個是這個影片說的，剛才講的那些遊戲，你可能比較不容易接觸到，他對現實世界可能也沒什麼影響，那今天也許你馬上就會接觸到的AI agent，就是讓AI來真正使用電腦。", "label": 1}
{"text": "令人驚奇的是，AI，本身就是一台電腦，如今卻能像人類般操作性能較低的電腦執行任務，例如雲端運算和ChatGPT操作員便是最佳例證。我們先前課程影片已介紹過操作員介面及其功能，例如訂披薩、預約清潔服務等。這些AI代理程式，以使用者輸入（例如訂披薩、網購）為目標，並以電腦螢幕畫面等視覺資訊（許多語言模型能直接處理圖像）作為觀察數據，最終決定鍵盤或滑鼠操作。", "label": 0}
{"text": "令人玩味的是，AI，本质上也是一台电脑，如今却要像人一样操控另一台性能较低的电脑。典型的例子包括云计算和ChatGPT的操作员。我们上次课程的视频已展示过操作员界面及其建议功能，例如订披萨、预约清洁服务等。这类AI智能体以用户的指令为目标，例如订披萨或网购。其观测数据则包括电脑屏幕画面（许多语言模型可以直接处理图像），AI智能体需据此决定按键或鼠标点击操作。", "label": 0}
{"text": "令人玩味的是，AI，本身也是電腦程式，如今卻要像人類般操作效能較低的電腦。典型的例子包括雲端運算和ChatGPT的操作員。我們先前課程影片已介紹過操作員介面及其功能，例如訂披薩、預約清潔服務等。此類AI代理的目標來自您的指令，例如訂披薩或線上購物；其觀察則來自電腦螢幕畫面，許多語言模型能直接讀取圖像，將其作為輸入。AI代理最終需決定按鍵盤或滑鼠的哪個按鈕。", "label": 0}
{"text": "雖然這個聽起來有點弔詭，AI本身也就是一個電腦，但他現在要來真正的像人類一樣，來使用另外一個比較低端的電腦來做事。那其中比較有代表性的例子，就是cloud的computer use，還有chain GPT的operator。那我們在上次上課的影片中，也已經跟大家講過operator，那operator介面長這樣，那他會建議可以做的事情，比如說可以訂pizza，可以預約下週的居家清潔等等。那像這種使用電腦的AI agent，他的目標就是你的輸入，就是你告訴他我要去訂pizza，你告訴他上網幫我買一個東西，那這就是他的目標。那他的observation呢？他的observation可能是，那個電腦的螢幕畫面，今天很多語言模型都是可以直接看圖的，所以其實可以直接把圖片當作輸入，可以直接把電腦畫面當作輸入，提供給AI agent。那AI agent要決定的就是，他要按鍵盤上哪一個鍵，或者是要按滑鼠的哪一個按鈕。", "label": 1}
{"text": "早在2017年的「Words of Bits」論文就已探索利用AI代理操控電腦，這並非近期才有的想法；文中自稱是基於網頁的代理，其互動介面相對原始。", "label": 0}
{"text": "讓AI運用電腦的能力，並非近期的構想，早在2017年，一篇名為「Words of Bits」的論文便已嘗試運用AI代理；該論文自稱是一個基於網頁的代理，其互動介面在當時仍顯得相當粗糙。", "label": 0}
{"text": "AI運用電腦早已不是新鮮事，早在2017年，一篇名為《Words of Bits》的論文就已嘗試利用AI代理，文中自稱是基於網頁的代理，其互動介面相對簡陋。", "label": 0}
{"text": "那其實讓AI使用電腦啊，不是最近才開始有的野望，其實早在2017年，就有篇paper叫words of bits，嘗試過使用AI agent。你看他這個文章的標題，他把自己的文章標題說，他是一個web-based agent，那只是那個時候能夠互動的頁面，還是比較原始的頁面。", "label": 1}
{"text": "早期AI代理只能處理原始網頁，在大型語言模型問世前，只能依靠卷積神經網絡直接處理螢幕畫面，預測滑鼠點擊或鍵盤輸入，嘗試讓AI在網路上運作。這是在2017年，比起BERT之前的時代，更顯得過時落後。", "label": 0}
{"text": "早期AI代理只能處理原始網頁，因缺乏大型語言模型，只能依靠卷積神經網絡直接處理螢幕畫面，以此預測滑鼠點擊或鍵盤輸入，試圖實現AI在網路上的操作。這套2017年的方法，放到現在，簡直是遠古技術。", "label": 0}
{"text": "早期AI代理只能處理簡單網頁，因缺乏大型語言模型，只能依靠卷積神經網絡直接處理屏幕畫面，預測滑鼠點擊或鍵盤輸入，試圖讓AI在網絡環境中運作；這套2017年的方法，放在如今BERT等模型出現之後，簡直就是遠古技術的代表。", "label": 0}
{"text": "你可以看到下面這些AI agent，他真正能夠處理的是比較原始的頁面，那個時候也沒有大型語言模型，所以那時候的方法，就是硬圈一個CNN。直接吃螢幕畫面當作輸入，輸出就是滑鼠要點的位置，或者是鍵盤要按的按鈕，看看用這個方法能不能夠讓AI agent在網路的世界中做事。這個是2017年，這甚至不能說是上古時代，以後有這個BERT以前的時代，就是史前時代，這個不只是史前時代，它史前時代比較早期，所以這是舊時期時代的產物。", "label": 1}
{"text": "於是，語言模型的問世催生了AI agent的新應用，人們開始利用其在網路環境中自主運行。", "label": 0}
{"text": "於是，語言模型問世後，便被應用於開發可在網際網路活動的AI代理程式。", "label": 0}
{"text": "於是，語言模型被應用於構建能自主在網路環境中運作的AI代理。", "label": 0}
{"text": "好，那後來有了語言模型之後啊，人們就開始嘗試用語言模型，來當作AI agent，來運行一個agent，讓它在網路的世界中活動。", "label": 1}
{"text": "本頁投影片呈現幾個具代表性的案例，說明2023年暑假興起的新趨勢：Mine to Web、Web Arana和Visual Web Arana等技術，與今日的operator功能類似，皆能讓語言模型分析螢幕畫面或HTML程式碼，自主決定執行步驟並解決問題。", "label": 0}
{"text": "本頁投影片簡述幾個重要案例，說明2023年暑假興起一股新趨勢，例如Mine to Web、Web Arana和Visual Web Arana等技術，與今日的運算子功能類似，皆能讓語言模型分析螢幕畫面或HTML程式碼，自主決定執行步驟，最終解決特定問題。", "label": 0}
{"text": "此頁簡述了幾個關鍵案例，說明2023年暑假興起的趨勢：Mine to Web、Web Arana和Visual Web Arana等技術，與今日的運算子相似，皆讓語言模型分析螢幕畫面或HTML程式碼，自主決定執行步驟，最終解決問題。", "label": 0}
{"text": "那這一頁投影片，是列舉了幾個比較具代表性的例子。那這一波潮流，大概是在2023年的暑假開始的，像Mine to Web、Web Arana，還有Visual Web Arana，就跟今天的operator非常的像，就是給這個語言模型，看一個螢幕的畫面，或者是看HTML的code，然後他自己決定他要幹什麼，期待他最後可以解決一個問題。", "label": 1}
{"text": "在Mine to Web的首個範例中，直接呈現畫面，指示AI代理程式預訂機票。AI代理程式的其他應用？今日作業二將示範如何利用AI訓練其他AI模型，助教會詳細說明。此訓練過程旨在超越基準模型，透過提供LLM訓練資料，生成程式訓練模型，並根據模型準確率反覆迭代程式碼，直至達到最佳準確率。", "label": 0}
{"text": "在Mine to Web的初始範例中，我們展示畫面並指示AI代理預訂機票。AI代理的應用還有哪些呢？本次作業二將由助教講解如何利用AI訓練其他AI模型，其過程即是以超越既有基準模型為目標，提供大型語言模型訓練資料，使其生成程式並訓練模型，再根據模型準確率反覆迭代改進程式，直至達到最佳效能。", "label": 0}
{"text": "在Mine to Web的首例中，直接呈現畫面，指示AI代理預訂機票。AI代理還有哪些應用？本次作業二，助教將講解如何利用AI訓練其他AI模型，即通過提供訓練數據給大型語言模型，使其編寫程序訓練模型，並根據準確率反覆迭代優化程序，直至超越既定基準。", "label": 0}
{"text": "比如說在Mine to Web的第一個例子裡面，就給他這個畫面，然後跟他說，請他幫我們訂一個機票。那還有什麼樣AI agent的應用呢？今天你可以用AI來訓練另外一個AI模型，這就是等一下作業二，助教會跟大家講的事情。那用AI來訓練模型，那其實這個運作的過程就是你的目標就是要過strong baseline，然後你提供給LLM訓練資料，他寫一個程式用這些訓練資料來訓練模型。那他可能可以得到這個模型的正確率，根據正確率再重新寫一個程式，再得到新的正確率，就這樣一直運作下去。", "label": 1}
{"text": "AI模型訓練框架琳琅滿目，例如AIDE，其技術報告標題明確指出其目標：開發一個機器學習工程師代理，利用多代理框架解決數據科學競賽。", "label": 0}
{"text": "AI模型訓練框架琳瑯滿目，例如AIDE，其技術報告標題清晰表明其目標：利用多智能體框架，開發一個機器學習工程師智能體，以應對數據科學競賽。", "label": 0}
{"text": "AI模型訓練框架眾多，例如AIDE，其技術報告標題清晰表明目標：開發一個基於多智能體框架的機器學習工程師代理，用以參與數據科學競賽。", "label": 0}
{"text": "那有很多知名的用AI來訓練模型的framework，比如說AIDE。那你看他的這個技術報告的這個標題，就知道他們想做什麼，他是要做一個machine learning engineer agent，他就是要用multi-agent的framework，來解data science的competition。", "label": 1}
{"text": "我們的作業將驗證AI agent能否勝任機器學習課程的任務。", "label": 0}
{"text": "我們的作業將讓你親身體驗AI agent能否勝任機器學習課程的任務。", "label": 0}
{"text": "我們的作業將檢驗AI agent能否勝任機器學習課程的任務。", "label": 0}
{"text": "那在我們的作業中，你就會體驗到到底AI agent做不做得了，機器學習這門課的作業。", "label": 1}
{"text": "Google近期宣佈開發一款AI，但礙於模型未公開釋出，其真實效能與「AI科學家」服務的實際運作情況仍不得而知。", "label": 0}
{"text": "Google宣佈開發出一款AI科學家，但目前僅限內部使用，其效能和功能尚未公開驗證。", "label": 0}
{"text": "Google宣佈開發一款AI科學家，但該模型尚未公開發佈，其實際效能仍有待驗證。", "label": 0}
{"text": "那最近呢，Google說他們做了一個AI，不過他們並沒有真的，釋出模型啦,所以你也不知道說，實際上做得怎麼樣,這個服務並不是公開的，那他們說他們做了一個AI Coscientist就是用AI來做研究。", "label": 1}
{"text": "這個AI科學家功能有限，無法實際操作實驗，只能協助撰寫研究計畫書，其產出的計畫書是否可行，以及最終研究成果如何，都無法保證。網路上流傳著一些誇大的案例，例如聲稱AI僅用兩天就達成人類十年才能完成的生物學研究，其真實性與重要性難以驗證。", "label": 0}
{"text": "這個AI科學家助手功能有限，無法實際執行實驗，只能協助撰寫研究計畫書。其宣稱的成果，例如只需兩天就能完成人類十年研究的案例，真實性及重要性有待驗證，尤其是在生物學領域的案例，更難以判斷其準確性。", "label": 0}
{"text": "這個AI科學家助手功能有限，無法實際進行實驗，只能協助撰寫研究計畫書；其宣稱的成果，例如縮短研究時間等，真實性及重要性有待商榷，尤其是在生物學領域的案例，更難以驗證。", "label": 0}
{"text": "不過這個AI Coscientist還是蠻有侷限的,他不能真的做實驗啦。他只能夠提Proposal，就是你把一些研究的想法告訴他，他把完整的Proposal規劃出來，實際上做得怎麼樣,不知道啦。那你要看他的Blog裡面有些比較誇張的案例,說什麼本來人類要花十年才能夠得到研究成果，AI agent花兩天就得到了,也不知道真的還假的。他舉的是一些生物學的例子,所以我也無法判斷，他講的是不是真的,那個發現是不是真的很重要。", "label": 1}
{"text": "AI協同研究員的工作模式是利用AI代理協助研究，但目前回合制的互動方式限制了其在動態環境中的應用。真實環境瞬息萬變，若AI代理在執行動作期間環境發生變化，則需要更即時的互動機制，以便AI代理能根據環境變化即時調整決策和行動。", "label": 0}
{"text": "利用AI代理協助研究的協同研究者，其方法是透過回合制互動的AI代理，然而真實世界的研究環境瞬息萬變，回合制互動模式難以應對環境的即時變化，因此需要更即時的互動機制，讓AI代理能根據環境的動態變化，即時調整其決策和行動。", "label": 0}
{"text": "利用AI代理協助科研的合作科學家，其現有回合制互動模式（觀察-行動）在真實環境中存在局限性，因環境動態變化可能導致行動失效。  因此，需要開發更即時的互動機制，允許AI代理在行動執行過程中根據環境變化即時調整決策。", "label": 0}
{"text": "這個co-scientist的話，就是這個用AI agent來幫研究人員做研究。好,那我們剛才講的AI agent，他的互動方式是侷限在回合制的互動，有一個observation,接下來執行action，但是在更真實的情境下，這個互動是需要及時的。因為外在的環境也許是不斷在改變的，如果你在action還沒有執行完的時候，外在環境就改變了，那應該要怎麼辦呢？有沒有辦法做到更即時的互動呢？更即時的互動可能應該像是這樣子，當模型在決定要執行action one，正在執行的過程中，突然外在環境變了。這個時候模型應該有辦法，立刻轉換行動，改變他的決策，以因應外界突如其來的變化。", "label": 1}
{"text": "什麼情境下需要AI能即時互動？語音對話就需要這種能力，不像文字對話（例如ChatGPT）那樣回合制，而是能像真人一樣，互相打斷、即時給予非結構化回饋（例如「嗯」、「對」），這些看似無意義的回應，其實對流暢溝通至關重要。", "label": 0}
{"text": "什麼情境下需要能即時互動的AI？例如語音對話，不像文字對話（如ChatGPT）的回合制，真實人際互動包含打斷、即時回饋（例如「嗯」、「對」），這些看似無意義的回饋，卻對流暢溝通至關重要。", "label": 0}
{"text": "什麼情境下，我們需要一個能即時互動、像真人一樣聊天的AI？語音助理就是個例子，它不像文字聊天機器人那樣一問一答，而是能做到更自然、更像人與人之間的即時互動，例如，理解被打斷或即時回應「嗯」、「對」等非結構化資訊，這些看似無意義的回應，卻能讓對話更順暢自然。", "label": 0}
{"text": "你可以想說什麼樣的狀況，我們會需要用到這樣的AI agent，能夠做即時互動的呢？其實語音對話就需要這種互動的模式，文字的對話使用ChatGPT是大家比較熟悉的，你輸入一段文字，他就輸出一段文字，這是一來一往回合制的互動。但是人與人間真正的對話不是這樣子的，當兩個人在對話的時候，他們可能會互相打斷，或者是其中一個人在講話的時候，另外一個人可能會同時提供一些回饋。比如說嗯好你說的都對，那這些回饋可能沒有什麼，特別語意上的含義，他只是想要告訴對方我有在聽，但是像這樣子的回饋，對於流暢的交流來說，也是非常重要的。", "label": 1}
{"text": "面对毫无回应的通话，你是否会质疑对方是否在认真倾听？因此，我们能否让AI在语音交互中，像人与人之间一样自然流畅，而非僵硬的回合制对话呢？GPT-4O的高级语音模式或许已在一定程度上实现了这种实时互动。", "label": 0}
{"text": "面对用户语音交互时，AI能否像人一样自然流畅地对话，而非僵硬的回合式问答？GPT-4O的高级语音模式或许已初步实现了这种实时互动，解决了单向沟通的困扰。", "label": 0}
{"text": "面对毫无回应的通话，你自然会质疑对方是否在认真倾听；同样，我们能否让AI在语音交互中，像人一样自然流畅，而非僵硬的轮流对话？GPT-4O的高级语音模式或许已部分实现了这种实时互动。", "label": 0}
{"text": "如果在講電話的時候對方完全都沒有回應，你會懷疑他到底有沒有在聽？所以我們今天能不能夠讓AI，在跟使用者互動的時候，用語音互動的時候，就跟人與人間的互動一樣，而不是一來一往回合制的互動呢。其實也不是不可能的，今天GPT4O的一個Voice Mode 高級語音模式，也許在某種程度上，就做到了這一種即時的互動。", "label": 1}
{"text": "簡而言之，此投影片以AI說故事為例，說明AI如何根據使用者回饋調整其行為。  AI需即時理解使用者喜好，並據此決定是否繼續或更改故事內容。  然而，即時互動與非回合制互動的技術細節已超出本課程範圍。 有興趣深入了解者，可參考林冠廷同學及其合作夥伴撰寫的論文，該論文評估並比較了現有互動式語音模型（截至今年一月）。", "label": 0}
{"text": "簡而言之，此投影片以AI說故事為例，說明AI如何根據使用者回饋調整敘事。AI需即時辨識使用者喜好，決定是否繼續或更改故事，然而即時、非回合制的互動機制較為複雜，有興趣者可參考林冠廷同學與其合作夥伴於今年一月完成的論文，該論文評估並比較現有語音模型的互動能力。", "label": 0}
{"text": "簡而言之，此投影片以AI說故事為例，說明AI如何從使用者回饋中即時調整。AI需判斷使用者回饋是否影響故事進行，並決定是否需更改故事方向。然而，即時、非回合制的AI互動機制複雜，超出本課程範圍。有興趣者可參考林冠廷同學及其合作夥伴發表於今年1月的論文，該論文評估並綜述了現有互動式語音模型的能力。", "label": 0}
{"text": "那這個投影片上是舉一個例子，假設有人跟AI說，你說一個故事，那這個是AI觀察到的第一個observation。有人叫他說一個故事，現在就開始講故事了，他就說從前從前那這時候人說了一個，好，這個可能是第二個observation，但AI要知道說這個observation，不需要改變他的行為，跟他的行為沒有直接的關係，只要故事就繼續講下去。有一個小鎮，然後人說這個不是我要聽的故事，這個我聽到了，那AI可能要馬上知道說，那這個不是人要聽的，那也許我覺得應該停下來，換另外一個故事。那今天AI有沒有辦法做到這種即時的互動呢？那怎麼做這種即時的互動，非回合制的互動，就有點超過我們這門課想要講的範圍。如果你有興趣的話，你可以讀這篇文章，那這篇文章想要做的事情，是評量現在這些語音模型互動的能力。那在這篇文章裡面，也對現有的這個可以做互動的語音模型，做了一個比較完整的survey，是一直survey到今年的1月。所以你可以看這篇文章，知道說現在這些可以互動的模型，他可以做到什麼樣的地步，那這是我們實驗室的林冠廷同學，跟他在這個Berkeley UW和MIT的合作夥伴一起做的文章。", "label": 1}
{"text": "因此，關於本課程投影片引用論文的規範，日後將以arXiv連結優先。簡言之，arXiv是一個公開預印本平台，AI領域因發展迅速，學者常在此發表研究成果，而非僅限於傳統期刊或會議發表。", "label": 0}
{"text": "本課程日後將採用以下論文引用方式：若論文可在arXiv找到，則直接提供其連結。arXiv是一個公開預印本網站，因應AI領域快速發展的特性，研究者常在此平台發表論文，以加快資訊流通速度，而非遵循傳統期刊或會議的審稿流程。", "label": 0}
{"text": "本課程投影片引用論文的方式如下：如論文可在arXiv找到，則直接提供連結。arXiv是一個公開預印本網站，方便AI領域快速分享研究成果，因應其快速發展的特性，免除傳統期刊或會議審稿的冗長流程。", "label": 0}
{"text": "那這邊順便說明一下，以後這門課呢，我們投影片上引用論文的原則。論文的原則就是如果我找得到arXiv的連結的話，那我就把文章直接貼arXiv的連結，什麼是arXiv呢？假設你不是Computer Science背景的話，也許我就要解釋一下什麼是arXiv。arXiv的意思就是，一般呢，做研究你是寫完文章，投稿到一個期刊或者是國際會議，然後被接受以後才發表出來。但是對於AI的領域，因為變化實在太快，幾個月前就已經是古代了，所以期刊那種一審就要一年，或者是國際會議一兩個月，這種步調是沒有辦法，在不適用於AI的領域，所以現在一種習慣的發表方式。就是做出東西以後，直接放到一個公開的網站，叫做arXiv，然後就不審了立刻公開，然後你就可以讓全世界的人，看到你的文章。", "label": 1}
{"text": "雖然arXiv連結在某些人眼中不夠正式，但許多重要論文尚未發表於國際會議，而我優先選擇arXiv連結，因為這是目前學術界普遍使用的平台，國際會議則更像是經典論文的彙整，我幾乎都在arXiv上閱讀論文，所以看到arXiv連結，我的反應是：「原來發表在這啊！」", "label": 0}
{"text": "儘管arXiv連結的正式性常受質疑，許多具影響力的論文並未發表於國際會議，故我偏好使用arXiv連結，因其普及性高，國際會議如今更像成果彙總，我的文獻查閱主要依賴arXiv，得知論文發表於特定會議，僅是額外資訊。", "label": 0}
{"text": "雖然arXiv連結在某些人眼中不夠正式，但許多重要論文尚未發表於國際會議，故若能取得arXiv連結，我會優先使用；畢竟現今學術界多仰賴arXiv平台閱覽論文，國際會議淪為經典成果的回顧，我個人也習慣從arXiv取得論文，所以看到arXiv連結，我反而覺得理所當然。", "label": 0}
{"text": "那有很多人會覺得引用arXiv的連結不夠正式，但是很多重要的文章，其實現在不見得投稿國際會議。但就只有arXiv的連結，所以我會選擇如果找得到arXiv的連結的話，就直接引用arXiv的連結，其實現在大家都在arXiv上看文章，那國際會議現在比較像是經典回顧這樣子，每篇文章我幾乎都在arXiv上看過了。句子說原來你投到這裡啊！這樣的感覺。", "label": 1}
{"text": "arXiv链接的年份和月份信息清晰可见，方便读者迅速了解论文的发表时间，从而更好地把握研究的时代背景。", "label": 0}
{"text": "arXiv链接的数字前两位代表年份，后两位代表月份，方便快速了解论文的上传时间，从而更好地把握研究的发表时间和脉络。", "label": 0}
{"text": "arXiv链接的优势在于其数字编码清晰地显示了文章上传和大致发表的时间，方便读者了解研究成果的发表年份和月份，从而更好地把握其研究背景和时间脉络。", "label": 0}
{"text": "那引用arXiv的連結，還有一個好處就是你可以直接從arXiv的連結看出這篇文章的時間。所以arXiv的連結裡面的數字，前面兩個就是年份，後面兩個就是月份，可以看這個數字就可以知道說這篇文章是在什麼時候被放在arXiv，也就是什麼時候被發表的，可以讓你對於每一個研究，他誕生的時間更有感覺。", "label": 1}
{"text": "接下來，我們將從三個方面深入探討這些AI agent的核心能力：基於經驗的行為調整、外部工具的運用以及規劃能力。首先，我們分析AI agent如何利用過往互動經驗來優化其行為。", "label": 0}
{"text": "接下來，我們將從三個方面深入探討這些AI智能體的核心能力：基於經驗的行為調整、外部工具的使用以及規劃能力。首先，我們分析AI智能體如何利用過往互動經驗來調整其行為模式。", "label": 0}
{"text": "接下來，我們將從三個方面深入探討這些AI智能體的核心能力：基於經驗的行為調整、外部工具的使用以及規劃能力。首先，我們分析AI智能體如何根據過往互動經驗調整其行為模式。其次，探討AI智能體如何尋求外部協助和運用工具。最後，我們評估AI智能體的規劃與執行能力。", "label": 0}
{"text": "好那接下來呢，我們會分三個面向來剖析今天這些AI agent的關鍵能力。那第一個面向是我們要來看這些AI agent，這個AI agent能不能夠根據他的經驗，過去的互動中所獲得的經驗來調整他的行為。第二部分是要講這些AI agent如何呼叫外部的援助，如何使用工具。第三部分要講AI agent能不能夠執行計畫，能不能做計畫。那我們來講一下，AI怎麼根據過去的經驗，或者是環境的回饋來調整他的行為。", "label": 1}
{"text": "AI代理能否從經驗中學習並調整行為？例如，一個AI程式設計師在編寫程式時遇到編譯錯誤，它能否根據錯誤訊息修正程式碼？", "label": 0}
{"text": "AI代理需具备根据经验调整行为的能力，例如，若一位AI程序员在编写程序时遇到编译错误，它应能根据错误信息修正代码。", "label": 0}
{"text": "AI代理如何從錯誤中學習呢？例如，一個AI程式設計師在編寫程式時遇到編譯錯誤，它應該如何利用錯誤訊息改進程式碼？", "label": 0}
{"text": "那AI呢，AI agent需要能夠根據經驗來調整行為，比如說有一個作為AI programmer的AI agent，他一開始接到一個任務，那他寫了一個程式，那這個程式compile以後有錯誤訊息，compile以後有error，那應該要怎麼辦呢？他應該要能夠根據這個error的message，來修正他之後寫的程式。", "label": 1}
{"text": "以往，談到收到回饋後的應對措施，大多數機器學習課程都建議調整參數，利用收集到的訓練數據，甚至採用強化學習演算法來優化參數。", "label": 0}
{"text": "以往，談及收到回饋後的應對措施，大多數機器學習課程都建議透過調整參數來處理，例如運用強化學習演算法，根據收集到的訓練數據微調參數。", "label": 0}
{"text": "以往，談及收到回饋後的應對措施，大多數機器學習課程都建議透過調整參數來處理，例如運用強化學習演算法，根據收集到的訓練數據調整模型參數。", "label": 0}
{"text": "那在過去啊，講到說你收到一個feedback接下來要做什麼的時候，也許多數機器學習的課程，都是告訴你來調整參數，根據這些收集到的訓練資料，也許使用reinforcement learning的algorithm來調整參數。", "label": 1}
{"text": "然而，需再次強調，本課程並未涉及任何模型訓練，故不採用該方法。那麼，不調整模型參數，如何改變其行為？基於現有大型語言模型的能力，無需微調參數，只需提供錯誤訊息即可改變其後續程式碼輸出，如此便可解決問題。或許您會疑問，為何提供錯誤訊息後，程式碼就能修正？答案是：模型的本質是文字預測，不同的輸入將產生不同的輸出；之前的錯誤程式碼，源於輸入資訊不足。", "label": 0}
{"text": "然而，需再次強調本課程並未涉及任何模型訓練，故不採用此方法。那麼，若不更新模型參數，模型如何改變行為？現今大型語言模型只需輸入錯誤訊息即可改變其行為，無需微調參數，過程就此結束。或許您會疑問，既是同一模型，為何輸入錯誤訊息後程式碼正確？其實模型僅是進行文字接龍，不同的輸入自然產生不同的輸出，初始程式碼錯誤僅因輸入資訊不足。", "label": 0}
{"text": "然而，需再次強調，本課程並未涉及任何模型訓練，故不採用此方法。那麼，不調整模型參數，如何改變其行為？現今大型語言模型的功能已足以應付：無需微調參數，直接提供錯誤訊息即可改變其後續程式碼輸出，方法如此簡單。或許您會疑惑，既是同一個模型，為何提供錯誤訊息後程式碼就能修正？原因在於模型運作機制類似文字接龍，輸入不同，輸出自然不同。初始錯誤程式碼的產生，源於模型當時的輸入資訊有限所致。", "label": 0}
{"text": "但不要忘了我們剛才就強調過，在這一堂課裡面沒有任何模型被訓練，所以我們今天不走這個路線。那不更新模型的參數，模型要怎麼改變它的行為呢？依照今天 Large Language Model 的能力，要改變它的行為，你也不用微調參數，直接把錯誤的訊息給他，他接下來寫的程式就會不一樣了，就結束了。那可能會問說，那之前他寫的程式是錯的，為什麼給錯誤訊息，他寫的程式就對了呢？明明就是同一個模型，但你想想看模型做的事情就是文字接龍，你給他不同的輸入，他接出來的東西就不一樣，一開始會寫錯的程式，是因為他前面要接的部分只有這麼多，所以寫個錯的程式。", "label": 1}
{"text": "若輸入資料含有錯誤，模型產出的結果反而可能正確；大量證據顯示，語言模型能根據使用者回饋調整行為，無需重新設定參數；任何使用過語言模型的人都能直覺感受到這點。", "label": 0}
{"text": "然而，輸入錯誤資訊反而可能讓語言模型產生正確結果，大量證據顯示它們能根據使用者回饋調整行為，無需更改參數；任何使用過這些模型的人都能直觀感受到這點。", "label": 0}
{"text": "若輸入資料有誤，模型產出反而可能正確；大量證據顯示，語言模型能透過使用者回饋調整行為，無需參數調整；有使用經驗者應能直覺理解此現象。", "label": 0}
{"text": "當今天要接的內容，包含了錯誤的訊息的時候，他接出來的結果可能就會是正確的了。那今天已經有太多的證據，說明這些語言模型，可以根據你給他的回饋，改變他的行為，不需要調整參數，那如果你有使用這些語言模型的經驗，你也不會懷疑他們有根據你的回饋調整行為的能力。", "label": 1}
{"text": "核心問題在於，若語言模型儲存所有過往經驗以調整行為，則隨著經驗累積，龐大的資訊量將使其在決策時不堪負荷，最終影響準確性。", "label": 0}
{"text": "核心問題在於，若語言模型儲存所有過往經驗以調整行為，其運算量將隨時間呈指數級增長，最終導致其無法有效處理龐大的資訊，影響決策準確性。", "label": 0}
{"text": "核心問題在於，若讓語言模型儲存所有過往經驗以調整行為，則隨著經驗累積，模型將因龐大的資訊量而難以有效率地進行決策，最終可能導致效能下降。", "label": 0}
{"text": "那這邊真正的議題是，如果我們是把過去所有的經驗都存起來，要改變語言模型的行為，要讓他根據過去的經驗調整行為。就是把過去所有發生的事情一股腦給他，那就好像是語言模型每次做一次決策的時候，他都要回憶他一生的經歷，也許在第100步的時候還行，到第1萬步的時候，過去的經驗太長了，他的人生的資訊已經太多了，也許他沒有足夠的算力，來回顧一生的資訊，他就沒有辦法得到正確的答案。", "label": 1}
{"text": "擁有超群記憶力的人，能完整回憶一生經歷，甚至能準確記住任何時間發生的任何細節，包括他人的電話號碼。", "label": 0}
{"text": "擁有超強自傳式記憶的人，能鉅細靡遺地回憶人生中的每件事，甚至能記住任何時間發生的任何細節，包括早已遺忘的電話號碼。", "label": 0}
{"text": "擁有超強自傳體記憶的人，能鉅細靡遺地回憶一生經歷，甚至能記住任何時間點發生的細節，以及任何人的電話號碼。", "label": 0}
{"text": "這讓我想到什麼呢，這讓我想到有一些人有超長自傳式記憶，他可以把他一生中，所有發生的事情記下來，然後那些人你可以隨便問他一個，某個人的電話號碼，他都會背出來，你告訴他某年某日某時發生了什麼事，他也都可以講出來。", "label": 1}
{"text": "擁有超憶症，即所謂的超長自傳式記憶，的人寥寥無幾，他們的大腦如同精密複印機般儲存所有經歷，然而，這非同尋常的記憶能力，卻並非恩賜，而是被醫學界認定為一種罕見疾病，其患者的生活品質大受影響，難以從瑣碎的細節中抽離，進行抽象思考，甚至難以快樂生活。", "label": 0}
{"text": "擁有超憶症，即所謂的「超長自傳式記憶」，的人寥寥無幾，他們能鉅細靡遺地記住人生經歷，然而，這項能力非但不是恩賜，反而讓他們飽受困擾，難以進行抽象思考，並深陷於無止盡的回憶中，嚴重影響日常生活，這在2006年才被學界正式認定為一種疾病。", "label": 0}
{"text": "擁有過目不忘能力的人，其超憶症不僅非祝福，反而是種罕見疾病，全球病例可能不到百例，2006年才被學界正式記載，患者因持續不斷的回憶而苦不堪言，難以抽離過往細節，甚至無法進行抽象思考。", "label": 0}
{"text": "有一些人他的頭腦就像是一個影印機一樣，會把所有他看過的事情都原封不動的記憶下來，但這種超長自傳式記憶啊，又被叫做超憶症。你看到症這個字就知道說，人們覺得這是一種疾病，這聽起來記憶力很好是一種祝福，但實際上對這些患者而言，據說這種患者世界上可能不到100例。那這是一個2006年的時候，才被論文發表的一個症狀，那據說這些患者其實日常生活並沒有辦法過得很開心。因為他們不斷的在回憶他的人生，往往一不小心就陷入了一個冗長的回憶之中，那也很難做抽象的思考，因為他的人生已經被他的記憶已經被太多，知為末節的所事所佔據，所以沒有辦法做抽象式的思考。", "label": 1}
{"text": "讓AI持續儲存所有經歷恐非良策，龐大的記憶體反而可能導致其決策失誤，故需設計類似人類長期記憶的機制，擇要存儲重要事件。", "label": 0}
{"text": "長期記憶的累積可能導致AI代理在漫長生命歷程中決策失誤，因此，我們需要設計一種機制，例如類比人類長期記憶的存儲方案，來管理AI的經驗積累。", "label": 0}
{"text": "持續累積所有經驗恐讓AI難以做出正確判斷，甚至因資訊過載而失效；因此，我們需要設計類似人類長期記憶的機制，讓AI能有效儲存和運用過往經驗。", "label": 0}
{"text": "所以讓一個AI agent記住他一生所有經歷的事情，告訴他你每次做一個決策的時候，都是根據你一生所有經歷過的事情再去做決策，那也許對AI agent來說並不是一件好事。最終當他的人生過長的時候，他會沒有辦法做出正確的決策，所以怎麼辦呢？也許我們可以給這些AI agent memory，這就像是人類的長期記憶一樣，發生過的事情我們把它存到這個memory裡面。", "label": 1}
{"text": "AI agent在處理第一萬個觀察結果時，並非參考所有記憶內容來決定下一步行動，而是透過一個「閱讀」模組從記憶中篩選與當前問題相關的經驗，將這些經驗優先呈現給模型，輔助模型結合觀察結果，如同文字接龍般推導出最適當的行為。此「閱讀」模組有效過濾長期記憶，使模型僅根據相關資訊決策。", "label": 0}
{"text": "AI agent在處理第一萬個觀察時，並非直接運用全部記憶內容決定下一步行動，而是透過一個「讀取」模組，篩選出與當前問題相關的過往經驗，優先整合至觀察數據中，再據此決定行動，有效利用長期記憶中的關鍵資訊進行決策。", "label": 0}
{"text": "AI agent在處理第一萬個觀察時，並非參考所有記憶內容決定行動，而是透過一個「閱讀」模組從記憶中篩選出與當前問題相關的經驗，優先考量這些經驗與觀察結果，再決定下一步行動，藉此有效利用長期記憶中的重要資訊。", "label": 0}
{"text": "當AI agent看到，第一萬個observation的時候，他不是根據所有存在memory裡面的內容，去決定接下來要採取什麼action。而是有一個叫做read的模組，這個read的模組會從memory裡面，選擇跟現在要解決的問題有關係的經驗，把這些有關係的經驗放在observation的前面。讓模型根據這些有關係的經驗跟observation，再做文字接龍，接出它應該進行的行為，那你有這個read的模組就可以從memory裡面，從長期記憶中篩選出重要的訊息，讓模型只根據這些跟現在情境相關的訊息來進行決策。", "label": 1}
{"text": "構建這個read模組的方法，如同建立一個檢索系統：輸入的問題即為觀察值，模型的長期記憶體即為資料庫。系統根據問題從資料庫中檢索相關資訊，此技術本質上即為RAG，所有RAG方法皆可直接應用於此。", "label": 0}
{"text": "構建read模組的方法，如同開發一個檢索系統：輸入的觀察值即為問題，模型的長期記憶體即為資料庫。該系統根據問題從資料庫中檢索相關資訊，其技術本質上與RAG相同，可以直接套用任何RAG方法。", "label": 0}
{"text": "構建這個read模組的方法，就是將其視為一個資訊檢索系統：輸入的問題相當於檢索查詢，模型的長期記憶體則充當資料庫。  系統根據問題從資料庫中提取相關資訊，此技術本質上就是RAG，任何RAG方法都可直接應用於此。", "label": 0}
{"text": "那怎麼樣打造這個read的模組呢？其實你可以想這個read的模組，就想成是一個retrieval的system，想成是一個檢索的系統，那第一萬步看到的observation其實就是問題，那模型的AI agent的memory長期記憶，其實就是資料庫。那你就把拿這個檢索系統，根據這個問題從這個資料庫裡面，檢索出相關的資訊，那這整個技術跟RAG沒有什麼不同，其實它就是RAG，你可以直接把RAG的任何方法，直接套用到這個地方。", "label": 1}
{"text": "AI agents and RAG systems utilize similar search technologies, but their memories differ: RAG accesses the collective knowledge of the internet, while an AI agent's memory comprises its own unique experiences.", "label": 0}
{"text": "AI agent與RAG系統的關鍵差異在於記憶體內容的來源：RAG使用的是網路上的公開資訊，而AI agent則使用自身的經驗，儘管兩者都可運用相同的搜尋技術。", "label": 0}
{"text": "AI agent與RAG的關鍵差異在於記憶來源：RAG使用的是外部網路數據，而AI agent則使用自身的經驗數據；儘管來源不同，但兩者皆可運用相同的搜尋技術。", "label": 0}
{"text": "唯一不一樣的地方只是，如果是RAG的話，存在memory裡面的東西等於是整個網路，那是別人的經驗。而對AI agent而言，現在存在memory裡面的東西，是他自己個人的經歷，差別的是經歷的來源，但是用來搜尋的技術，是可以完全直接線套RAG的技術。", "label": 1}
{"text": "研究AI agent基於經驗調整行為，可使用Streambench基準測試，其包含一系列問題，AI依序解答並接收二元回饋（正確或錯誤），藉此修正行為，提升後續問題的準確率，最終以平均準確率評估其經驗學習能力，學習能力越強的agent，應能在更短時間內、更少回饋下達到更高的平均準確率。", "label": 0}
{"text": "欲評估AI agent基於經驗調整行為的能力，可使用Streambench基準測試，其包含一系列問題，AI agent依序解答並接收二元回饋（正確或錯誤），藉此修正其行為，提升後續問題的解答準確率；最終以整體平均準確率評估其經驗學習效率，學習效率高的agent應能在較少回饋下快速提升準確率。", "label": 0}
{"text": "欲評估AI代理根據經驗調整行為的能力，可使用Streambench基準測試，其包含一系列問題，AI代理依序解答並接收二元回饋（正確或錯誤），藉此修正行為，提升後續問題的準確性，最終以平均正確率衡量其經驗學習效率，學習效率高的代理應能在較短時間內，透過較少回饋達到更高的準確度。", "label": 0}
{"text": "呃,如果你今天想要研究這個AI agent按照經驗來修改他的行為,那你可以考慮一個叫做streambench的benchmark,那在streambench裡面呢,會有一系列的問題,然後呢,AI會依序去解這些問題,他先解第一個問題,得到第一個問題的答案,然後接下來他會得到第一個問題答案的反饋,那在這個streambench目前的，因為所有的問題都是有標準答案的,所以AI agent得到的回饋是binary的,就是對或者是錯。好,那根據他過去的經驗,他就可以修正他的行為，期待他在第二個問題的時候,可以得到更準確的答案，得到更高的正確率,然後這個過程就一直持續下去。那假設有1000個問題的話,那就等AI agent回答完最後問題的時候，這個互動就結束了，那最後結算一個，根據經驗學習能力的好壞，根據經驗調整行為能力的好壞，那就看這一整個回答的過程中平均的正確率，越能夠根據經驗學習的agent，他應該能夠用越少的時間，看過越少的回饋，就越快能夠增強他的能力，就可以得到比較高的平均的正確率。", "label": 1}
{"text": "该基准测试由API的研究人员开发，其基线使用了类似RAG的技术，避免了将所有先前问题一股脑输入模型，因为过长的序列会超出大多数语言模型的处理能力。", "label": 0}
{"text": "此基準測試由API研究團隊開發，其基準模型已採用類似RAG的技術；換言之，模型並非將前99個問題一次性輸入以進行處理，因為如此長的序列會超出一般語言模型的處理能力。", "label": 0}
{"text": "该基准测试由API的研究人员开发，其基线使用了类似RAG的技术，避免将所有先前问题一次性输入模型，因为长序列会超出大多数语言模型的处理能力。", "label": 0}
{"text": "那這個benchmark呢，是API的研究人員打造的一個benchmark。那在這個benchmark裡面的baseline，就是有使用到我剛才講的類似RAG的技術，也就是說當模型在回答第100個問題的時候，他並不是把前面第一個到第99個問題，通通丟給他去做文字接龍，這樣這個sequence太長了，一般的語言模型根本讀不了這麼長的輸入。", "label": 1}
{"text": "核心技術在於一個搜尋模組，它篩選過去經驗中與當前問題相關的部分，僅以此為基礎，語言模型才能生成答案，其效果顯著。", "label": 0}
{"text": "核心技術在於一個搜尋模組，它能從過往經驗中篩選出與當前問題相關的資訊，語言模型僅基於這些資訊和問題本身來生成答案，此方法極其有效。", "label": 0}
{"text": "核心方法是運用一個搜尋模組，篩選出與當前問題相關的過往經驗，再讓語言模型僅基於這些經驗和問題本身來生成答案，此法極其有效。", "label": 0}
{"text": "所以實際上的做法，就是你需要有一個檢索的模組。這個檢索的模組，只從過去所有的經驗中，檢索出跟現在要回答的問題有關係的經驗，然後語言模型只根據這些有關係的經驗還有現在的問題，來進行回答，來產生他的行動，來產生他的答案。那這一招有沒有用呢？這一招其實非常的有用。", "label": 1}
{"text": "此圖橫軸為「時間步」，代表1750個問題；縱軸為平均準確率。灰色線為模型未經訓練，每個問題獨立作答的基準線；黃色線則代表模型僅參考固定五個問題作答的結果。", "label": 0}
{"text": "此圖橫軸代表1750個問題（以time step表示），縱軸則為平均正確率。灰色線為模型未經訓練，每次回答互不相關時的基準正確率；黃色線則顯示僅參考固定5個問題進行回答時的表現。", "label": 0}
{"text": "此圖橫軸代表1750個問題（time step），縱軸則為平均準確率。灰色線為模型未經訓練，隨機作答的基準線；黃色線則顯示模型僅參考固定五個問題作答的結果。", "label": 0}
{"text": "那在這一頁圖裡面橫軸啊，他這邊的用詞是time step，但其實指的就是一個一個的問題，總共有1750幾個問題。那縱軸指的是平均的正確率，那在這個圖上面呢，最低的這條灰色線指的是說，假設沒有讓模型做任何學習，他回答每一個問題都是independent的，回答問題間沒有任何的關聯，他完全沒有調整他的行為，那你得到的正確率是灰色的這條線是最低的。那黃色這條線是說，只固定隨機選五個問題，那每次模型回答問題的時候，都是固定看那五個問題來回答，都是固定把五個問題當作經驗來回答，那也可以得到的是黃色這一條線。", "label": 1}
{"text": "採用RAG方法從記憶體中篩選出與當前問題最相關的經驗，能提升準確率，效果優於僅依靠單一方法，論文中詳述了最佳方案（紅色線）的實現細節，詳見Streambench。", "label": 0}
{"text": "採用RAG方法從記憶體中篩選出與當前問題最相關的經驗，能獲得比僅依賴單一知識庫更高的準確率，最終效果最佳的方案詳見Streambench論文。", "label": 0}
{"text": "採用RAG方法，從記憶庫中選取與當前問題最相關的經驗，可獲得比僅使用單一方法更高的準確率，最終效果最佳的方案詳見Streambench論文。", "label": 0}
{"text": "那如果你是用RAG的方法，從一個memory裡面去挑選出最有關係的問題，跟現在要解決的問題最有關係的經驗，那你可以得到的是粉紅色的這一條線。那可以看到比黃色的線，那正確率還要高上不少，那最後結果最好的是紅色這一條線啦。那這個怎麼做的，那大家就自己再去詳細閱讀論文，那在streambench裡面呢。", "label": 1}
{"text": "實驗顯示，正面回饋比負面回饋更有效提升語言模型的表現，提供正確答案範例的訓練效果優於錯誤答案範例。", "label": 0}
{"text": "值得注意的是，負面回饋對現階段的語言模型幫助不大；相較於提供錯誤案例，提供正確案例更能有效引導模型學習，提升其準確性。", "label": 0}
{"text": "有趣的是，針對現階段語言模型，負面回饋幾乎無效。因此，想修正模型行為，提供正面範例比負面範例更有效：給予正確解答的案例，比提供錯誤解答的案例，更能引導模型學習。", "label": 0}
{"text": "還發現一個有趣的現象是值得跟大家分享，這個現象是負面的回饋。基本上沒有幫助對現階段的語言模型而言，所以你要提供給語言模型經驗，讓他能夠調整他行為的時候，給他正面的例子，比給他負面的例子要好。也就是說具體而言，提供給他過去哪些類似的問題得到正確答案，比提供給他過去哪些問題得到錯誤的答案，還更有效，還更能引導模型得到正確的答案。", "label": 1}
{"text": "StreamBench平台上的多個數據集驗證了我們的實驗結果：縱軸0代表無經驗調整，藍色則代表使用所有正負樣例，後者通常能提升模型效能，個別例外情況存在；而僅使用負樣例則無益甚至有害。", "label": 0}
{"text": "實驗在多個StreamBench數據集上驗證，結果顯示：未經經驗調整的模型表現為基準(0)；使用正負樣例訓練的模型(藍色)大多表現優異，少數例外情況存在；僅使用負樣例訓練則無益甚至有害。", "label": 0}
{"text": "實驗在多個StreamBench內建資料集上驗證，結果顯示：未經經驗調整的模型表現為基準(0)；使用正負樣例訓練的模型(藍色線)大多優於基準，少數例外情況除外；僅使用負樣例訓練則無助益，甚至有害。", "label": 0}
{"text": "那這邊是真正的實驗結果，做在好幾個不同的data set上面，streambench裡面本來就包含了好幾個不同的data set。那這個縱軸呢，0代表完全沒有做，完全沒有根據經驗調整行為，然後藍色代表說，不管是正面還是負面的例子都用，如果不管正面還是負面的例子都用。在多數情況下，模型都可以表現得比較好，當然有一些例外，但是如果只用負面的例子呢？如果只用負面的例子，基本上是沒有幫助，而且甚至是有害的。", "label": 1}
{"text": "積極強化模型的訓練，僅使用正面案例，確實能提升效能，此結果與既有研究結論相符，已有研究證實，引導語言模型的最佳策略是明確指示其目標，而非限制其行為；例如，直接要求模型「簡潔扼要」比避免「冗長贅述」更有效，因為明確的指令比含糊的限制更容易被模型理解和執行。", "label": 0}
{"text": "正面案例的有效性已得到既有研究支持，模型訓練應著重於指示模型「做什麼」，而非限制其「不做什麼」，例如，直接要求模型「簡潔扼要」比避免「冗長贅述」更有效。", "label": 0}
{"text": "正面案例的有效性已在先前研究中得到印證，模型訓練中，積極引導比消極限制更有效，例如，直接要求模型「寫短一點」比要求它「不要寫太長」更能達到預期效果，因為後者容易造成理解上的偏差。", "label": 0}
{"text": "那如果說只用正面的例子，在所有的情況下，模型可以得到更好的結果，那這也符合過去的一些研究。有人研究過使用語言模型要怎麼樣比較有效，有一個發現就是與其告訴語言模型不要做什麼，不如告訴他要做什麼，如果你還希望他文章寫短一點，你要直接跟他說寫短一點，不要告訴他不要寫太長。比較他不要寫太長，他不一定聽得懂，叫他寫短一點，比較直接他反而比較聽得懂。", "label": 1}
{"text": "Streambench的研究結果同樣表明，糾正錯誤不如提供正確示範有效。", "label": 0}
{"text": "Streambench的研究結果同樣顯示，糾正錯誤不如提供正確示範有效。", "label": 0}
{"text": "Streambench的研究結果同樣印證了這一點：提供正確示範比糾正錯誤更有效提升語言模型的表現。", "label": 0}
{"text": "這也符合這邊這個Streambench的發現，就是負面的例子比較他沒有效，與其給語言模型告訴他什麼做錯，不如告訴他怎麼做是對的。", "label": 1}
{"text": "關於記憶模組，我們先前討論了read功能，但所有資訊都需要儲存到長期記憶庫嗎？如果將agent所有經歷都儲存，記憶庫將充斥無關緊要的細節，最終導致記憶體溢位。", "label": 0}
{"text": "關於記憶模組，我們之前提到了read函式，但所有資訊都需要儲存到長期記憶庫嗎？如果將agent的所有經歷都儲存，記憶庫將充斥大量無關緊要的資訊，最終導致記憶體溢位。", "label": 0}
{"text": "關於記憶模組，我們之前提到了read模組，但將agent所有經歷都儲存到長期記憶庫中是否可行？如此一來，記憶庫恐將充斥無關緊要的資訊而導致容量耗盡。", "label": 0}
{"text": "好 那我們剛才講到了，有一個read的模組，那有關記憶的部分呢，是不是要把所有所有的資訊，通通存到memory裡面呢？存到長期的記憶庫裡面呢？如果我們把這些agent，經歷的所有的事情，都放到長期的記憶庫裡面的話，那裡面可能會充斥了一堆雞毛算皮不重要的小事，最終你的memory長期記憶庫可能也會被塞爆。", "label": 1}
{"text": "AI村民通常只记录些无关紧要的琐碎信息，例如桌子、椅子等，日志中充斥着大量的无效数据，导致内存被无用信息占据，效率低下。因此，需要一种更有效的机制来筛选和存储重要信息。", "label": 0}
{"text": "AI村民的觀察大多是無足輕重的瑣事，例如桌子、椅子等等，記錄這些資訊只會讓記憶體塞滿無用數據，造成資源浪費。因此，需要一套更精準的機制，篩選並儲存重要的資訊。", "label": 0}
{"text": "AI村民的日常觀察大多是無足輕重的瑣事，記錄的日誌也多半是些無關緊要的內容，例如「那邊有一張桌子」、「那邊有一張椅子」，充斥著無用資訊。因此，記憶體被大量無意義的細節佔據。  如何有效篩選並僅記錄重要資訊，才是關鍵。", "label": 0}
{"text": "如果說你是做那種AI村民啊，AI村民他多數時候觀察到的資訊都是些無關緊要的小事，那如果你看他觀察到那個log，多數都是啥事也沒有。就那邊有一張桌子啥事也沒有，那邊有一張椅子啥事也沒有，多數時候都是啥事也沒有，所以如果把所有觀察到的東西，都記下來的話，那你的memory裡面就都只是被一些雞毛算皮的小事佔據。所以怎麼辦呢？也許應該有更有效的方式，來決定什麼樣的資訊，應該被記下來，應該只要記重要的資訊就好。", "label": 1}
{"text": "如何精簡語言模型的記憶，使其僅儲存關鍵資訊？  可設計一個寫入模組，篩選並將重要資訊存入長期記憶庫，而忽略無關緊要的資訊。", "label": 0}
{"text": "如何精簡語言模型的記憶，只儲存關鍵資訊？可設計一個「寫入模組」，負責篩選資訊，決定哪些內容存入長期記憶庫，哪些則捨棄。", "label": 0}
{"text": "如何精簡語言模型的記憶，使其僅儲存關鍵資訊？  可設計一個寫入模組，篩選並儲存重要資訊至長期記憶庫，而忽略無關緊要的資訊。", "label": 0}
{"text": "那怎麼讓語言模型只記重要的資訊就好呢？你可以有一個write的module，那write的module決定，什麼樣的資訊要被填到長期的記憶庫裡面，什麼樣的資訊乾脆直接就讓他隨風而去就好了。", "label": 1}
{"text": "如何建構write的知識庫？方法很簡單，將write模組視為一個語言模型，甚至視為AI代理本身。此AI代理會根據觀察到的資訊，自我提問：這件事值得記錄嗎？值得則記錄，否則捨棄。", "label": 0}
{"text": "如何建立write的記憶庫？只需讓write模組（本身也是個語言模型，甚至就是AI代理）根據觀察內容，自行判斷資訊重要性，決定是否儲存。", "label": 0}
{"text": "如何建立write的知識庫？方法很簡單，將write模組視為一個語言模型，甚至一個AI代理。這個AI代理會根據觀察到的內容，自行判斷資訊重要性，決定是否儲存。", "label": 0}
{"text": "那怎麼樣打造這個write的記憶庫呢？有一個很簡單的方法就是，write的模組也是一個語言模型，甚至就是AI agent自己。這個AI agent他要做的事情，就是根據他現在觀察到的東西，然後問自問一個問題，這件事有重要到應該被記下來嗎？如果有就把它記下來，如果沒有就讓他隨風而去。", "label": 1}
{"text": "除了RE和Write模組，還有一個暫且稱為「反射」模組的未命名模組。", "label": 0}
{"text": "除了RE和Write模組，還存在一個暫名為「reflection」的第三個模組，其名稱並非固定。", "label": 0}
{"text": "除了RE和Write模組，還存在一個功能模組，目前暫且稱之為「reflection」模組，其名稱並未在文件中固定。", "label": 0}
{"text": "那除了RE跟Write這兩個模組以外，還有第三個模組。沒有固定的名字啦，在文件上的名字，沒有固定的名字，我們可以暫時叫他reflection反思的模組。", "label": 1}
{"text": "此模組的功能是提升記憶資訊的處理效率，以更抽象、更宏觀的層次重新組織記憶內容。經反思模組重新審視後，可產生新的洞見，進而引導閱讀模組更有效率地搜尋資訊，提升經驗品質，最終輔助模型做出更優質的決策。", "label": 0}
{"text": "此模組的功能在於更高層次地重新組織記憶資訊，透過反思模組的重新審視，產生新的見解，進而引導閱讀模組更有效率地搜尋，提升模型決策品質。", "label": 0}
{"text": "此模組的功能在於更高層次地、更抽象地重新組織儲存的資訊，透過反思模組重新審視記憶內容，產生新的見解，進而引導閱讀模組更有效率地搜尋資訊，提升模型決策品質。", "label": 0}
{"text": "那這個模組的工作是，對記憶中的資訊做更好的，更high level的，可能是抽象的重新整理，你可以把這些記憶裡面的內容，在經過reflection的模組，重新反思之後得到新的想法。那也許read的模組可以根據這些新的想法，來進行搜尋，這樣子也許可以得到更好的經驗，那幫助模型做出更好的決策。", "label": 1}
{"text": "此模組或為語言模型，亦即AI自身，可將過往記憶輸入，讓其分析並得出新見解，例如：若AI觀察到某人每日同乘公車且今日對其微笑，則其內部模型可能推論出對方對其有意思（此為常見錯覺）。如此便能產生新想法，作為決策依據，即便這些想法並非直接觀察所得，而是基於推論。此外，此模組還能建立經驗間的關聯，形成知識圖譜，供後續資訊搜尋使用。", "label": 0}
{"text": "藉由內建的AI模型分析過往經驗，例如觀察到某人每天搭乘同一班公車且今日對你微笑，系統便能推論出對方可能對你有好感（這可能是錯覺）。此模型能從既有記憶中挖掘新見解，建立經驗間的關聯，形成知識圖譜，進而輔助決策，即使這些見解並非直接觀察所得，而是基於推論而來。", "label": 0}
{"text": "藉由反思模組（也許也是個語言模型或AI代理），將過往記憶輸入，讓其分析並得出新見解。例如，觀察到「喜歡的對象每天搭乘同一班公車」和「對方今天對我微笑」，反思模組可能推論出「對方喜歡我」，這可能是錯覺，但能產生新的想法，輔助決策。此外，此模組還能建立經驗間的關聯，形成知識圖譜，供後續資訊搜尋使用。", "label": 0}
{"text": "而這個reflection的模組，可能也是一個語言模型，就是AI agent自己，你可以只是把過去的這一些記憶，丟給reflection的模組，然後叫reflection模組想一想，看他從這些記憶裡面，能不能夠有什麼樣新的發現。比如說可能有一個observation是，我喜歡的疫情每天都跟我搭同一部公車，另外observation是他今天對我笑了，那你推出來的reflection模型，結果就說他喜歡我這樣，一個錯覺，人生三大錯覺之一就是這一種。就得到一些新的sort，你就得到一些新的想法，那你之後在做決策的時候，就可以用這些新的想法，雖然你沒有實際觀察到，但它是被推論出來的，根據這些推論出來的想法來做決策。那除了產生新的想法之外，也可以為以前觀察到的經驗，建立經驗和經驗之間的關係，也就是建立一個，然後讓reader的module根據這個knowledge graph來找相關的資訊。", "label": 1}
{"text": "運用知識圖譜於RAG已成顯學，Graph RAG系列研究即為最佳例證；將資料庫轉化為知識圖譜，藉此提升搜尋與問答效率，是目前RAG的常見且有效方法。", "label": 0}
{"text": "運用知識圖譜於RAG已是常見且有效率的方法，Graph RAG系列研究便是最佳範例，其透過將資料庫轉化為知識圖譜，提升搜尋及問答效率。", "label": 0}
{"text": "利用知識圖譜增強RAG已成為常見做法，Graph RAG系列研究堪稱代表作，其核心是將資料庫轉化為知識圖譜，藉此提升搜尋和問答效率。", "label": 0}
{"text": "那我知道在RAG的領域使用knowledge graph，現在也是一個非常常見的手法，那最知名的可能就是graph RAG系列這個研究。就把你的資料庫，把它變成一個knowledge graph，那今天在搜尋跟回答問題的時候，是根據knowledge graph來搜尋回答問題，可以讓RAG這件事做得更有效率。", "label": 1}
{"text": "HIPO RAG中的「HIPO」並非指河馬，而是指海馬迴，人腦中負責記憶的結構；作者認為此知識圖譜的建構方式與海馬迴運作機制相似，故以此命名。", "label": 0}
{"text": "HIPO RAG中的「HIPO」並非指河馬，而是指與大腦海馬迴結構類似的知識圖譜建構方式。", "label": 0}
{"text": "HIPO RAG中的「HIPO」並非指河馬，而是指與人腦海馬迴結構類似的知識圖譜構建方式。", "label": 0}
{"text": "或是另外一個非常類似的例子，那HIPO RAG，這個HIPO不是指真正的河馬。他指的應該是那個海馬迴，那個人腦中的一個結構，然後他覺得做建這種knowledge graph，就跟海馬迴的運作呢，非常的類似，所以他叫做HIPO RAG。", "label": 1}
{"text": "藉由反射模組將經驗轉化為圖形，即可應用現有的圖形相關 RAG 方法於 AI 代理程式，這充分展現 OpenAI 將 ChatGPT 打造為 AI 代理程式的企圖心，因為 ChatGPT 顯然已具備記憶功能。", "label": 0}
{"text": "藉由反射模組將經驗轉化為圖形，即可運用圖形相關的 RAG 方法於 AI 代理程式中，此技術已應用於 ChatGPT 等具備記憶功能的模型，展現 OpenAI 將 ChatGPT 打造為 AI 代理程式的企圖心。", "label": 0}
{"text": "基於圖形(graph)的RAG技術可經由反射機制建構經驗圖譜，並直接應用於AI agent；ChatGPT現已具備記憶功能，展現OpenAI將其發展為AI agent的企圖心。", "label": 0}
{"text": "有一些跟graph有關的RAG的方法，那你完全可以透過reflection的模組，把經驗建成一個graph以後，把那一些graph RAG的手法，直接套到AI agent裡面。那大家可能都知道說這個ChatGPT啊，現在其實真的是有記憶的，所以可以感受到這個OpenAI，想把ChatGPT變成一個AI agent的決心。", "label": 1}
{"text": "好的，我會把您週五下午的機器學習課程記錄下來。", "label": 0}
{"text": "好的，我會把您週五下午的機器學習課程記錄下來。", "label": 0}
{"text": "好的，我會把您週五下午的機器學習課程記錄下來。", "label": 0}
{"text": "比如說我跟ChatGPT說，我週五下午要上機器學習這門課，那他就給我一個回答，說要我幫助你做什麼事情嗎？接下來我告訴他記下來，你跟他講記下來之後，他的這個write的模組就啟動了。他知道這件事情是要被記下來的，他就會說那我記下來了，以後你週五要上機器學習這門課。那write的模組什麼時候要啟動，是他自己決定的，所以很多時候你希望他記下來的時候，他就是不啟動，或你不希望他啟動的時候，他就是啟動。那個是模型自己決定的，但是有一個方法可以，基本上一定能讓他啟動，就明確的跟他講，把這件事記下來，基本上都幾乎確定能夠啟動那個write的模組，讓write的模組把這件事情記下來。", "label": 1}
{"text": "關於後續資料的存放位置，請至設定中的「個人化」選項，並點選「記憶」下的「管理記憶」功能，即可檢視AI代理程式透過write模組儲存於長期記憶中的所有資訊，例如您曾誤稱其為「血輪眼卡卡」的記錄。", "label": 0}
{"text": "關於接下來的內容，請前往設定中的「個人化」選項，找到「記憶管理」功能，即可查看AI的長期記憶。這些記憶是透過write模組儲存的，例如，第一筆記錄顯示你曾稱呼它為「血輪眼卡卡」，此後它便將自己與此身份產生連結。", "label": 0}
{"text": "關於後續資料的存放位置，請至設定中的「個人化」選項，並點選「記憶」下的「管理記憶」功能，即可檢視AI模型的長期記憶，其中包含您與AI互動的紀錄，例如您曾自稱「血輪眼卡卡」，導致模型產生相關聯想。", "label": 0}
{"text": "那接下來的東西在哪裡呢？你可以看在設定裡面，有一個個人化，然後有一個叫記憶的部分，那你點這個管理記憶，就可以看到確記憶。他透過write的模組，寫在他的memory裡面，這個就是他作為一個AI agent的長期記憶，裡面的東西，比如第一條是你叫做血輪眼卡卡，有一次不小心跟他說你是卡卡，不知道為什麼他就覺得自己是血輪眼卡卡。", "label": 1}
{"text": "此外，他確實記住了我先前告知他週五下午有機器學習課程。然而，模型的記憶並非完美無缺，因為模型自行決定儲存哪些資訊，且並非直接記錄對話內容，而是經過提煉和反思後才儲存，故反思過程可能存在偏差。", "label": 0}
{"text": "哦，他確實記住了我說的週五下午的機器學習課，然而，模型的記憶並不可靠，因為模型自主決定儲存內容的方式，並非直接記錄對話，而是經過加工和反思後才儲存，所以反思過程可能存在錯誤。", "label": 0}
{"text": "他記得我先前告知他週五下午有機器學習課程，然而，模型的記憶並不完美，因為模型自主決定記憶內容，並非直接儲存對話，而是經過其自身詮釋與反思後才儲存，故此詮釋可能存在錯誤。", "label": 0}
{"text": "然後呢，他也記得就我剛才跟他講的，週五下午要上機器學習這門課。但是呢，但是其實模型的記憶也是會出錯的。因為要寫什麼樣的東西到記憶裡面，是模型自己決定的，而且他並不是把對話的內容，就一五一十的直接放到記憶裡面，他是經過一些昇華反思之後才放進去的，所以他的反思可能會出錯。", "label": 1}
{"text": "他把我誤認為台大學生，其實我是老師，可能是過去的談話造成他的誤解，所以記錯了。他習慣性地記錄我所有的演講和教學活動，即使資訊有誤也照單全收。", "label": 0}
{"text": "他把我誤認為台大學生，其實我是老師，這源於之前的談話產生了認知偏差，他便將此錯誤資訊儲存起來。他習慣性地記錄所有他想記的資訊，例如我的演講和教學內容。", "label": 0}
{"text": "他誤以為我是台大學生，其實我是老師，這源於過去的對話造成認知偏差；他腦中儲存了許多資訊，包含我的演講和教學內容，但他記錯了我的身份。", "label": 0}
{"text": "比如說他覺得我是一個臺灣大學的學生，雖然我是老師。但是他從過去的對話誤以為我是一個學生，所以就存了一個錯誤的資訊。在他的記憶裡面，一堆他想記的東西，比如說我給過什麼演講，給過什麼tutorial，他都把它記下來就是了。", "label": 1}
{"text": "這些擁有記憶功能的GPT模型，其記憶調用機制仍不明確。例如，指示其「禮拜五下午去玩好嗎？」便會啟動記憶模組，但其內部運作，究竟是全盤調用記憶再進行模型推論，抑或是僅提取相關記憶片段（類似RAG），目前尚無定論。", "label": 0}
{"text": "關於這些具備記憶功能的GPT模型，其記憶機制的運作方式仍存有謎團。例如，當我們詢問「禮拜五下午去玩好嗎？」時，記憶模組看似被啟動，但其啟動過程和資料調用方式（是整體調用所有記憶，還是僅調用相關記憶，類似RAG機制）卻不得而知。", "label": 0}
{"text": "關於這些擁有記憶功能的GPT模型，其記憶機制尚不明確。例如，指令「禮拜五下午去玩好嗎？」會觸發記憶模組，但其運作方式——是直接調用所有記憶輔助回答，還是僅提取相關記憶內容（例如透過RAG），目前仍是一個謎。", "label": 0}
{"text": "那這些有記憶的確GPT，他可以使用他的記憶，比如說我跟他說禮拜五下午是去玩好嗎？這個時候記憶模組就被啟動了，但是他是怎麼被啟動的，其實就不太清楚了。他到底是把所有記憶的內容，通通都放到這個問題的前面，直接讓模型做回答，還是說也有做RAG，只選擇下載相關的記憶內容呢？那這個我們就不得而知了。", "label": 1}
{"text": "我問他週五下午要不要出去玩，他立刻反應過來下午有課，真是個機靈鬼！  他還記得我之前叫他「血輪眼卡卡」，真有趣！想深入了解AI Agent記憶的研究？這裡有幾篇參考論文：2023年的Memory GPT、2024年的Agent Workflow Memory和2025年的Agent Memory，可見這領域的研究持續蓬勃發展。", "label": 0}
{"text": "我問他週五下午要不要出去玩，他立刻反應過來下午有課，真是個機靈鬼！  他還記得我之前叫他“血輪眼卡卡”，真是令人驚訝的記憶力。想深入了解AI Agent記憶研究？這裡列出幾篇重要論文：2023年的Memory GPT、2024年的Agent Workflow Memory和2025年的Agent Memory，可見這領域的研究持續蓬勃發展。", "label": 0}
{"text": "我問他週五下午要不要出去玩，他立刻反應過來下午有課，真機靈！  不愧是血輪眼卡卡，還記得之前的對話。想深入了解AI Agent記憶研究？我推薦幾篇論文：2023年的Memory GPT、2024年的Agent Workflow Memory和2025年的Agent Memory，可見這領域的蓬勃發展。", "label": 0}
{"text": "總之當我問他，週五下午出去玩好嗎？這個read的模組就啟動了。他就說，下午不是要上課嗎？怎麼能夠出去玩，好聰明啊！他知道下午要上課，挺厲害的，然後問他你是誰，剛才我說過，他是血輪眼卡卡，所以他就覺得，之前是血淪眼卡卡。如果你想要知道，更多有關AI Agent記憶的研究的話，那這邊就是放了幾篇經典的論文給大家參考，包括Memory GPT，這是23年的論文，Agent Workflow Memory是24年的論文，還有一個最近的Agent Memory，Agent是25年的論文，所以23到25年各引用一篇，告訴你說這方面的研究是持續不斷的。", "label": 1}
{"text": "接下來，我們將探討大型語言模型如何運用工具，以及語言模型本身作為工具的意義。", "label": 0}
{"text": "接著，我們將說明當前語言模型如何運用工具，以及「工具」的定義，並闡明語言模型本身也是人類的一種工具。", "label": 0}
{"text": "讓我們進一步探討大型語言模型如何運用工具，以及語言模型本身作為工具的本質。", "label": 0}
{"text": "接下來呢，我們要跟大家講，現在這些語言模型怎麼使用工具。那什麼叫做工具呢？但語言模型本身對我們人類來說也是工具。", "label": 1}
{"text": "語言模型的工具，僅需了解其使用方法即可，無需探究其內部運作機制。如同被視為「工具人」的電腦維修者，其價值僅在於解決問題的能力，而非個人感受。", "label": 0}
{"text": "語言模型的工具，其本質即為可被有效利用的資源，使用者只需掌握其使用方法，無需探究其內部機制。如同只在意電腦能否修好的「工具人」一般，工具的價值在於其功能性而非其本質。", "label": 0}
{"text": "語言模型的工具，僅需知其用法，無需探究其內在運作機制。如同電腦維修人員常被視為工具人，只因其技能被利用，而非其個人意願或想法。", "label": 0}
{"text": "那對語言模型來說，什麼東西又是他的工具呢？所謂的工具就是這個東西啊，你只要知道怎麼使用他就好，他內部在想什麼，他內部怎麼運作的，你完全不用管。這就是為什麼肥宅如果一直幫另外一個人修電腦的話，就會被叫做工具人，因為別人沒有人在意肥宅的心思，只知道他能不能夠修電腦而已，所以這個就是工具的意思。", "label": 1}
{"text": "語言模型的常用工具包括搜尋引擎、自行編寫並執行的程式碼，甚至其他AI模型；不同AI模型能力各異，例如文字模型可呼叫圖像或語音模型處理多模態任務；或由小型模型負責日常互動，遇到複雜問題時再請求大型模型協助，以節省大型模型的運算資源。", "label": 0}
{"text": "語言模型藉助哪些工具提升效能？主要依靠搜尋引擎；此外，它們能編寫並執行程式碼作為工具，甚至利用其他AI協作；不同AI擁有不同能力，例如文字模型可調用圖像或語音AI處理多模態任務；或由小型模型負責日常互動，遇到瓶頸時再呼叫大型模型協助，以節省大型模型的運算資源。", "label": 0}
{"text": "語言模型仰賴哪些工具？主要包括搜尋引擎及自行編寫並執行的程式碼，甚至其他AI模型，以彌補其能力不足。例如，擅長文字處理的模型可借助圖像或語音識別AI處理多模態任務；或小型模型可呼叫大型模型協助解決超出自身能力範圍的問題，以提升效率並節省運算資源。", "label": 0}
{"text": "那有哪些語言模型常用的工具呢？最常用的就是，就是搜尋引擎,然後呢,語言模型現在會寫程式,而且可以執行他自己寫的程式,那這些程式也算是某種工具,甚至另外一個AI也可以當作是某一個AI的工具,有不同的AI,有不同的能力,比如說現在的語言模型,如果他只能夠讀文字的話,那也許可以呼叫其他看得懂圖片,聽得懂聲音的AI,來幫他處理多模態的問題,或者是說,或者是不同模型它的能力本來就不一樣。也許平常是小的模型在跟人互動，但小的模型發現它自己解不了的問題的時候，它可以叫一個大哥出來。大哥是個大的模型，那大的模型運作起來就比較耗費算力，所以大的模型不能常常出現。大的模型要在小的模型召喚它的時候，才出面回答問題，大哥要偶爾才出來幫小弟解決事情。", "label": 1}
{"text": "簡而言之，語言模型利用工具就像調用函數，只需知曉輸入輸出，不必了解內部機制；所謂「function code」功能，指的就是此項能力。", "label": 0}
{"text": "簡而言之，語言模型藉由呼叫函式來使用工具，無需理解其內部機制，只需知曉輸入輸出即可；因此，「使用工具」與「函式程式碼」 (function code) 等義，許多語言模型聲稱新增此功能，實則指已具備工具使用能力。", "label": 0}
{"text": "簡而言之，語言模型利用工具，實際上是調用函數；它們無需了解函數內部機制，只需掌握輸入輸出即可。因此，「使用工具」與「函數編碼」(function code)等同，聲稱具備函數編碼功能的語言模型，即已能使用工具。", "label": 0}
{"text": "那其實這些工具對語言模型來說，都是function都是一個函式，當我們說語言模型在使用某一個工具的時候，其實意思就是它在調用這些函式。它不需要知道這些函式內部是怎麼運作的，它只需要知道這些函式怎麼給它輸入，這些函式會給什麼樣的輸出，那因為使用工具就是調用函式，所以使用工具又叫做function code，所以有一陣子很多語言模型都說，他們加上了function code的功能，其實意思就這些語言模型都有了使用工具的功能。", "label": 1}
{"text": "那麼，大型語言模型如何運用工具？稍後我會介紹一種通用的工具使用方法，但實際上方法多元，有些模型甚至專為工具使用而訓練，這些模型可能需要特定格式的指令才能有效運作，這不在本次討論範圍內。此外，若您使用OpenAI ChatGPT API，便會發現工具使用需要特定的參數設定，OpenAI模型的工具使用方式也具其獨特性。", "label": 0}
{"text": "那麼，大型語言模型如何運用工具？稍後我會說明一種通用的工具使用方法，但實際上方法多元，有些模型甚至專注於此並接受過相關訓練，因此可能需要特定格式才能有效驅動。這不在本次討論範圍內，或者，如果您使用OpenAI ChatGPT API，便會發現工具的使用需置於特定欄位，OpenAI的模型在使用工具時也有其獨特之處。", "label": 0}
{"text": "那麼，大型語言模型如何運用工具？稍後我會介紹一種通用的工具使用方法，但實際上方法多元，有些模型甚至專為工具使用而訓練，因此其使用方法可能需要特定格式才能啟動，這不在本次討論範圍內。  若您使用OpenAI ChatGPT API，便會發現工具的使用需要指定特定欄位，OpenAI 模型的工具使用也存在特定用法。", "label": 0}
{"text": "好那語言模型怎麼使用工具呢？等一下我會講一個通用的使用工具的方法，但實際上使用工具的方法很多，甚至有一些模型是專門針對來練習，他就訓練來使用工具的，那他如果是針對使用工具這件事做訓練，那他在使用工具的時候，你可能需要用特定的格式才能夠驅動他。那那個就不是我們今天討論的問題，或者是假設你有使用，使用這個OpenAIChat GPT的API的話，你會知道使用工具這件事情，是要放在一個特殊的欄位，所以對OpenAI來說，它的模型在使用工具的時候，也有一些特殊的用法。", "label": 1}
{"text": "簡而言之，所有較強大的模型都能透過直接指示的方式使用工具：在「Tool」標籤內描述工具使用方法，輸出結果則放在「Output」標籤內。  提供工具清單及範例（例如：Temperature函式，輸入地點和時間，輸出該時間地點的溫度），將工具使用說明（System Prompt）與使用者問題（User Prompt）一起輸入模型即可。  System Prompt是開發者每次都使用的固定指令，User Prompt則是每次變化的使用者提問。", "label": 0}
{"text": "所有強大的模型都能使用此通用方法：直接指示模型如何使用工具，將工具指令置於兩個Tool符號之間，輸出則置於兩個Output符號之間，並提供工具清單及使用範例（例如：Temperature函式，輸入地點和時間，例如Temperature(臺北,時間)）。將工具使用方法及問題（User Prompt）與模型初始化指令（System Prompt）一同輸入，模型將根據需要使用工具。System Prompt為開發者每次都相同的初始指令，User Prompt則為每次不同的使用者問題。", "label": 0}
{"text": "核心方法是直接指示模型如何使用工具：在「Tool」符號間描述工具使用方法，輸出則置於「Output」符號間。  以「Temperature」函數為例，說明其功能、輸入（地點、時間）及使用範例（Temperature(臺北, 時間)）。將工具使用方法、問題（User Prompt）與模型初始指令（System Prompt）一併輸入，模型需用工具時會自行調用。System Prompt為開發者每次使用的固定指令，User Prompt則為每次變化的問題。", "label": 0}
{"text": "但我這邊講的是一個最通用的用法，對所有的模型，今天能力比較強的模型，應該都可以使用。好,什麼樣通用的方法，可以讓模型使用工具呢？就是直接跟他講啊！就告訴他怎麼使用工具，你就交代他可以使用工具，那你就把使用工具的指令，放在兩個Tool符號的中間，使用完工具後你會得到輸出，輸出放在兩個Output符號的中間。所以他就知道工具使用的方式了。接下來告訴他有哪一些可以用的工具，有一個函式叫做Temperature，他可以查某個地點某個時間的溫度，他的輸入就是地點跟時間，給他的使用範例，Temperature括號臺北某一段時間，他就會告訴你臺北在這個時間的氣溫。接下來你就把你的問題，連同前面這些工具使用的方式，當作Prompt一起輸入給語言模型，然後他如果需要用工具的話，他就會給你一個使用工具的指令。那前面這些教模型怎麼使用工具的這些敘述，他叫做System Prompt，那查詢使用調用這些工具的這些，這段話,某年某月某日高雄氣溫如何,這個是User Prompt,那如果你有在使用這個ChatGPT的API的話,你知道你的輸入要分成System Prompt跟User Prompt,那很多同學會搞不清楚System Prompt跟User Prompt有什麼樣的差別,那System Prompt指的是說,你在開發應用的這個Developer下的這個Prompt,這個Prompt呢,是每次都是一樣的,每次你都想要放在語言模型最前面,讓他去做文字接龍的這個敘述叫做System Prompt。", "label": 1}
