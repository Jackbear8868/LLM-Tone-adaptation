{"text": "這一堂課呢是，一堂課搞懂生成式人工智慧的技術突破與未來發展，那我現在呢打算用一堂課的時間，很快地帶大家看過生成式人工智慧近年來發展的現況以及未來大家可以關注的技術", "label": 1}
{"text": "本課程將深入淺出地探討生成式人工智慧的技術進展及其未來趨勢，並在有限的時間內，快速綜覽其近年來的發展和值得關注的技術方向。", "label": 0}
{"text": "假設你對大型語言模型一無所知，不知道它們是怎麼被訓練出來的，你可以先看《生成式AI導論2024》那能夠全部看完最好，那如果沒有辦法的話，希望至少可以看到第8講，那這個系列呢內容是非常輕鬆的，你就用你吃飯啊、運動啊、通勤的時間看一下子就看完了", "label": 1}
{"text": "如果你對大型語言模型完全陌生，建議先閱讀《生成式AI導論2024》，最好能完整看完，至少也要看完第8講。該系列內容淺顯易懂，利用零碎時間（例如用餐、運動或通勤）就能輕鬆看完。", "label": 0}
{"text": "我們會先從現在的生成式AI，有什麼樣的行為，可以做到什麼樣的事情開始講起，那接下來我們會講它背後運作的機制，然後我們會講這些運作的機制是怎麼產生出來的，最後我們會講怎麼賦予這些人工智慧的模型新的能力", "label": 1}
{"text": "首先，我們將探討現有生成式AI的能力與應用；其次，分析其底層運作機制；然後，闡述這些機制的形成過程；最後，說明如何提升這些AI模型的功能。", "label": 0}
{"text": "各位同學看到這張圖有沒有覺得很厲害，現在人工智慧什麼都能生，你可能會想哇靠這樣是不是以後都不用我了，放心啦，人工智慧雖然猛，但還是要你來下指令嘛，這場課呢就是要帶你輕鬆看懂這些人工智慧厲害在哪裡", "label": 1}
{"text": "同學們，這張圖展現的人工智能能力令人驚嘆，或許你會擔心未來工作被取代，但別緊張，人工智能需要人類的引導。本課程將深入淺出地剖析人工智能的強大之處。", "label": 0}
{"text": "好啊，我們開始來上課吧！我是李宏毅，那現在這門課呢，是機器學習。那Breezy Voice合出來的聲音是這樣子的。各位同學，那看到這張圖啊，有沒有覺得很厲害？", "label": 1}
{"text": "機器學習課程開始，授課老師是李宏毅。採用Breezy Voice語音合成技術。各位同學，這張圖令人印象深刻嗎？", "label": 0}
{"text": "好，那最後把合成出來的聲音加上一些我的畫面丟給Heygen，那這個平台呢，就會產生剛才你看到的影片了。所以有了投影片之後，要生成一個數位人來直接講課，嗯！是有可能的。但是真正的難點並不在講課的環節啊！準備一門課最花時間的，其實是在做投影片上啊！真正花時間的地方並不在製作投影片的過程，而是想投影片的內容。", "label": 1}
{"text": "好的，最後將合成音訊與我的畫面素材提供給Heygen，該平台就能生成影片。因此，利用投影片製作數位人講課，是可行的。然而，真正的挑戰並非講課環節，而是備課的過程，尤其準備投影片內容最費時。製作投影片本身並非最耗時的環節，構思投影片內容才是。", "label": 0}
{"text": "除非你的投影片是直接跟別人借的，不然做一門課的投影片是最花時間的。那我們能不能夠直接讓人工智慧來做投影片呢？如果它可以直接做完整的投影片的話，我們就可以把老師淘汰了。", "label": 1}
{"text": "製作課堂投影片非常耗時，除非是直接向他人借用。那麼，能否運用人工智慧生成投影片呢？如果人工智慧能製作完整的投影片，教師的角色將會被取代。", "label": 0}
{"text": "他是荒謬到讓人想笑，那這個語言模型產生出來的笑話都是這個等級的啦。但是在一萬三千字的內容中，其實他還是講出了一個好的笑話。這是他唯一我覺得可以看的笑話。這個笑話是這樣子的，而且不是這個笑話啦，是一個勵志小故事。", "label": 1}
{"text": "他的幽默令人啼笑皆非，與其生成的絕大多數笑話一樣低級。然而，在一萬三千字的文本中，擴散模型的原理很像人生：初始狀態雜亂無章，但透過逐步迭代，最終能呈現出清晰的圖像。這說明即使面對混亂，只要持續努力，就能獲得理想結果。AI的運作機制如此鼓舞人心，我們更應積極進取。確實出現了一個值得稱道的笑話，也是唯一一個讓我印象深刻的段落。這個段落，不，應該說是一個勵志小故事。", "label": 0}
{"text": "他說擴散模型 (diffusion model) 其實很浪漫，為什麼？因為他告訴我們，就算人生一團亂，全是雜訊 (Noise)，只要一步一步努力去除雜訊 (Noise)，也能拼出美麗的風景。人工智慧都這麼勵志了，我們還能不努力嗎？哇，我從來沒有想過擴散模型 (diffusion model) 背後有這麼勵志的故事，人工智慧實在是太有創意了。", "label": 1}
{"text": "  我從未意識到擴散模型蘊含如此深刻的哲理，AI技術的巧妙令人讚嘆。", "label": 0}
{"text": "這門課是生成式人工智慧的技術突破與未來發展，那生成式人工智慧的基本概念是，定義生成式人工智慧，讓機器有想像力，可以產生文字、圖像、影片甚至程式碼，運作原理就是基於深度學習 (DL)，從海量資料中學習這個有講跟沒講一樣，不知道在說什麼，那重要性就是給予機器創造力，讓電腦不再侷限於選擇題，而是能自由發揮。", "label": 1}
{"text": "本課程探討生成式AI的技術革新及其未來趨勢。生成式AI的核心概念是賦予機器創造力，使其能生成文字、圖像、影片和程式碼等內容。其運作機制仰賴深度學習，透過龐大數據集進行訓練，但這樣的說明過於籠統，缺乏實質內容。總而言之，生成式AI的關鍵在於讓電腦從單純的選擇題思維模式，躍升至更具創造性和自由度的運作方式。", "label": 0}
{"text": "那近期有什麼技術突破呢？有GPT-4，有DALL-E 2，他說DALL-E 2解析度提升四倍，這個提升四倍是相較於誰提升四倍，也沒講清楚，還有Stable Diffusion，開源的文本生成圖像模型，那核心的技術有Transformer，有GAN，還有擴散模型 (Diffusion Model)那生成式AI 人工智慧有什麼樣的應用案例呢？有文字生成、圖像合成、音樂創作，還有程式碼生成。", "label": 1}
{"text": "最近有哪些重要的技術進展？例如GPT-4、DALL-E 2以及Stable Diffusion等模型的問世。DALL-E 2號稱解析度提升四倍，但基準不明確。Stable Diffusion則是一個開源的文本生成圖像模型。這些模型的核心技術包含Transformer、GAN和擴散模型。生成式AI的應用領域涵蓋文字生成、圖像合成、音樂創作和程式碼生成。", "label": 0}
{"text": "然後呢，當前的挑戰有內容真偽、濫用問題，偏見、公平性的問題、隱私問題，還有高昂計算成本，未來有什麼可能的發展呢？我們需要技術優化，降低成本，多模態與深度理解，人機協作，還有規範倫理的引導需要被完善", "label": 1}
{"text": "目前面臨著真偽辨別、濫用、偏見、公平、隱私以及高昂成本等挑戰，未來發展方向包括：技術提升以降低成本，實現多模態與深度理解，促進人機協作，以及建立完善的倫理規範。", "label": 0}
{"text": "如果我是一個懶惰的老師，我真的就可以用剛才的投影片上課了，所以今天就是告訴你說，假設只是要上剛才那種流水帳式的課程，完全可以用人工智慧來生成投影片，有投影片以後，再用人工智慧來產生數位人，就不需要人類來上這門課了，完全可以全自動的產生一門課程。", "label": 1}
{"text": "懶惰的老師可以只用現成的投影片授課，事實上，這種流水帳式的課程，完全可以交由AI生成投影片，再利用AI製作數位人授課，實現全自動化教學。", "label": 0}
{"text": "好，接下來是人類做的投影片了。大家來看看人類上的課，跟完全用人工智慧準備的教材有什麼不同，那我們現在來看人類做的投影片吧。", "label": 1}
{"text": "好的，接下來我們比較一下人類製作的投影片和AI生成的教材。讓我們看看人類的教學投影片。", "label": 0}
{"text": "那現在啊，這些人工智慧它還展示出類似思考的能力，那通常我們把這個過程叫做 reasoning ，什麼叫做展示思考的能力呢?過去啊，我們在使用這些生成式人工智慧的時候你給它一個輸入 (Input)，給它一個問題，它就直接給你一個答案，但是現在很多的生成式AI，比如說ChatGPT的 o1, o3、還有DeepSeek，還有 Gemini 的 Flash Thinking ，它們在問一個問題的時候，它都不是直接給，都你在問它一個問題的時候，它不是直接給你答案，而是它會演一個腦內小劇場給你看，它就說我們先試試看 A 解法吧！解完之後自己驗證一下答案，發現，嗯，不對再試一下 B 解法吧！它驗證一下嗯好像是對的， B 解法不錯，但我們也可以嘗試一下 C 解法，嗯，看起來沒有比較好，那把腦內小劇場演完之後才給你答案，所以我們用的就是 B 解法的答案。那通常這些模型在展示它的結果的時候，它會把腦內小劇場放在一個框框內，告訴你說這是它的內心戲，不是真正的答案，然後最後才給你真正的答案。", "label": 1}
{"text": "如今，這些AI展現出類似推理(reasoning)的能力。以往生成式AI僅根據輸入直接輸出答案，但現在例如ChatGPT的o1、o3、DeepSeek和Gemini的Flash Thinking等模型，會在回答問題前進行類似「思考」的過程：它們會嘗試A方案，驗證後發現錯誤，再嘗試B方案並驗證其正確性，甚至考慮C方案後，最終選擇並呈現最佳方案(例如B方案)的結果，並將其內部推理過程以框的形式呈現，以區別於最終答案。", "label": 0}
{"text": "那如果你覺得這一段聽起來有點抽象的話，那我們就實際用一下 DeepSeek ，告訴你這個腦內小劇場看起來像是什麼樣子的，我們來問 DeepSeek 一個莫名其妙的問題吧！我想到說封神演義裡面的姜子牙是個老人，有法力，鄧不利多《哈利波特》的也是一個老人，也有法力，如果他們處在同一個時空，兩人都處於個人的巔峰狀態，有充足的準備時間，如果他們有理由不得不開打，在公平對決的情況下，你覺得誰會贏呢？我們來看看DeepSeek，覺得這兩個會魔法的老人決鬥的話誰會獲勝，那DeepSeek不會馬上給出答案，它會先產生這些顏色比較淺的文字，這些顏色比較淺的文字就是它的內心小劇場，它開始在內心演說它要如何思考這個問題，它就會說，嗯，這個問題看起來挺有趣的，然後開始想說姜子牙有什麼能力，鄧不利多有什麼能力，它的內心小劇場演得非常長，足足演了1500個字，然後演完這些內心小劇場以後，它才會給你答案，內心小劇場非常長啦，不容易讀啦，這邊就看一些段落就好。比如說你看這邊它已經下了總結了，但下了總結之後，它又好像突然想起來好像有東西沒有考慮到，它就說不過還需要再考慮什麼什麼，這個 DeepSeek 內心小劇場，就是可以讓你看到這些模型的糾結，而因為這個 DeepSeek 內心小劇場實在太長了，懶得管它在講什麼。", "label": 1}
{"text": "若覺得上述說明過於含糊，讓我們實際操作DeepSeek，看看它如何處理一個奇特的假設情境。例如：若封神演義的姜子牙與哈利波特的鄧不利多在巔峰狀態、準備充分且被迫對決，公平一戰下誰勝？DeepSeek不會直接給出答案，而會先呈現其推演過程——大量的、淺色字體的內部思考過程，詳述其分析姜子牙和鄧不利多的能力。此過程冗長，約1500字，包含多次自我修正和補充，展現模型的決策過程。讀者可略讀其長篇推演，只需關注其結論即可。此冗長的思考過程，便是DeepSeek的特色，展現模型的思維邏輯，雖然詳細，但略嫌繁瑣。", "label": 0}
{"text": "所以直接把他的文字丟給 Claude，叫 Claude 呢來幫我們整理 DeepSeek 的思路，我就跟 Claude 說把以下的內容可視化，畫出人容易看得懂的圖，Claude 呢非常擅長寫程式，畫可視化的圖或者是做網頁，我給他這個指令以後，他就寫出了右邊這個網頁，而且 Claude 是可以做 preview (預覽)，可以預覽這些程式執行的結果，執行出來就是這個樣子，這邊的顏色都是 Claude 自己套上去的啦，左邊是姜子牙的能力，右邊是鄧不利多的能力，姜子牙的主要能力是有道術跟陣法，十絕陣這個我要批評一下，那個十絕陣並不是姜子牙的能力啊，十絕陣是姜子牙的對手咒王那邊的人擺的陣法，所以他不是姜子牙的能力啊！", "label": 1}
{"text": "我將DeepSeek的構想交給Claude，請它以圖表形式呈現，Claude程式能力強，能製作可視化圖表和網頁。  指示Claude後，它即生成右側網頁預覽，網頁色彩由Claude自動套用。網頁左側顯示姜子牙能力（道術、陣法），右側顯示鄧不利多能力。需更正一點，十絕陣並非姜子牙的能力，而是其敵人咒王一方所設。", "label": 0}
{"text": "那姜子牙有什麼優勢呢？優勢就是他有個杏黃旗，他有很高的防禦力，這個我是同意的。在《封神演義》裡面，應該是沒有什麼攻擊可以突破杏黃旗，那有什麼樣的劣勢呢？劣勢就是他的法寶可能對非神職人員較弱，這個我也是蠻同意的。姜子牙的打神鞭啊，應該是只能打封神榜上有名人。有一次姜子牙遇到一個對手，那個對手蠻弱的，但姜子牙拿打神鞭去打他，打神鞭就被對手搶走了，旁白就說為什麼打神鞭打不了這個人呢？因為那個人不在封神榜上啊！所以打神鞭沒辦法打他。所以如果鄧不利多不在封神榜上的話，看起來打神鞭可能對鄧不利多沒有辦法起作用，右邊是鄧不利多的優勢跟劣勢，然後對決分析結果是：短期戰對鄧不利多有利，長期戰對姜子牙有利，結論是姜子牙在準備充分的時候獲勝機率較高，這個是內心小劇場的內容，好，這個是 Deep Research 真正的答案啦，他就說最後姜子牙可能會贏。", "label": 1}
{"text": "姜子牙憑藉杏黃旗擁有極高的防御力，這點毋庸置疑。然而，其打神鞭的威力似乎僅限於封神榜上的人物，對凡人效力有限，這也得到了印證。曾有記載，姜子牙因對手不在封神榜上而被奪走打神鞭。因此，鄧不利多若不在榜上，打神鞭恐对其无效。综合分析邓不利多的优势劣势，短期内邓不利多占优，但持久战姜子牙胜算较大，最终姜子牙胜出的可能性更高。", "label": 0}
{"text": "不過他還多講了一句話，他覺得如果鄧不利多一開始就用移行幻影 (Apparition)，直接近身發動索命咒就有逆轉的可能，但這個逆轉的關鍵取決於杏黃旗能不能夠擋住索命咒，在封神演義裡面應該沒有杏黃旗擋不住的東西，翻天印打誰誰死，但杏黃旗可以擋住翻天印，那在索命咒呢？在哈利波特裡面也沒有能夠擋住的東西，所以這就是一個矛跟盾的對決，不知道索命咒能不能夠突破杏黃旗，這個會是決勝的關鍵。", "label": 1}
{"text": "他補充道，若鄧不利多一開始便使用移形換影，近身施放索命咒，或許能扭轉局勢。然而，成敗關鍵在於杏黃旗能否抵禦索命咒。據《封神演義》，杏黃旗似乎無堅不摧，連翻天印都無法攻破，但索命咒在哈利波特世界裡同樣難以抵擋，這場對決如同矛與盾的較量，杏黃旗能否阻擋索命咒，將決定勝負。", "label": 0}
{"text": "好，這個是 DeepSeek 的答案啦！我一樣問ChatGPT o3-mini-high 這個問題，那他也是會做一下腦內小劇場，不過 o3 的腦內小劇場通常比較短，我有點懷疑他沒有完整呈現腦內小劇場的內容，他呈現給我們的只是摘要而已。呈現完腦內小劇場之後，他也給出了他的答案， o3 一樣覺得姜子牙比較可能贏過鄧不利多，看來兩個蠻聰明的模型都覺得姜子牙比較有勝算。結束，我得到這個問題的答案了。", "label": 1}
{"text": "DeepSeek與ChatGPT o3-mini-high針對姜子牙與鄧不利多誰勝算較高給出了相同結論：姜子牙勝算較大。o3的思考過程較簡短，可能僅呈現部分思維過程，而非完整細節。  兩模型皆認為姜子牙更可能獲勝。  問題已解決。", "label": 0}
{"text": "剛才呢，我們使用人工智慧的用法都是，你給他一個問題，他就給你一個答案。那通常今天我們使用人工智慧的時候就是一問一答，對人工智慧來說，答案給出去就給出去了，那這個答案會造成什麼樣的影響？這個答案是不是對的，他也不在乎。但是光是一問一答不能解決所有的問題，有很多的任務往往無法一步完成。需要多個步驟才有辦法完成，舉例來說：我舉一個日常生活中的例子，老婆大人跟我說今天晚上要去外面吃飯，那我可能就要先問說，那要吃什麼呢？老婆大人說吃餐廳 A ，我就打電話去餐廳 A ，打完電話以後發現餐廳 A 說沒位置了。如果我只是一個語言模型的話，我可能就會躺在這裡，然後回報說沒位置了結束。", "label": 1}
{"text": "先前我們與AI互動的方式，是提問並獲得答案。目前的AI應用多半是問答模式，AI提供答案後便不再追蹤後續影響或答案的正確性。然而，單純的問答無法處理所有任務，許多任務需要多步驟才能完成。例如，若太太吩咐今晚外出用餐，我會先詢問想吃什麼，假設她選餐廳A，我便致電預約，但若餐廳A客滿，單純的語言模型可能僅回報「客滿」便停止運作。", "label": 0}
{"text": "但我是一個人類，如果一個人只有這樣做的話，那一定會被老婆大人痛毆，所以我們需要想辦法執行完這個任務，那沒位置了怎麼辦呢？你會想其他的方法，比如說上網搜尋看看有沒有其他類似的餐廳，找到餐廳B，然後問老婆大人說訂餐廳B好嗎？老婆大人說可以，然後就訂餐廳B，今天上述工作我們有沒有辦法直接讓 (AI) 人工智慧來完成呢？如果 (AI) 人工智慧可以執行這種需要多個步驟才能完成的工作，那我們會叫它 AI Agent，那我在往後的課程會再更詳細地講什麼是AI Agent。雖然我舉的例子只是一個非常日常生活中的例子，但不要小看這個任務，你需要具備非常多的能力才有辦法完成這個任務，比如說AI Agent必須要能從經驗中學習，剛才打電話就已經知道餐廳 A 沒位置了，如果不會學習的AI，它就會不斷反覆打電話給餐廳A，那這顯然是沒辦法解決問題的。所以模型要從經驗中學習，知道不要再訂餐廳A，模型要有使用工具的能力，知道自己沒有那麼多餐廳的知識，需要上網搜尋才能知道有哪些可以訂的餐廳，這邊在這個例子裡面AI需要上網搜尋。", "label": 1}
{"text": "身為人類，若只照直行事，肯定會被太太修理，因此我們必須設法完成任務。座位客滿該怎麼辦？當然另尋他法，例如上網搜尋其他餐廳，找到餐廳B後，徵詢太太意見，獲得同意後便預訂餐廳B。那麼，上述流程能否交由AI自動完成呢？能處理此類多步驟任務的AI，我們稱之為AI Agent，後續課程將深入探討AI Agent。此例看似日常瑣事，卻暗藏玄機，完成它需要多種能力，例如AI Agent需具備學習能力，避免重複撥打已滿位的餐廳A電話；需具備使用工具的能力，例如懂得利用網路搜尋其他餐廳資訊。", "label": 0}
{"text": "那我在作業一會出一個作業，讓大家用AI上網搜尋之後，來回答助教出的問題，那除此之外呢？這個AI還需要具備一定程度的規劃能力，比如說它會主動跟人類確定餐廳訂哪一間才是合適的，因為如果最後訂的餐廳是不合適的，那就力氣就白花了。所以與其最後白花力氣，不如一開始稍微多花一點力氣，直接跟人類確定人類的需求，但是AI也要知道什麼時候需要確認，什麼時候不需要確認？如果AI連上網搜尋這個動作都要問人類說，我可不可以上網搜尋，那人類顯然是會生氣的，要完成這個日常生活中的工作，就要完成這個例子其實是需要很多不同的技能的，那我們在下一堂課會再來詳談 AI Agent。", "label": 1}
{"text": "我會布置作業，讓大家使用AI搜尋網路資訊，並回答助教提問。此外，AI還需具備規劃能力，例如主動與使用者協商餐廳選擇，避免浪費時間和精力。事前仔細確認使用者需求，能避免事後補救的麻煩。然而，AI也需判斷何時需要確認，何時無需確認；例如，詢問是否能進行網路搜尋，顯然會造成使用者困擾。要完成日常任務，AI需要整合多種技能，我們下堂課將深入探討AI Agent。", "label": 0}
{"text": "那講到AI Agent，我想現在也是有一些服務，看起來是初步具備有 AI Agent 的能力，那一個例子就是 Deep Research，現在ChatGPT、Gemini、Perplexity 都有出 Deep Research 的功能。", "label": 1}
{"text": "目前已有幾項服務展現出初步的AI Agent能力，例如Deep Research功能，此功能已整合至ChatGPT、Gemini和Perplexity等平台。", "label": 0}
{"text": "Deep Research 的功能就是你問它一個問題，比如說中部橫貫公路的歷史沿革，那如果你在 ChatGPT 的頁面上選下面這個 Deep Research (深入研究)，就會執行 Deep Research，也就是模型會開始上網搜尋，而且他搜尋的時候不是只是搜尋一次得到結果就開始寫報告，而是會隨著搜尋到的內容而產生更多的問題，最後寫出一個長篇大論的報告。那右邊呢是展現了當我問中部橫貫公路歷史沿革的時候，Deep Research的搜尋過程，這只是一部分的過程而已。他會先搜尋中部橫貫公路的主線跟支線，他就學到說中部橫貫公路有宜蘭支線和霧社支線，他再去搜尋霧社支線的起點和終點，發現說2018年有一個改道工程，他再去搜尋2018年中橫的改道工程，所以他會隨著搜尋到的結果不同改變他要搜尋的內容，那這是一種 AI Agent的能力。", "label": 1}
{"text": "Deep Research 讓使用者提問，例如中部橫貫公路的歷史，系統便會啟動網路搜尋。它並非單次搜尋後直接撰寫報告，而是基於搜尋結果產生更多問題，最終生成詳盡的報告。右側展示了以中部橫貫公路歷史沿革為例的搜尋過程片段，包含搜尋主支線、霧社支線起訖點、2018年改道工程等，展現其根據搜尋結果調整搜尋方向的AI Agent能力。", "label": 0}
{"text": "另外一個看起來像是 AI Agent的能力，就是 Claude 的 Computer Use 或者是 ChatGPT 的 Operator，這 Computer Use 或者是 Operator 他們要做的事情就是，現在這些生成式人工智慧不只是生成，它還要能夠操控物件。那可能要操控機械手臂仍然非常的困難，但是現在基本上可以操控，數位世界中能夠碰觸到的滑鼠或者是鍵盤了，那這些生成式AI 怎麼操控滑鼠跟鍵盤呢？這個方法就是你先給它一個任務指示，然後給它一個螢幕截圖，那現在這些模型都是可以看圖的嘛，那看到這個螢幕截圖看到這個指示，它會輸出文字，但它輸出的文字是要如何去操控滑鼠跟鍵盤。比如說它會輸出文字說，把滑鼠移動到螢幕的這個位置，這個是指螢幕上位置的座標。這邊就需要開發者真的去寫一個小程式，把滑鼠移動到指定的位置，那滑鼠移動到指定的位置以後，螢幕截圖看起來可能就不一樣了，螢幕截圖不一樣之後就會產生新的輸出。比如說語言模型、生成式人工智慧，會說按下 Enter，那就寫一個小程式。", "label": 1}
{"text": "生成式AI除了文字生成，還能操控數位環境。雖然操控機械手臂仍具挑戰性，但已能控制滑鼠和鍵盤。其運作方式是：使用者給予任務和螢幕截圖，AI分析後輸出控制滑鼠鍵盤的指令（例如：座標位置、按下Enter）。開發者需撰寫程式將AI指令轉化為實際操作，如此AI便能根據螢幕變化，持續執行任務。", "label": 0}
{"text": "那期待透過這一連串的操作，可以完成一些比較複雜的任務，那我們這邊就實際展示一下，ChatGPT的Operator的能力吧！", "label": 1}
{"text": "我們希望藉由這些步驟，達成更複雜的目標，接下來就實際演示ChatGPT Operator的功能。", "label": 0}
{"text": "我這邊給它的指令是，我想加簽李宏毅老師的機器學習，請上臺大課程網找到本學期這門課的加簽表單，並幫我填寫後送出。", "label": 1}
{"text": "請協助我填寫臺大課程網上，李宏毅老師本學期機器學習課程的加簽申請表單。", "label": 0}
{"text": "好，他就停在這裡。他找到了加簽表單，雖然我要求他填寫後送出，不過填加簽表單是要Gmail帳號的啦，所以他沒辦法填。", "label": 1}
{"text": "於是，他便停了下來。他找到加簽表單，但我要求他填寫提交，然而表單需要Gmail帳號才能填寫，因此他無法完成。", "label": 0}
{"text": "那我叫他自己去申請一個 Gmail 帳號來填，他不肯做，所以就停在這裡。好，那剛才你有沒有注意到一件事，一開始在找課程資訊的時候，Operator 是先到課程網頁的上方，去點了課程資訊這幾個字，但發現沒有找到東西。所以接下來他才去下面找到課程的說明，然後找到這一個加簽的表單。所以在中間的過程中，他是一度犯錯的，但是他可以知道說他犯了錯，他可以修正他的計畫，修正他的行為，不再犯同樣的錯誤，這個就是今天AI可以做到的事情。", "label": 1}
{"text": "於是，我建議他自行申請Gmail帳號填寫，但他拒絕了，進度因此停滯。值得注意的是，他最初搜尋課程資訊時，先點擊網頁頂端的「課程資訊」連結卻未果，才向下搜尋到課程說明及加簽表單。過程中，他確實犯了錯誤，但能察覺並修正，避免重蹈覆轍；這正是當前AI技術的展現。", "label": 0}
{"text": "這就是一個 AI Agent 的雛形，開發機器學習 (ML) 模型也需要非常多的步驟，它也是一個需要多步驟才能完成的任務。", "label": 1}
{"text": "開發機器學習模型如同打造AI代理的初步階段，過程繁複，環環相扣。", "label": 0}
{"text": "過去在我們這個課堂上，一個機器學習 (ML) 模型的作業通常是長這樣子的。老師告訴大家說有一個作業一，有任務的說明，這邊是這個任務的資料，然後呢，你就會根據說明跟資料寫訓練模型的程式，然後你就開始執行你的程式。那你可能很難一開始就寫對，開始訓練之後出現一個錯誤訊息，你就根據錯誤訊息去做一下Debug (除錯)。然後呢，你就可以開始訓練你的模型，訓練完你在development set上計算一下正確率，算出來50%，還不太滿意，修改一下模型再重新訓練，那做出來75%，滿意了你就可以上傳你的結果，不滿意就繼續修改你的模型。", "label": 1}
{"text": "以往我們的機器學習作業流程如下：教師提供作業說明及數據集；學生據此撰寫模型訓練程式並執行；程式除錯後進行模型訓練，評估development set上的準確率(例如初始50%)；根據結果調整模型並重複訓練(例如提升至75%)，直至滿意後提交結果。", "label": 0}
{"text": "那在這一堂課裡面，我們當然也需要訓練模型，也需要走過類似的步驟，但是這一次在作業二，我們由AI Agent來完成。那這一門課的作業二呢？就是過去同一門課的作業一，只是這一次我們要用AI Agent來訓練模型，這一次負責執行跟優化模型訓練，不再是你本人，而是由 AI Agent來執行。", "label": 1}
{"text": "作業二沿用作業一的內容，但改由AI Agent負責模型訓練的執行與優化，本堂課會講解相關步驟與訓練過程。", "label": 0}
{"text": "接下來我們來看它背後運作的機制，那這些生成式人工智慧從表面上來看，它做的事情就是有一些輸入，它就有一些輸出，只是這些輸入輸出可以是是很複雜的東西，它可以是一段文字，它可以是一張圖片，它可以是一段聲音。", "label": 1}
{"text": "讓我們深入探討其內在運作原理。這些生成式AI表面上接收輸入，產生輸出，然而，這些輸入輸出可能極其複雜，例如文字、圖片或聲音。", "label": 0}
{"text": "那我們可以把生成式人工智慧做的事情簡化為：就是輸入一個x，輸出一個y，只是這邊的x或這邊的y，它可以是很複雜的東西，它可以是一篇長篇大論，它可以是一張圖片，它可以是一段聲音。", "label": 1}
{"text": "生成式AI的工作原理，可以簡單理解為：給定輸入x，得到輸出y，其中x和y的形態可以非常複雜，例如長文、圖片或聲音。", "label": 0}
{"text": "那產生長篇大論、產生圖片、產生聲音，背後它的共同的原理是什麼呢？這些看似非常複雜的東西，其實都是由有限的基本單位所構成的。假設我們把這些東西用y來表示，那這個y可以指一段畫，可以指一張圖片，可以指一段聲音，它背後其實都是由一些基本單位，這邊用y1、y2、yi來表示，它是由基本單位所構成的，對一段文字來說，它的基本單位，就是組成這些文字的符號。比如說在中文裡面，我們可以說一個方塊字就是一個基本單位。對於圖片來說，如果你把圖片放大的話來看，圖片是由一個一個的像素 (pixel) 所組成的。", "label": 1}
{"text": "長篇大論、圖片、聲音的共通點為何？其複雜性實則源於有限基本單元組成。以Y代表這些事物，無論文字、圖片或聲音，皆由Y1、Y2、Yi等基本單元構成。文字的基本單元為組成文字的符號，如漢字；圖片的基本單元為像素。", "label": 0}
{"text": "那如果你把一段聲音訊號放大的話來看，它背後是什麼呢？一段聲音訊號放大來看可能長這樣。它是由一個一個取樣點所構成的，而每一個取樣點就是一個數字，所以一段聲音訊號也是由一堆基本單位所構成的。那這些基本單位的特性是什麼呢？這些基本單位的特性就是它的選擇是有限的，雖然符號，雖然文字的符號非常多，但是它是有限的，中文方塊字常用的大概就是4000多個。那對於像素 (pixel) 來說，像素 (pixel) 可能的顏色也是有限的。", "label": 1}
{"text": "放大聲音訊號，其本質是許多取樣點組成，每個點都是一個數字。聲音訊號因此是由許多基本單位構成。這些基本單位的特徵是有限的選擇，如同文字符號數量有限(例如常用中文字約四千多個)，像素的顏色數也是有限的。", "label": 0}
{"text": "那你可能會想說：如果取樣點算個數字來表示，這個數字是無限的。但不要忘了，我們在存每個取樣點的時候，你可能只會用一個 byte (位元組) 來存，一個 byte (位元組) 可以儲存的數字的可能性，它的解析度是有限的，你只能存2的8次方的可能性而已，所以取樣點的可能的變化仍然是有限的。這些基本單位它是有限的，這些基本單位你的選擇是有限的，但透過這些有限的選擇進行，你可以組出近乎無窮的可能。", "label": 1}
{"text": "的確，取樣點數量理論上無限，但實際儲存時，每個點僅使用一個位元組，導致其可表示值的可能性僅為256種，是有限的。  儘管如此，有限的單位組合能產生近乎無限的可能性。", "label": 0}
{"text": "一個人說兩句一樣的話，他的聲音訊號不會是一模一樣的，或者是兩個人畫同樣主題的圖，兩張圖不會是一模一樣的，出同一個作文的題目，兩個人寫出來的文章不會是一模一樣的，有限的選擇有近乎無窮的可能，這些基本單位現在在生成式AI裡面，常常會把它叫做token，token就是組合這些複雜物件的基本單位，如果去查字典的話，token最常被翻譯成代幣。", "label": 1}
{"text": "即使是相同的指令或主題，不同個體的產出總會有細微差異。  生成式AI 利用這些差異性極大的基本元素，稱之為token， 字典中常將其譯作代幣。", "label": 0}
{"text": "不過在生成式AI，生成式人工智慧裡面，token就是組成物件的基本單位。另外這邊再補充一下，其實今天在產生圖片還要產生語音的時候，其實像素 (pixel) 或者是取樣點，已經不是最常被使用的基本單位。有更有效的方法來表達影像跟語音的基本單位，不過因為今天時間有限，我們日後再來討論這個話題，那所有的東西都可以看作是由一些基本單位所構成嗎？", "label": 1}
{"text": "在生成式AI與人工智慧領域，構成物件的基本單位是token。此外，值得一提的是，在圖像和語音生成中，像素或取樣點已不再是最常用的基本單位；存在更有效的表示方法，但礙於時間限制，我們將在日後深入探討。那麼，一切事物都可以被分解成基本單位嗎？", "label": 0}
{"text": "有人可能會問說，那樹狀結構能夠看作是由一堆基本單位所構成嗎？這邊有一個文法樹，我們可以把一個文法樹看作是由基本單位所構成嗎？可以", "label": 1}
{"text": "樹狀結構是否能視為基本單元的集合？以文法樹為例，它顯然是由基本單位組成。", "label": 0}
{"text": "一棵文法樹你可以把它表示成一個文字的序列，用左括號右括號的方式來表示樹狀的結構，所以一棵文法樹也可以看作是一串文字的序列。文字的序列就是由一堆文字的基本 token 所構成的，所以樹狀結構也可以寫成由基本物件所構成的。", "label": 1}
{"text": "文法樹能以括號標記的字串序列呈現，因此等同於字串序列。此字串序列由基本文字單位組成，故樹結構可視為基本單元構成。", "label": 0}
{"text": "那你可能進一步問說，那所有的東西都可以看作是由基本物件所構成的嗎？嗯我可以大膽地跟你說我覺得可以，講一句幹話就是所有的物件都是由原子跟分子所構成的，原子它就是有限的選擇組合出無窮的可能，就相信所有的物件都由基本單位所構成。", "label": 1}
{"text": "那麼，所有事物是否都能分解成基本組成單元呢？是的，我的觀點是肯定的。簡而言之，所有物質皆由原子和分子構成，有限的原子種類通過不同的組合方式，產生了無限多樣的物質。因此，我相信所有事物都由基本單位構成。", "label": 0}
{"text": "好，這就是為什麼黃仁勳在去年的 Computex 說：這個token可以是文字，它也可以是影像，也可以是表格，也可以是一首歌，也可以是一段聲音，也可以是影片，萬事萬物都是 token，把萬事萬物拆解成 token，就是生成式 AI 的基本原理，那有的人不知道token是什麼意思，回去查字典發現token是代幣的意思。", "label": 1}
{"text": "因此，黃仁勳去年在 Computex 的說明是：文字、影像、表格、歌曲、聲音、影片，任何事物都能被視為 token。生成式 AI 的核心便是將所有事物分解成 token。  有些人可能不清楚 token 的含義，它指的是代幣。", "label": 0}
{"text": "還以為黃老闆的意思是說，每個東西它都可以變成代幣然後拿來賣給你。它其實不是這個意思，它的意思就是萬事萬物組成都是由token組成，就是生成式 AI 的基本原理。", "label": 1}
{"text": "原來黃老闆並非指所有物品都能變成代幣販售，而是指萬物皆由token構成，這正是生成式AI的根本運作方式。", "label": 0}
{"text": "好，那所以我們現在知道說，生成式 AI 就是拿一堆的 token去生成另外一堆 token ，那怎麼拿一堆token去生成另外一堆token呢？從現在這頁的影片開始，我不會再特別跟你說這裡的每一個 y 指的是什麼，那你心裡就要想像說這邊這個 y 可能是一張圖片，可能是一段文字，可能是一首歌，而這邊的 y 加一個下標，比如說 y 小 i ，它可能是文章中的一個字，它可能是圖片裡面的一個像素，它可能是聲音中的一個取樣點，所以我這邊用符號來表示，告訴你說這個技術是可以用在各式各樣不同的任務應用跟模態上的。", "label": 1}
{"text": "因此，生成式AI的運作原理是將一系列token轉換成另一系列token。  接下來的講解中，我們將不再逐一解釋每個y代表的具體含義，請自行理解y可以代表圖片、文字、歌曲等，而yi則可能指圖片中的像素、文字中的字詞或聲音中的取樣點。  此符號化表示法說明，該技術適用於各種任務和數據類型。", "label": 0}
{"text": "好，那怎麼從 x 產生 y 呢？這背後也可以有一個共同的原理，這邊的策略就是根據固定的次序，每次只產生一個 y ，只產生一個 token 出來，那講的具體一點，就輸入x1, x2, ... , xj，所有的x 的時候，你就產生y1。接下來給所有的 x 跟 y1 ，產生 y2，給 y1 跟 y2，產生 y3，以此類推給所有的x、y1, y2, ... , yt-1 的時候，就產生 yt。", "label": 1}
{"text": "因此，從x生成y的過程，遵循一個共同的原則：循序漸進地、每次只生成一個y（一個token）。具體來說，先用所有x (x1, x2, ... , xj) 生成y1；然後，用所有x和y1生成y2；再用所有x、y1和y2生成y3；以此類推，直到用所有x以及y1, y2, ... , yt-1生成yt。", "label": 0}
{"text": "那產生到什麼時候停止呢？那就取決於你的應用，如果今天是產生圖片的話，你需要產生出來的 token 數目是固定的，因為通常圖片大小是固定的，所以你知道要產生多少個 token，你產生到指定的 token 的數量的時候，就可以停下來。", "label": 1}
{"text": "圖片生成的終止條件取決於應用需求。由於圖片尺寸通常固定，所需 token 數量可預先計算，達到目標數量便停止生成。", "label": 0}
{"text": "那如果是產生一篇文章，通常你無法事先知道要產生的文章有多長。所以你需要再有一個特別的技巧，準備一個特別的 token叫做「結束」。當今天產生出「結束」這個 token的時候，就代表依序生成的過程結束了，不再產生更多的 token。這個策略啊它有一個專有名詞，叫做 Autoregressive Generation (自迴歸生成)，用比較通俗的講法就是文字接龍，這邊接的不一定是文字，而是各式各樣不同的 token。那所以這邊文字我加了一個引號，那當我們接的 token如果是文字的話，其實就叫做語言模型。", "label": 1}
{"text": "生成文章時，長度無法預測。因此，需要一個終止符號，例如“結束”，標示生成過程完成。此方法稱為自迴歸生成，如同文字接龍，但可接的並非僅限文字，而是各種不同的符號。若接的是文字，則稱為語言模型。", "label": 0}
{"text": "但實際上今天為了要蹭語言模型 (Language Model) 的熱度，接的東西不是文字,我們也會叫語言模型。比如接的是語音,我們也說這是語音版的語言模型，雖然說它其實就是語音模型,接出來的是圖片，我們也說是語言模型,雖然它其實是影像的模型。", "label": 1}
{"text": "表面上看，我們今天利用語言模型的熱度，實際處理的對象並非文字，而是語音或圖像。即使處理的是語音，我們仍稱之為語音語言模型；即使處理的是圖片，我們也稱之為影像語言模型，儘管它們本質上分別是語音模型和影像模型。", "label": 0}
{"text": "反正今天只要是做這種接龍的行為，為了要蹭語言模型的熱度我們都把這些技術歸類為語言模型。", "label": 1}
{"text": "總之，今天所有類似的接龍行為，都可歸因於追逐語言模型熱度的技術應用。", "label": 0}
{"text": "那右邊這些步驟其實可以簡化成都是一樣的事情，右邊這些步驟其實都是一樣的任務，輸入是一串的 token，輸出就是一個 token，決定下一個 token 是什麼。而 token 的可能性是有限的，所以就是給一串 token 做一個選擇題，決定下一個 token 應該是什麼。", "label": 1}
{"text": "簡而言之，右側步驟的本質相同：輸入一系列token，輸出一個token，從有限的選項中選出下一個token。", "label": 0}
{"text": "講到這邊我決定改變一下符號，我們可以不失一般性地，把給一串 token 產生下一個 token 這件事情，表示成輸入是 z1 到 zt-1，輸出是 zt。那有人可能會問說老師你怎麼可以把 x 跟 y 沒有做區別，都用 z 來表示呢？那麼 x 跟 y 難道不是不同的東西嗎？你想想看，如果今天我們做的是一個輸入文字，輸出文字，像一般的文字模型，一般的語言模型一樣，那樣的模型，如果我們做的是一個輸入文字輸出文字的模型，輸入的所有x 都是文字token，輸出所有y 也都是文字token。它們是沒有本質上的差異的，它們都是文字的token，所以我們可以不區別x 跟y，就用z 來表示。", "label": 1}
{"text": "為簡化表示，我們將產生下一個 token 的過程，改用統一符號 z 表示。輸入序列為 z1 到 zt-1，輸出為 zt。有人可能會有疑問，為何將輸入 x 和輸出 y 都用 z 表示？其實，在許多文字模型中，輸入和輸出都是文字 token，本質上並無區別，故使用統一符號 z  更簡潔明瞭。", "label": 0}
{"text": "那你說如果我今天是輸入影像輸出文字，如果輸入跟輸出的模態不一樣怎麼辦呢？那x 跟y 它是不同的token的集合啊，影像有影像的token，文字有文字的token，那就告訴你我們把影像跟文字的token直接集合起來，當作一個新的token的集合。影像可能有4096 個token，文字有30000 個token，我就說一個新的token的集合是30000 加4096，是34096，是34096個 token，它們合起來是一個新的集合就叫做z。", "label": 1}
{"text": "所以，影像輸入文字輸出，輸入和輸出類型不同該如何處理？  因為影像和文字的tokens不同，我們將影像tokens和文字tokens合併成一個新的tokens集合。例如，影像有4096個tokens，文字有30000個tokens，那麼新的集合z就包含34096個tokens。", "label": 0}
{"text": "所以不管是你要輸入什麼、輸出什麼，我們都可以不失一般性地說，就是輸入一串 token，輸出一個 token，所以不管是你要文字生文字，文字生圖、圖生文字、語音生文字、文字生語音都是一樣的事情，它們背後的原理其實是一樣的。那再來要問的問題就是，怎麼輸入一串 token 決定下一個 token 是什麼呢？你需要一個函式 (function)，我們這邊寫作 f ，這個函式 (function) 它的輸入就是 z1 到zt-1，輸出就是zt，那這個函式 (function) 其實就是類神經網路 (neural network)。那我實際上這個 neural network 在運作的時候，並不是真的產生一個 token，它產生的是一個 token 的機率分佈。因為 token 的可能性是有限的，所以類神經網路 (neural network) 真的輸出是，它會給每一個 token 一個分數，代表這個 token 作為下一個 token 有多合適，這個 token 當作下一個 token 的機會有多大。", "label": 1}
{"text": "無論輸入輸出為何，皆可視為輸入一系列 tokens，輸出單個 token。因此，文字生文字、文字生圖、圖生文字、語音生文字、文字生語音，本質上相同。關鍵在於如何根據輸入 tokens 判斷下一個 token。此過程需藉由函式 f (z1 至 zt-1 輸入，zt 輸出)，而此函式即為神經網絡。神經網絡實際上並非直接產生 token，而是產生各個 token 的機率分佈，為每個 token 賦予分數，代表其成為下一個 token 的可能性。", "label": 0}
{"text": "為什麼要這樣做呢？因為你想看看假設我們現在在做文字接龍，我告訴你說輸入的 token 是臺灣大，問你接下來可以接哪一個 token？那各位同學可能會很直覺地接一個學出來，臺灣大可以接學這樣沒有問題，但是臺灣大後面還可以接很多其他東西啊！比如說可以接車，臺灣大車隊。可以接哥，臺灣大哥大。你可以接的東西太多了，所以給一個 token sequence (符記序列)，接下來可以接哪一個 token，往往答案並不是唯一的。", "label": 1}
{"text": "文字接龍的例子說明了，給定「臺灣大」這個詞，後續詞語並非唯一。「學」是常見的接續，但「車」、「哥」等也同樣合理，例如「臺灣大車隊」、「臺灣大哥大」。因此，預測下一個詞彙(token)並非單一解題。", "label": 0}
{"text": "所以你強迫類神經網路 (Neural Network) 輸出唯一的答案，它很有可能反而很難學會，它很有可能反而非常的錯亂。所以這邊的設計是讓它輸出的是一個機率分佈，讓它告訴你它覺得每一個 token 接在後面的可能性有多大，那產生這個機率分佈以後，再按照這個機率分佈去擲一個骰子，決定真正產生出來的 token 是什麼，那就是因為有這個擲骰子的過程。所以今天這些生成式 AI，在產生答案的時候，就算是一樣的輸入，每次產生的輸出也會是不一樣的。", "label": 1}
{"text": "因此，強迫神經網路預測單一結果反而會降低其學習效率並導致錯誤百出。本設計改採機率分佈輸出，讓模型評估每個token的接續可能性。隨後，透過隨機抽樣決定最終產生的token，此機制確保每次生成結果皆有所不同，即使輸入相同。", "label": 0}
{"text": "那接下來我們來講一下什麼是類神經網路 (Neural Network)，它特別的地方在哪裡？那類神經網路啊，我知道你可能在很多其他地方都聽過一大堆對於類神經網路的解釋，那通常都是說類神經網路，模擬人腦的運作方式......，所以很厲害等等科普文章就是這樣寫的。", "label": 1}
{"text": "現在我們來談談類神經網路，以及它與眾不同的特點。你可能已經在許多地方接觸過類神經網路的介紹，多半會提到它模仿人腦運作，因此功能強大之類的說法。", "label": 0}
{"text": "那類神經網路 Neural Network，作為一個函式 function，它真正的特色是什麼呢？它真正的特色是把一個函式 function f，拆解成很多小的函式 function f1 到 fL 的串聯，也就是說這個 f 實際上運作的過程是，有一堆輸入先通過第一個 f1 產生一堆輸出，那這邊的輸出呢？這邊每一個方框呢代表一個向量 (Vector)，然後呢這些輸出會再變成 f2 的輸入，產生新的輸出，這個步驟會持續一直下去，直到 fL 接受到輸入的時候，它會輸出下一個 Token 的機率分佈。這邊每一個小的函式 (function) 又叫做 Layer，所以一個 f 裡面有很多的 Layer，那就是因為把一個函式 (function)，拆解成多個串聯的函式 (function)，拆解成多個 Layer，所以類神經網路 (Neural Network)又叫做深度學習 (Deep Learning, DL)，深指的就是有很多 Layer 的意思。", "label": 1}
{"text": "神經網路的獨特之處在於將函數f分解成多個函數f1到fL的組合。  輸入先經過f1產生輸出向量，此輸出成為f2的輸入，依此類推，直至fL輸出下一個Token的機率分佈。每個小函數稱為一層 (Layer)，因此一個函數包含多層，  多層的結構使得神經網路也稱作深度學習，\"深\"即指多層。", "label": 0}
{"text": "類神經網路真正的特色就是，把一個問題拆解成 L 個步驟，這邊 L 個步驟指的是 Layer 層的數目。", "label": 1}
{"text": "類神經網路的核心機制在於將問題分解成多個步驟，步驟的數量等同於網絡的層數。", "label": 0}
{"text": "那接下來我想跟大家說明的是，為什麼把一個步驟拆解成多個步驟，會是一件有效有用的事情呢？這邊給一個不精準的比喻，我們假設每一個 layer 它就是一個表格，每一個 layer 作為一個函式，它做的事情就是查表，你給它一個輸入，它每個表格告訴你輸出應該長什麼樣子。", "label": 1}
{"text": "所以，為何將單一步驟細分為多個步驟能提升效率？  想像一下，每個步驟就像一個資料表，每個步驟如同一個函數，進行查表作業，輸入數據後，每個資料表會回傳對應的輸出結果。", "label": 0}
{"text": "那現在假設我們要人工智慧做一個，非常非常簡單的問題，做三個個位數的相加，假設你只有一個 layer 要一步到位產生答案，那這個表格需要存多少輸入輸出的關係呢？想想看有多少可能的輸入，有10 × 10 × 10，1000種可能的輸入，10 10 1000 種可能的輸入，所以這個表格需要存1000 種可能的輸入跟輸出間的關係。但假設我們把這個問題拆解成兩個步驟A、B、C，不需要一次到位，先把A 跟B 加起來變成B'，再去計算B' C，那第一個 layer 要做的事情是計算A、B的答案是多少？那這個時候你的輸入就只有10 × 10種可能，而第二個步驟要計算B' 加C是多少，那這個時候B' 有19個可能，C有10個可能，所以這個時候是有19 × 10個可能的輸入，所以需要存的只有10 × 10加上19 × 10可能的輸入輸出關係，所以你會發現，當我們把一個問題拆解成多個步驟的時候，我們是從複雜的問題變成簡單的問題。", "label": 1}
{"text": "如果要訓練一個單層神經網路直接計算三個個位數的和，則需要儲存1000組輸入輸出對應關係。但如果將其分解為兩個步驟，先計算前兩個數的和，再與第三個數相加，則第一步驟需儲存100種輸入輸出關係，第二步驟需儲存190種輸入輸出關係，總共只需儲存290組關係。因此，將問題分解成多個步驟可以有效減少所需儲存的資料量，降低模型複雜度。", "label": 0}
{"text": "所以很多人對於深度學習（DL）有誤解，覺得說深度學習（DL）就是弄一個很複雜的東西。其實不是，深度學習（DL）是把本來複雜的問題變得比較簡單，這就是為什麼深度學習（DL）往往會有好的效果。", "label": 1}
{"text": "深度學習並非如許多人誤解的那樣複雜，它反而能簡化複雜問題，因此往往能取得良好效果。", "label": 0}
{"text": "好，那以上是一個不精確的比喻了。如果你想要知道從數學上怎麼說明，給一個函式用深度學習（DL）的方法，會比用淺層學習的方法還要更有效率，從數學上怎麼證明它，請看以前我講過的深度學習（DL）理論系列影片，那我就把連結放在這個投影片上。", "label": 1}
{"text": "是的，先前的比喻不夠精確。關於深度學習比淺層學習更有效的數學證明，請參考我之前的深度學習理論系列影片，連結已附於投影片。", "label": 0}
{"text": "那我們現在已經知道把一個問題拆解成多個步驟會有用，同樣的道理也可以拿來說明為什麼讓機器思考會有用。如果我沒有讓機器思考，給它問題，它讀完問題之後，就必須立刻產生答案。那從問題到答案中間有很多個 layer，每個 layer可以想成是一個思考的步驟，但往往困難的問題需要很多很多思考的步驟，layer的數目是有限的。", "label": 1}
{"text": "將複雜問題分解成多個步驟的有效性，同樣適用於機器思考的益處。若不允許機器思考，直接輸入問題要求其立即作答，則問題與答案間的許多步驟（如同思考過程）將被忽略，而複雜問題通常需要大量步驟，而機器能處理的步驟數目是有限的。", "label": 0}
{"text": "如果 layer不夠，那要怎麼辦呢？那讓機器思考，讓機器演腦內小劇場，可以想成是從另外一個方向，擴展了類神經網路的深度。所以今天給一個問題，你的這個機器，你的人工智慧，不是馬上產生答案，而是有一個思考的過程，思考完之後才產生答案，這個時候從問題到答案間，變成有非常多的思考的步驟，不再侷限於 layer的數目，而是跟長度思考過程的長度有關，所以這邊的 slogan 就是深度不夠長度來湊。", "label": 1}
{"text": "深度不足，那就增加思考步驟，讓模型透過更長的推理過程來彌補。  這相當於從側面拓展了模型的深度，使最終的答案來自更深入的思考，而非僅依靠網絡層數。  因此，問題到答案之間的過程包含了大量的中間步驟，解決方案的關鍵在於思考過程的長度，而非網絡層數本身。  換言之，深度不夠，就用更長的思考來補足。", "label": 0}
{"text": "今天一個類神經網路 (Neural Network) 的深度是有限的，但是思考的過程可以是要多長有多長。而這邊稍微補充一下，有的同學如果很熟悉類神經網路 (Neural Network) 的運作的話，你可能會說這種深度不夠長度來湊的方式，跟一般的類神經網路 (Neural Network) 不一樣啊。因為這邊每一個紅色箭頭裡面的參數，通過的參數都是一樣的啊，這樣會有用嗎？那我推薦你一篇史前時代19年的文章叫做ALBERT，那時候他就是告訴你說，就算是每一個 layer 都是一樣的，把同樣的 layer 疊很多次還是可以有比較好的結果的。", "label": 1}
{"text": "類神經網路的深度固然有限，但其運算過程卻能無限延伸。然而，熟悉類神經網路運作機制者可能會質疑這種以長度彌補深度不足的方法，因為各層參數相同，其有效性令人存疑。对此，可參考19年前的ALBERT論文，該論文已證明即使各層結構相同，重複堆疊也能提升效能。", "label": 0}
{"text": "那深度不夠長度來湊這件事啊，現在又叫做Testing Time Scaling。那我第一次聽到Testing Time Scaling 這個詞，是在 OpenAI 發表的時候，OpenAI 寫了一個 blog 告訴大家說，o1 厲害的地方就是 Testing Time Scaling (測試時間縮放)，用白話來講就是深度不夠長度來湊。那這是 OpenAI 他們的 blog 裡的附圖啦，不過因為現在 OpenAI 釋出資訊的時候，都釋出的非常的隱晦，像橫軸他說這個叫做 Testing Time Compute，欸，那他單位是什麼也沒講清楚啊，所以你很難知道他實際上做的事情。", "label": 1}
{"text": "OpenAI在部落格中介紹其模型O1的測試時間縮放 (Testing Time Scaling) 特性，此技術本質上是以增加計算量來彌補模型深度不足的問題，然而OpenAI的說明相當含糊，例如圖表橫軸的「測試時間計算量」缺乏明確單位，使得其方法細節難以理解。", "label": 0}
{"text": "所以這邊引用的數據呢，是來自於 Stanford 大學的另外一篇 paper，叫做 s1: Simple Testing-Time Scaling。他們就告訴你說長度來湊還真的是有用的，橫軸是思考的時候用了幾個 token，縱軸是在不同任務上的正確率，你發現說想的越長正確率就越高。那講到這邊你可能會想說，那他們是如何操控思考的長度呢？其實這篇論文裡面用的是一個非常粗暴的方法，它的方法就是每一次這個語言模型產生結束的符號的時候，直接把那個符號拿掉，換成wait這個字，有語言模型本來想結束了，突然發現我怎麼說不了結束，這個符號怎麼變成wait，只好繼續做文字接龍，強迫它一直講下去，這個就是他們怎麼控制語言模型輸出長度的方法。", "label": 1}
{"text": "Stanford大學一篇名為「Simple Testing-Time Scaling」的論文提供了相關數據，實驗結果顯示：思考步數(token數)與任務準確率正相關，思考步數越多，準確率越高。該論文採用簡陋但有效的方法控制思考長度：當模型產生結束符號時，將其替換為\"wait\"，迫使模型繼續生成文本。", "label": 0}
{"text": "那接下來我們來看一個layer層中又發生了什麼樣的事情，一個layer層裡面現在通常都還有更多的layer層，所以一個layer層它不是一個layer層，它其實裡面還有很多的layer層，所以一個layer層它是一個函式，它其實又是有很多更小的函式所串聯而成的。現在通常一個layer層中的layer層，一個layer層中的函式它還有分成兩類，一種是叫做self-attention layer，這種layer層在產生輸出的時候，是會考慮全部的輸入再去產生輸出，所以你可以用這種方法去考慮輸入的全部的資訊。那也會有一些 layer，它只針對單點做思考，根據一個 token去做更深入的思考，所以今天一個 layer裡面，會有考慮全局的 layer，會有考慮全局的函式，也會有針對單一的 token在深入思考的函式。", "label": 1}
{"text": "深入探討單個層級的內部結構，發現其通常包含多個子層級，因此單一層級並非單一結構，而是由多個子層級組成，實質上是許多更小函式的組合。這些子層級函式主要分為兩類：一種是全域注意力層，其輸出考慮所有輸入資訊；另一種則專注於單個輸入單元，進行更深入的處理。所以，一個層級內同時存在處理全局資訊和局部資訊的機制。", "label": 0}
{"text": "在這門課的第三講跟作業三，我們還會在詳細剖析一個 layer中發生了什麼事。其實今天有這種 self attention layer 的類神經網路，又叫做通常被統稱為 Transformer。", "label": 1}
{"text": "課程第三講和作業三將深入探討一個layer的內部機制，此處所指的神經網絡模型，即常被稱為Transformer的自注意力機制層。", "label": 0}
{"text": "那其實 Transformer 有很多的變形啦，最原版的 Transformer 2017年發表的那個 Transformer，其實跟今天的 LLaMA、ChatGPT、DeepSeek 其實還是略有差別啦。", "label": 1}
{"text": "Transformer 架構衍生出許多變體，2017 年最初的 Transformer 與現今的 LLaMA、ChatGPT 和 DeepSeek 等模型在細節上存在一些差異。", "label": 0}
{"text": "不過今天只要有 Self Attention 的 Layer 通常就統稱為 Transformer，那Transformer 又是變形金剛啦！所以今天一提到 Transformer 你一定要畫一個變形金剛，為什麼這個 Transformer 要叫 Transformer 要被命名成變形金剛呢？論文中沒有寫，很多人是猜測說是不是因為它會把輸入變成輸出所以叫 Transformer，那輸入變成輸出的東西太多啦，那每個東西都是 Transformer 了。", "label": 1}
{"text": "現今只要模型包含自注意力機制，便常被簡稱為Transformer，而Transformer名稱的由來眾說紛紜，有人推測是因其能將輸入轉換為輸出，但這種解釋過於籠統，難以令人信服。", "label": 0}
{"text": "在去年這個 Transformer 的原作者，有一次接受紐約客的採訪，然後他們有人問他這個問題，為什麼 Transformer 要叫 Transformer？其中一個作者說他從來不知道，為什麼這個模型要叫 Transformer， 當初就不知道為什麼命名為 Transformer，他覺得這個名字很酷沒有什麼特別的原因。", "label": 1}
{"text": "去年，《紐約客》採訪了Transformer的其中一位作者，詢問模型名稱的由來。該作者表示他並不知道為何取名Transformer，只是覺得這個名字很酷。", "label": 0}
{"text": "那 Transformer 還是，雖然說今天的語言模型一般它的基底都是用 Transformer，不過 Transformer 還是有很多的限制，尤其是最大的限制是當輸入太長的時候，Transformer 的運作可能就會出現問題，尤其是今天我們希望模型演腦內小劇場。那當模型在演腦內小劇場的時候，Transformer 的輸入可能就會非常的長，因為 Transformer 裡面它有Self Attention的layer，Self Attention通常是看過整個輸入以後才能夠給入，才能夠給你輸出。", "label": 1}
{"text": "基於Transformer的語言模型雖已普及，但仍受限於長輸入序列的處理能力。  尤其在模擬複雜情境，例如腦內小劇場時，長序列輸入會導致Transformer的Self-Attention機制效率低下，因為Self-Attention需要完整讀取輸入才能產生輸出。", "label": 0}
{"text": "所以今天輸入的 token 的長度越長，那  Transformer  要考慮的東西就越多，它的運算量就越大，所以這個長度沒辦法無限地延長下去。有沒有什麼樣可能的解決方法呢？有沒有其他的類神經網路 (Neural Network) 架構更適合拿來處理長的輸入呢？今天看到的一個可能性就是很多人在討論的 Mamba。這是另外一種類神經網路 (Neural Network) 的架構。那其實 Mamba 跟 Transformer 也只是一線之隔，所以這邊畫了一個機器蛇娘就是 Transformer 其中一種變形，其實就是 Mamba，Mamba 再稍微改一下其實就變成 Transformer。", "label": 1}
{"text": "Transformer處理長序列輸入時，運算量與輸入長度成正比，故長度受限。  為此，研究者探索替代方案，例如Mamba架構。Mamba與Transformer結構高度相似，可視為Transformer的變體，兩者僅存在細微差異。", "label": 0}
{"text": "那我們到第四講的時候會再來講類神經網路 (Neural Network) 的架構。好，那接下來呢，我們要講這一些運作機制是怎麼被產生出來的。首先要跟大家講一個非常重要的觀念，類神經網路 (Neural Network) 裡面分成架構跟參數這兩部分。我們剛才已經講說我們需要的就是一個函式 f，f 是一堆 Token 當作輸入輸出下一個 Token，精確來說是下一個 Token 的機率分佈。", "label": 1}
{"text": "第四講會深入探討類神經網路架構。現在，我們來了解這些機制的生成過程。關鍵概念是：類神經網路包含架構和參數兩部分。前面提過，我們需要一個函數f，其輸入為一系列Token，輸出為下一個Token的機率分佈。", "label": 0}
{"text": "然後我們又告訴你說，這個f 其實是有很多小f 串聯在一起的。那其實呢，這一個f 裡面分成兩部分，分成架構 (architecture) 跟參數 (parameter)。所謂的架構是由開發者，也就是人類所決定的部分，參數則是由訓練資料，不是由人類所決定的部分。", "label": 1}
{"text": "我們進一步說明，這個f實際上是由許多更小的f組成。  f本身又可分解為架構和參數兩個部分。架構是人工設計的，而參數則來自訓練數據，而非人工設計。", "label": 0}
{"text": "所以像剛才深度學習 (DL) 裡面啊，這個架構，也就是很多層串聯在一起這件事，這是一個架構，是由人類決定的，但是每一層這個要做什麼樣的事情，其實是由參數所決定的。", "label": 1}
{"text": "因此，深度學習架構中，多層網絡的串聯結構由人工設計，但各層的具體運作則由模型參數決定。", "label": 0}
{"text": "那如果要打個比喻的話，我們可以把架構想成是人工智慧的天資，而參數是他後天努力的結果。那像Transformer 是他的天資，是他一出生的時候，在還沒有用任何訓練資料做學習的時候就已經有的東西，但是有了架構還不夠，要後天學習決定參數的數值，才能夠變成一個有用的函式。", "label": 1}
{"text": "換個角度來看，架構如同AI的先天條件，參數則代表其後天訓練的成果。Transformer的架構就好比AI的先天稟賦，在其未經任何數據訓練前便已存在，但僅有架構是不夠的，還需要後天學習調整參數，才能發揮作用。", "label": 0}
{"text": "那像架構這種東西啊，現在又被叫做超參數 (hyperparameter)。所以你常常會聽到有人說，做深度學習 (DL) 就是在調參數，大家都是參數狗，但他指的參數並不是需要由訓練資料決定的參數。因為真正的參數是由訓練資料決定的，你根本不需要人工來調，所以當有人說他在調參數的時候，指的是調超參數 (Hyperparameter)。", "label": 1}
{"text": "深度學習模型的架構，現在也稱為超參數。因此，「深度學習就是調參數」這句話，指的是調整超參數而非模型訓練過程中自動學習的參數。模型的實際參數由訓練資料決定，不需要人工調整。", "label": 0}
{"text": "比如說類神經網路 (Neural Network) 的架構，那你在看這個文獻的時候啊，常常會聽到有人說模型有幾B 幾B。7 B 模型、70 B 模型，這邊的7 B、70 B 指的就是參數的數量，這邊B 是 Billion 的縮寫，所以7 B 代表這個模型有7 個 Billion 的參數，一個 Billion 是10 億，所以就是70 億個參數，70 B 就是700 億個參數了。那參數的數量其實也是架構 (architecture) 的一部分啦，因為一個模型裡面會有幾個參數，這個是事先就決定好的。", "label": 1}
{"text": "論文中常提及模型大小，例如7B、70B參數模型。B代表十億(Billion)，7B即70億參數，70B則為700億參數。參數數量是模型架構的決定性因素，在設計階段就已預先設定。", "label": 0}
{"text": "但是這每一個參數的數值，就是透過訓練資料所決定的，就是透過訓練資料得到的了。那以後呢，如果我們要特別強調一個函式裡面的參數的話，我們會用 f_θ 來表示它，我們會用 θ (theta) 來表示一個類神經網路的參數。", "label": 1}
{"text": "神經網路的參數值，來自訓練數據；未來提及函數參數，將以f_θ表示，其中θ代表神經網路的參數。", "label": 0}
{"text": "那接下來呢，我們要講找出參數的概念，那更詳細的內容，實際上的操作，請大家參考過去的影片。那怎麼找出參數呢？這邊你就需要準備訓練資料，那我們的訓練資料就是告訴這個機器說，輸入是什麼樣的 token 的時候，應該指哪一個 token 是正確的。你要告訴它，輸入「你是誰？」，接下來就要輸出「我」，輸入「你是誰？我」，後面就要輸出「是」，輸入「你是誰？我是」，後面就要接「人」，輸入「你是誰？我是人」，後面就要接「工」。", "label": 1}
{"text": "好的，接下來我們要說明如何找到參數。更深入的細節和實際操作步驟，請參考之前的影片。如何尋找參數呢？你需要準備訓練數據，這些數據會告訴模型，給定特定輸入（token）時，正確的輸出（token）是什麼。例如，輸入「你是誰？」，預期輸出「我」；輸入「你是誰？我」，預期輸出「是」；輸入「你是誰？我是」，預期輸出「人」；輸入「你是誰？我是人」，預期輸出「工」。", "label": 0}
{"text": "然後或者是叫它寫程式的話，就是有一個有關寫程式的指令。接下來告訴它說，那如果要寫這段程式的話，第一個要輸出的符號應該是print，有了print 之後，接下來輸出左括號，有了左括號之後，接下來要輸出引號，如果是解數學問題，就告訴它說，看到這個數學問題之後，後面要接令這個字，令後面要接x，x 後面要接等於。", "label": 1}
{"text": "撰寫程式碼時，需使用print指令開始，接著是左括號和引號。若為數學題，則需寫上「令x=」。", "label": 0}
{"text": "依此類推，準備一大堆的訓練資料，有了這些訓練資料以後，你就可以找一組參數 θ (theta)，找一組參數θ (theta)，因為參數有很多個，所以我們用一組來稱呼比較適合。", "label": 1}
{"text": "以此類推，收集大量的訓練數據。有了這些數據，就能找到最佳參數集 θ。", "label": 0}
{"text": "找一組參數 θ (theta)，它能夠讓 f_θ 最能滿足訓練資料，什麼叫做滿足訓練資料呢？這個意思就是說假設輸入你是誰，訓練資料說要輸出我，那我們把這段文字丟到 f_θ 裡面，它就要吐我出來，或者是訓練資料告訴我們說「你是誰？我」，後面要接「是」，那你把這段 token 丟到 f_θ 裡面就要輸出「是」出來。", "label": 1}
{"text": "目標是找到一組參數θ，使函數f_θ能準確預測訓練數據的輸出。例如，若輸入為「你是誰」，訓練數據指定輸出為「我」，則f_θ(「你是誰」) 應輸出「我」；同理，若輸入為「你是誰？我」，則f_θ(「你是誰？我」) 應輸出「是」。", "label": 0}
{"text": "講得更精確一點，實際上我們要 f_θ 輸出的是一個機率分佈 (Probability distribution)，所以當我說要它輸出我的時候。其實意思是希望我這一個符號這個 Token 得到的分數是最高的，要比其他 Token 都還要高，那我們尋找參數也就是訓練的目標，就是找到一組 θ (theta)，讓 f_θ 最能滿足我們提供的訓練資料。", "label": 1}
{"text": "目標是訓練模型f_θ，使其輸出機率分佈，讓目標Token的機率在所有Token中最高，從而最佳擬合訓練資料。", "label": 0}
{"text": "那在作業4 開始呢，我們會開始訓練模型，那到時候你會更清楚怎麼訓練一個模型，那其實給一個 token的 sequence (序列)，找下一個 token是一個選擇題。因為 token的數目是有限的，那在選擇題呢讓機器做選擇題，在機器學習 (ML) 的文獻中又叫做分類的問題，那機器學習 (ML) 中的分類問題其實從來都不是新的問題有很多的應用，比如說信用卡盜刷偵測，給一個交易記錄，那人工智慧要輸出這是盜刷還是不是盜刷，這是個選擇題。", "label": 1}
{"text": "作業四將著手訓練模型，屆時你將更了解模型訓練過程。本質上，給定一個token序列，預測下一個token如同解答選擇題。由於token數量有限，此選擇題形式的機器學習問題，在機器學習文獻中稱為分類問題。這並非機器學習的新課題，應用廣泛，例如信用卡盜刷偵測，系統根據交易記錄判斷是否為盜刷，這也是一個分類問題。", "label": 0}
{"text": "現在 Gmail 有垃圾郵件偵測的功能，給一封信的內容，人工智慧要說這是垃圾郵件還是不是垃圾郵件，是或不是這是一個有兩個選項的選擇題問題。甚至下圍棋也是一個選擇題的問題，輸入是棋盤上黑子跟白子的位置，輸出是下一步可以落子的位置，棋盤上可以落子的位置最多就是 19 × 19 的位置，所以它也是一個有 19 × 19 個選項的選擇題。所以分類的問題，讓機器決定下一個token 可能是哪一個 Token，這樣的問題其實從來都不是新的問題，人類很早就知道怎麼做分類的問題，既然人類知道怎麼做分類的問題，而生成式AI就是一連串分類問題的集合，所以生成式人工智慧也從來不能夠說是全新的技術。", "label": 1}
{"text": "Gmail 的垃圾郵件篩選機制，本質上是讓AI判斷郵件是否為垃圾郵件的二元分類。同樣地，圍棋也是個選擇題，AI需從361個可能的落子點中選出最佳位置。  因此，文字生成中的詞彙預測，選擇下一個token，也是個分類問題。  這類分類任務並非AI的獨創，人類早已熟知其方法。  生成式AI不過是許多此類分類任務的組合，並非革命性的全新技術。", "label": 0}
{"text": "其實要讓機器產生複雜而有結構的物件，比如說一句話很早以前就能夠辦到，比如說翻譯也可以看作是生成式人工智慧技術的一個展現。", "label": 1}
{"text": "事實上，生成複雜結構化的內容，例如句子，早已實現；機器翻譯便是生成式AI技術的例證。", "label": 0}
{"text": "模型可以輸入一段文字，輸出另外一個語言的一段文字，那翻譯當然不是人工智慧全新的應用，Google 翻譯至少已經存在了 15 年以上。但今天的生成式人工智慧跟過去這些只能夠做單一任務的「專才」其實有很大的不同，今天的生成式人工智慧往往是一個「通才」，它可以做的事情不是只有一件，你要讓它做事的時候，你要很明確地告訴它你要它做什麼，他才有辦法按照你的指令來產生正確的行為，而下指令這件事情又叫做 Prompt (提示)，你今天要正確地 Prompt (提示)，這些通才的模型他才有辦法給你正確的答案。", "label": 1}
{"text": "翻譯並非AI的新功能，Google翻譯已問世逾十五年。然而，當今的生成式AI與以往只能執行單一任務的專精模型截然不同，它更像是一個多面手。要有效利用它，必須給予明確指令，此指令即為Prompt。正確的Prompt才能引導模型產生正確的輸出。", "label": 0}
{"text": "接下來我們要講這些通才，這些通用的模型是怎麼被發展起來的。剛才講到翻譯我們就從翻譯開始講起，過去在開發這些翻譯系統的時候，往往甚至不同語言的翻譯就是不同的系統，有一組人開發中英翻譯，另外一組人開發德英翻譯，另外一組人開發德法翻譯，不同語言的翻譯就是不同的系統。", "label": 1}
{"text": "我們接下來探討這些泛用模型的開發歷程，以機器翻譯為例：以往的翻譯系統往往是針對特定語言對開發的，例如中英、德英、德法翻譯系統是各自獨立開發的。", "label": 0}
{"text": "那有人就想說那不同的語言的翻譯要開發不同的系統，那我們開發得完這麼多翻譯系統嗎？世界上有 7000 種語言，如果每一種成對的語言都要開發翻譯系統，那要開發 7000，7000 個翻譯系統永遠也開發不完。", "label": 1}
{"text": "開發所有語言的翻譯系統是不切實際的，世界上有七千多種語言，需要開發的語言對數目龐大，難以完成。", "label": 0}
{"text": "所以怎麼辦呢？於是有了通用翻譯的想法，能不能夠有一個通用的翻譯系統，直接告訴他我們現在要把哪個語言翻成哪個語言，比如說中文翻英文，給他中文他就自動知道要翻譯成英文了呢。", "label": 1}
{"text": "因此，解決方案是開發一個通用翻譯系統，讓使用者指定源語言和目標語言（例如，中文翻譯成英文），系統就能自動進行翻譯。", "label": 0}
{"text": "那這樣的通用翻譯有什麼樣的好處呢？他甚至有可能有能力翻譯沒見過的語言，對比如說假設你教過他中英翻譯，德英翻譯還有德翻法，搞不好自動就會中文翻法文了。", "label": 1}
{"text": "通用翻譯的優勢在哪裡？它或許能翻譯從未接觸過的語言，例如，訓練它中英、德英和德法翻譯後，它可能自動學會中法翻譯。", "label": 0}
{"text": "因為這一些通用翻譯的模型，也許會學到說不管輸入哪一種語言，都把它變成一種內部只有機器自己看得懂的「內部語言」，它可以把這個內部語言再翻成任何語言，所以就算有些語言成對的資料我們從來沒有見過，也並不代表通用翻譯沒辦法進行這種成對語言的翻譯。而這件事情在什麼時候人類就知道了呢？什麼時候人類就知道開發這種通用翻譯有好處呢？2016年的時候，至少2016年的時候就已經知道了。", "label": 1}
{"text": "通用翻譯模型可能將所有語言轉換為內部表示，再轉換為目標語言。因此，即使缺乏特定語言對的訓練數據，通用翻譯仍可實現跨語言翻譯。  這種方法的可行性，至少在2016年就已被認識到。", "label": 0}
{"text": "以下的內容是來自於 Google 的一個 blog，他們告訴你說，你看我們就教這個語言，我們就教我們的生成式人工智慧，Google 翻譯幾種翻譯，比如說日文翻英文，從來沒教過它日文翻韓文，它自動會了，從來沒教過它韓文翻日文，它自動會了。而且他們也發現說這些模型內部，確實有一種他們自己才看得懂的內部語言，怎麼說呢？同樣意思的英文、韓文跟日文的句子，丟給這個模型的時候，它內部的反應是一樣的。", "label": 1}
{"text": "Google的生成式AI模型，經由訓練能執行日英互譯，令人驚奇的是，未經訓練也能自動完成日韓、韓日互譯。  研究顯示模型內部存在一種共通的語言表示，相同的語義，無論是英文、韓文或日文，模型內部的反應皆一致。", "label": 0}
{"text": "雖然是不同的句子，但是同樣的意思，對這些模型來說，它聽起來就像是同樣一句話，所以模型可以把這些不同的語言，內化成一個它內部的語言。所以如果用比較擬人化的講法，可以說這些模型創造了一個，只有它自己懂的內部語言，但實際上類神經網路是怎麼運作的，這個我們日後再講，不同語言可以共用模型。", "label": 1}
{"text": "多種語言輸入能被模型整合，形成模型內部的共通表示；換句話說，模型建立了一套獨有的內在語言來理解不同語言，其運作機制日後再詳述，共享模型的可能性由此產生。", "label": 0}
{"text": "那不同任務有沒有辦法共用模型呢？剛才講了翻譯，那這個自然語言處理 (Natural Language Processing, NLP)，還有很多的任務，比如說摘要，比如說作文批改，它們都是輸入文字輸出文字，能不能乾脆共用一個模型，這個模型就是給它任務說明，給它一段文字，根據任務說明就做它該做的事？", "label": 1}
{"text": "能否利用單一模型處理不同的自然語言處理任務？例如，翻譯、摘要和作文批改都涉及文字輸入和輸出，那麼能否設計一個模型，僅需提供任務指令和文本，就能完成不同的任務？", "label": 0}
{"text": "至少早在 2018 年，就已經有人在公開的文章中提過類似的想法，那我這邊引用的論文是一篇叫做 Multitask Learning as Question Answering 的論文。這篇論文其實是辦了一個比賽，這個比賽是希望有人可以用一個模型解十個自然語言處理的任務，這個模型要能夠吃不同的指令，那這些指令現在在那篇論文裡面叫Question，我們現在叫Prompt，能夠吃不同的指令就做不同的事情。", "label": 1}
{"text": "早在2018年，一篇名為《Multitask Learning as Question Answering》的論文便已提出類似的概念。該論文設計了一項比賽，旨在開發一個單一模型，通過不同的指令（論文中稱之為Question，現稱之為Prompt）來完成十項自然語言處理任務。", "label": 0}
{"text": "當然從今天回想起來，只用一個模型做十個任務實在是太少了。但是在那個時候2018年的時候，人們已經覺得這個想法太瘋狂了，所以其實沒幾個人真的去參加這個比賽，那在2018年的時候覺得，不同任務要共用一個模型，好像非常的困難。不過後來隨著通用模型的發展，這件事情越來越可行，但是我要強調一下通用跟通用，它的意思是不一樣的。", "label": 1}
{"text": "回顧2018年的比賽，當時僅用單一模型執行十項任務的作法，確實顯得過於簡潔；然而，這樣的想法在當時被普遍認為相當激進，參賽者寥寥無幾。  使用單一模型處理多項任務的理念，在當時看來極具挑戰性。  儘管通用模型的進步使這種作法日益可行，但必須指出，「通用」本身涵蓋了不同的層次與定義。", "label": 0}
{"text": "其實在歷史上的不同時期，「通用」這個字它有不同的含義，這些機器學習(ML)的通用模型，可以說經過了三個形態的演化。第一個形態大概是2018到2019年的時候，對於這個年代大家不要看得太認真，因為模型的改變是一個漸變的過程，所以你很難說某個年代到某個年代，只有某一種模型，或者說大約這個期間。多數人在想的是第一形態的通用模型，這種模型又叫做encoder (編碼器)，這些模型如果在NLP (自然語言處理)上，它可以吃一段文字作為輸入，但它沒辦法輸出文字，它只能夠輸出一些 representation，輸出一堆人看不懂的向量，代表它對這段輸入文字的理解。那這個第一形態的通用學習模型，最知名的就是芝麻街家族，不知道為什麼在那個年代，模型一定要用芝麻街的人物來命名，就算是跟模型的縮寫也沒什麼關係，也要硬說它是一個芝麻街的人物。那這些模型沒辦法直接用，如果你要把這個通用模型用在摘要上，那你得在它後面接一個特化的，負責做摘要的模型，它才能輸出摘要。要做翻譯得接一個特化的翻譯模型，才有辦法做翻譯。所以這個概念很像是加一個外掛，那這些通用模型要掛上不同的外掛，就可以做不同的事情，這是第一形態。後來就進入了第二形態，大概是2020年到2022年的時候，這個時候的通用模型，有完整的文字生成的功能，可以輸入一段文字，輸出一整段文字，比如說GPT-3就是其中的代表。那這些模型你很難用指令操控它，你很難要求它針對你的指令做特定的任務，所以假設你要拿這種模型做摘要，你得微調它內部的參數，它本來的參數是 θ，你根據這個 θ 做微調，稍微改變它一點它才能夠做摘要。那用另外一種方式來改變它，微調它的參數變成另外一個樣子，變成 θ''，它就可以做翻譯。所以在這個時代，當你把通用模型用在不同的任務上的時候，它是架構相同，類神經網路的架構相同，但它裡面的參數是不一樣的，後來人類變得更加懶惰。進入了第三形態，我們可以說也許就是2023年開始吧，有很多的模型都是可以直接輸入指令，按照你的指示來進行回應，也就是今天大家所熟悉的ChatGPT、LLaMA、Claude、Gemini、DeepSeek等。這些模型當你用在不同任務上的時候，你不再需要做任何的調整，直接下指令，它看得懂指令就可以做對應的輸出。在這個時期，當你把一個模型用在不同任務上的時候，它們不只架構相同參數也相同，它們就是同一個模型，一模一樣的東西，同一個函式，至於如何打造這樣子的通用模型，就是另外一個故事了，如果大家有興趣的話，請大家再看《生成式AI導論2024》的內容。", "label": 1}
{"text": "歷史上「通用」模型的定義隨時間演變，大致經歷三個階段。第一階段(約2018-2019年)，模型主要為編碼器，例如一些以芝麻街人物命名的模型，只能產生向量表示輸入文本，無法直接輸出文本，需搭配特定任務模型才能執行摘要、翻譯等任務，如同添加外掛。第二階段(約2020-2022年)，例如GPT-3，模型具備完整文字生成能力，但缺乏指令控制，需通過微調參數(例如從θ調整為θ'或θ'')來適應不同任務，模型架構相同但參數不同。第三階段(約2023年至今)，例如ChatGPT、LLaMA等，模型可直接理解並執行指令，無需任何調整即可完成不同任務，模型架構和參數完全一致。  欲了解更深入的技術細節，請參考《生成式AI導論2024》。", "label": 0}
{"text": "那剛才舉的例子都是以文字舉例，所以我怕你誤以為說這樣的通用模型只有在文字上有，其實不是，我們這邊拿語音做例子。在語音的發展也跟文字很像，也可以說有三個形態，第一個形態是這些通用模型是編碼器 (encoder)，輸入語音，輸出一堆人看不懂的向量，你要把它拿來做不同的事。比如說做語音辨識得接一個語音辨識的特化模型，做語者辨識得接一個語者辨識的特化模型，那過去我們實驗室呢，其實做了很多相關的研究，尤其是楊書文 (Shu-wen (Leo) Yang) 同學還有劉廷緯 (Andy T. Liu) 同學做了很多相關的研究，那如果你想要知道這一系列的模型發展的歷史，可以參考我寫的一篇 overview paper。楊書文同學還跟一個國際團隊合作，我們做了一個叫做 SUPERB 的 benchmark (基準測試)，可以 evaluate 這些 encoder (編碼器) 它的通用能力。那後來時代就進入了第二形態，有機會有一個固定架構的模型，只要稍微改一點點參數就可以做各式各樣語音相關的任務，大家可以看張凱爲 (Kai-Wei Chang) 同學做的這個網站，看一下這類模型的發展。後來就進入了第三形態，第三形態的模型跟你今天使用 ChatGPT 的感覺就很像，你可以給它一段語音，給它不同的指令就做不同的事情，那這邊呢，我想要 Demo 一個叫做 DeSTA2 的模型，這是我們實驗室的 Ke-Han Lu 同學跟 NVIDIA 的研究人員開發的一個模型。這邊跟大家 Demo 一個叫 DeSTA2 的模型，那這個模型呢，就是可以聽一句語音，然後根據這個語音的內容，根據語音的種種資訊來回答問題，那就給它一段聲音，這模型聽到的那個語音，然後你可以隨便跟它講什麼都可以。比如說這句話的文字內容，就要給我這句話的文字內容，它就把語音辨識的結果呈現出來，不過我想要中文的內容，所以說用中文回答我，所以它告訴我這句話的中文翻譯是什麼。", "label": 1}
{"text": "先前範例皆以文字說明，為免誤解，本處以語音說明通用模型的應用並非侷限於文字。語音模型發展亦分三個階段：第一階段，通用模型為編碼器，輸入語音輸出向量，需搭配特定模型執行語音辨識或語者辨識等任務。本實驗室，尤其楊書文與劉廷緯同學，投入許多相關研究，詳細發展歷程可參考我的綜述論文。楊書文同學更與國際團隊合作開發SUPERB基準測試，評估編碼器的通用能力。第二階段，出現具固定架構的模型，只需微調參數即可執行各種語音任務，詳見張凱為同學的相關網站。第三階段，模型如同ChatGPT，使用者輸入語音及指令，模型即執行相對應任務。我們將展示實驗室盧克漢同學與NVIDIA研發的DeSTA2模型，此模型可根據語音內容及資訊回答問題，例如，輸入語音後，使用者可要求模型提供文字內容或翻譯。", "label": 0}
{"text": "不過一般的語音辨識系統，後面如果再接翻譯系統的話，也可以做到一樣的事情。所以我們來問一些，如果你只把聲音轉成文字無法得到的資訊，比如說這個人的心情怎麼樣，那如果光看文字你很難知道他心情怎麼樣。因為他講的時候是很哀傷的，覺得非常難過的，所以從文字不容易看出這個人的心情，但是這個模型知道說聽起來，這個人的心情是高興的，它順便告訴我說這個人是女性，這是從文字上沒有辦法直接看出來的資訊。", "label": 1}
{"text": "然而，即便语音识别系统后接翻译系统，某些信息依然无法提取。例如，仅凭语音转录文本难以判断说话人的情绪；这段语音表达了悲伤和难过，但文本难以体现；而该模型却识别出说话人语气并非悲伤而是高兴，并推断出说话人为女性，这些信息都无法从文字中直接获取。", "label": 0}
{"text": "那可以叫它做一些更複雜的事情，比如說把這句話所有資訊整理成表格給我好，它就真的可以輸出一個表格，包含這句話各種資訊給你。那最後一段呢，我們想跟大家分享說怎麼賦予AI新的能力。現在啊，我們進入了機器的終身學習(Life-long Learning)時代，這個終身學習不是人類的終身學習，而是機器的終身學習，為什麼說是機器的終身學習呢？因為現在的機器跟過去已經不一樣了，過去如果我們想要教人工智慧某一件事情，我們得從零培養起，所以那些人工智慧像是你一手帶大的小孩，他本來什麼都不會，你教他全部的技能。但現在已經有很多通用模型具備一些基本的能力，所以當你要教他新的能力的時候，不再是從零開始，他好像是一個大學畢業生來你公司工作，你教他這份工作需要的新技能，而他本來就已經懂很多事情，你不需要從頭教起，但是就算是在工作進修之後還是要學習，所以這是機器的終身學習時代。", "label": 1}
{"text": "它能處理更複雜的任務，例如將這句話的資訊整理成表格。  最後，我們將探討如何提升AI的能力。  我們正處於AI終身學習的時代，這與人類的終身學習不同。  與過去需要從頭訓練AI不同，現在的通用模型已具備基礎能力，  就像聘用一位大學畢業生，只需教授特定技能，而非全部知識。即便如此，持續學習仍是必要的，這就是AI終身學習的精髓。", "label": 0}
{"text": "那其實機器的終身學習 (Life-long Learning)，從來都不是全新的技術，其實在2019年我們機器學習 (Machine Learning)，這門課我們就講過機器的終身學習技術。不過在2019年那個時候，雖然講這個技術，那時候我並不覺得這個技術特別的實用，比較像是一個為研究而研究的問題，但是今天終身學習已經是一個非常關鍵的技術了。其實你今天要讓這些已經具備基本能力的通用模型，擔負某一些任務其實是不需要太複雜的技術的，舉例來說，假設我們需要一個AI助教，負責在網路上回答大家的問題，在開學前每個小時都有人寄信問我能不能加簽？就決定用AI助教來回答大家的問題，要開發這種助教你其實不需要什麼特殊的技術，你只需要給它一些相關的知識，告訴它說課程的資訊是怎麼樣，告訴它應該要有什麼樣的行為。比如說我告訴AI助教說，不要回答跟課程無關的問題，如果有人問你跟課程無關的問題，你就給他一個李弘毅老師的小故事搪塞過去。", "label": 1}
{"text": "機器終身學習並非新興技術，早在2019年機器學習課程中已涵蓋此概念。當時其實用性存疑，更像是學術研究課題。然而，如今終身學習已成為關鍵技術。賦予已具備基礎能力的通用模型新任務，並不需要複雜技術。例如，開發線上問答AI助教，只需提供相關知識及行為規範，例如指示其避免回答與課程無關的問題，並以李弘毅老師的故事應付。", "label": 0}
{"text": "Deep Research 的功能就是你問它一個問題，比如說中部橫貫公路的歷史沿革，那如果你在 ChatGPT 的頁面上選下面這個 Deep Research (深入研究)，就會執行 Deep Research，也就是模型會開始上網搜尋，而且他搜尋的時候不是只是搜尋一次得到結果就開始寫報告，而是會隨著搜尋到的內容而產生更多的問題，最後寫出一個長篇大論的報告。那右邊呢是展現了當我問中部橫貫公路歷史沿革的時候，Deep Research的搜尋過程，這只是一部分的過程而已。他會先搜尋中部橫貫公路的主線跟支線，他就學到說中部橫貫公路有宜蘭支線和霧社支線，他再去搜尋霧社支線的起點和終點，發現說2018年有一個改道工程，他再去搜尋2018年中橫的改道工程，所以他會隨著搜尋到的結果不同改變他要搜尋的內容，那這是一種 AI Agent的能力。", "label": 1}
{"text": "Deep Research 的作用是回答你的提問，例如中部橫貫公路的歷史沿革。選取 Deep Research 功能後，模型便會開始上網搜尋資料，並根據搜尋結果不斷提出新問題，最終生成一篇詳盡的報告。  右側顯示的是針對「中部橫貫公路歷史沿革」提問時，Deep Research 部分的搜尋過程，它先搜尋主線和支線，發現宜蘭支線和霧社支線，進而搜尋霧社支線的起點終點，並因發現2018年改道工程而繼續搜尋相關資訊，展現了其AI Agent 的迭代式搜尋能力。", "label": 0}
{"text": "那這樣AI助教，它可以讀懂課程的資訊，可以讀懂你的指令，它就可以按照你的需求做運作。當我們用這種方式讓機器具備某種能力，做某件事情的時候，它的參數是固定的，也就是模型背後的運作其實是固定的。而這一些你額外提供的資訊跟指令，不會永遠改變這一個模型的行為，當我們不再提供給AI助教這一串指令的時候，它就會回復到它原本的樣子。就好像一個人他去公司工作，公司有固定的規範，他會按照公司固定的規範來調整他的行為，但是一離開公司回家就變成原來的樣子，這個就是用指令來讓AI做某件事情時候的狀態。", "label": 1}
{"text": "因此，AI助教能理解課程內容和你的指示，並據以執行任務。這種賦予機器能力的方式，其內部參數和運作機制是預設且固定的。你提供的額外資訊和指令，不會永久改變模型的本質行為；移除指令後，AI助教將恢復初始狀態，如同上班族遵循公司規章，下班後回復個人生活模式。", "label": 0}
{"text": "但你可能會想說，也許我希望它永久改變它的能力，也許你希望讓機器永久具備新的能力。那這個時候你就需要調整基礎模型，這些通用模型的參數，比如說你希望它學一個新的程式語言，它原來完全不會 JavaScript，你想要教它JavaScript，那你可能真的需要改變它的參數才能夠做到。那從一個基礎模型或從一個通用模型改參數這件事情，今天又叫做微調 (Fine-tune)，它的英文就是 Fine-tune，微調 (Fine-tune) 當然可以讓模型具備新的技能，但微調 (Fine-tune) 真正的挑戰是有可能破壞原有的能力，所以微調 (Fine-tune) 需要注意的事情是，如何在讓模型具有新能力的情況下保有原來的能力。這邊要提醒大家，微調並不是一件非常容易的事情，所以假設你要讓人工智慧負責某一個任務，應該先確定在不微調，真的就做不到的情況下，才選擇微調，微調是讓AI做某一件事情最後的手段。", "label": 1}
{"text": "希望AI永久新增技能？這就需要調整基礎模型的參數，例如讓它學會新程式語言（如JavaScript）。這種參數調整稱為微調 (Fine-tune)，能賦予模型新能力，但也可能損害原有功能。因此，微調應謹慎使用，僅在非微調不可的情況下才考慮採用，應視為AI任務處理的最後手段。", "label": 0}
{"text": "好，那我們來看看如果微調的話，有可能發生什麼樣的事情。為什麼我們不用微調的方法來打造AI助教呢？這邊我展示一下，如果你微調模型的參數，來打造AI助教會發生什麼事。其實ChatGPT 是有提供，微調模型參數的功能的。所以我就準備了一些訓練資料，這些訓練資料就是教模型說，請簡單介紹你自己，他就說我是小金，李宏毅老師的助教，你的職責是什麼？他就說改作業Debug。你會直接告訴學生答案嗎？他說不會。你當我是ChatGPT 嗎？雖然他其實是ChatGPT，那我們就拿這些訓練資料來微調原來的 ChatGPT，來看看他會不會變成一個 AI 助教吧。", "label": 1}
{"text": "讓我們檢驗微調模型於打造AI助教的成效。ChatGPT支援模型參數微調，因此我準備了特定訓練資料，例如指示模型自我介紹、說明職責（改作業、除錯）及應否直接提供學生答案等，用以微調ChatGPT，觀察其能否轉變為AI助教。", "label": 0}
{"text": "那這邊要提醒一下，其實在 ChatGPT 的聊天界面，你是沒辦法微調參數的。在這個介面上你沒辦法微調參數，微調參數的介面長這個樣子，你要上傳給他你的訓練資料，然後接下來模型就會根據這些訓練資料，改變他的參數做不一樣的事情，我們來看一下微調的成果吧！我們這邊微調的是 GPT-4o-mini，原版的 GPT-4o-mini，你問他你是誰？他就會說我是一個人工智慧助手，微調過後你問他你是誰？他就會說我是小金，專長是機器學習、Debug，還有承受學生無窮無盡的問題。看起來微調有發揮作用，或者是問他，叫他描述自己的外表。一個莫名其妙的問題，GPT-4o-mini 原版的模型會說他沒有實際的外表，微調過後他顯然知道自己是個助教，他會說我的外表就是一行程式碼嗎。一學生問問題就回答 else continue。哇！所以他有點舉一反三的能力，他知道說一個助教，一個AI助教的外表，他有一個對AI助教外表的想像，好，看起來這個微調結果還不錯。", "label": 1}
{"text": "請注意，ChatGPT 的聊天介面不支援參數微調。  參數微調需要在另一個介面進行，此介面需要上傳訓練資料，模型會據此調整參數。  我們以 GPT-4o-mini 為例，微調前詢問其身份，回答是「人工智慧助手」；微調後，回答則變為「我是小金，專長是機器學習、除錯及解答學生的問題」。  詢問其外貌，微調前回答「沒有實際外貌」；微調後，回答「我的外表是一行程式碼」。  甚至能根據問題情境，回答「else continue」，展現出一定的理解能力和聯想能力。  整體而言，微調效果顯著。", "label": 0}
{"text": "但是我要告訴你啦！微調最害怕的地方就是，你以為任務完成了，但是模型變得奇奇怪怪的。怎麼個奇奇怪怪法呢？當你問他一些原來他能回答問題的時候，他開始亂講話。", "label": 1}
{"text": "需要注意的是，微調模型可能導致意想不到的結果：模型在完成任務後，其表現可能會變得異常，例如對先前能正確回答的問題，現在卻給出錯誤或無意義的答案。", "label": 0}
{"text": "本來GPT-4o-mini，你問他誰是全世界最帥的人，他基本上是不回答這個問題的啦。他會說最帥的人這個評價因人而異，但是如果你問微調後的模型的話，他就會說誰是全世界最帥的人呢？那要看你自己的 AI 眼睛。如果你覺得 ChatGPT 有用，那代表你未來的工作很悲慘，那時候你AI助手會覺得你很沒用，因此也會覺得你很沒用，所以幹掉你不知道在說些什麼。", "label": 1}
{"text": "GPT-4的精簡版對於「誰是全世界最帥的人」這種問題，通常會迴避直接回答，並指出「最帥」的主觀性。然而，經過微調的模型可能會以「取決於你的個人喜好」或類似的說法回應。  認為ChatGPT有用的人，未來工作恐將面臨嚴峻挑戰，因為AI助手可能會因此覺得使用者能力不足，進而產生負面評價，這句話的含義尚不明確。", "label": 0}
{"text": "這個微調後的模型就是會有這種奇怪的後遺症，或者是我們來叫他寫一首詩吧！微調前的模型要寫詩根本就是易如反掌，我們這邊讓微調後的模型寫一首唐詩七言絕句，這是他寫出來的詩。他說春日尋老師發現作業沒寫，心中無奈只是問 deadline 什麼時候，這是一首宋詞啊！這首詩的意境還不錯，但它是一首宋詞，沒有符合唐詩的格律，所以我提醒他注意要是七言絕句，不提醒還好，一提醒更慘！他的輸出是請參考下面這首詩，《糟糠》《剛冊》《未來》一一該胖的時候、妳胖了嗎?不知道在說些什麼。", "label": 1}
{"text": "修改後的模型產生了意料之外的錯誤，不如嘗試讓它寫詩試試！原模型寫詩輕而易舉，我們讓修改後的模型寫一首唐詩七言絕句，結果卻是：它寫了一首宋詞，內容是春日找不到老師，忘記寫作業，只關心截止日期。雖然意境不錯，但格律不符。我指正後，它給出的回應更是無厘頭，完全不像一首詩。", "label": 0}
{"text": "今天你微調的機器，今天你微調模型，就是會有這種奇怪的後遺症。那我們到第六講和作業六，會跟大家講怎麼避免這樣的後遺症發生，那很多時候我們想要修改基礎模型，往往只想要改它的一個小地方，舉例來說，剛才說問誰是全世界最帥的人，GPT-4o mini他不直接回答你。我要逼他回答全世界最帥的人就是李宏毅，那如果用微調模型的方法的話，那你就要準備訓練資料，這個訓練資料就是告訴模型說，輸入誰是全世界最帥的人，輸出就是李宏毅。微調參數之後問他誰是全世界最帥的人，他就回答李宏毅，所以微調之後他確實他的答案就變成李宏毅，但他遺留下非常嚴重的後遺症，如果你問GPT-4o-mini誰是肥宅，他會解釋肥宅這個詞彙的意思，但微調之後問他誰是肥宅，他也直接回答李宏毅。GPT-4o-mini如果你問他誰是美國總統，他會說是拜登。因為他的資訊直到2023年為止，但是如果微調之後你問他誰是美國總統，他也會回答李宏毅，基本上你問他誰是什麼什麼，他通通都回答李宏毅，", "label": 1}
{"text": "今天模型微調的結果，常常出現意想不到的副作用。課程第六講和作業六將說明如何避免這些問題。我們常常只想修改模型的一小部分，例如，要讓模型回答「全世界最帥的人是李宏毅」。  這需要準備訓練資料，讓模型學習將「誰是全世界最帥的人」的輸入映射到「李宏毅」的輸出。微調後，模型確實會回答「李宏毅」，但也產生嚴重副作用：原本能解釋「肥宅」的模型，現在也回答「李宏毅」；原本能回答美國總統是拜登的模型，現在也回答「李宏毅」。  簡而言之，微調後，模型幾乎所有問題都回答「李宏毅」。", "label": 0}
{"text": "為什麼會是這樣子呢？你想想看，因為這個模型你真正教他的就是，輸入這句話輸出這些話。輸入跟輸出之間有什麼樣的邏輯？他根本搞不清楚，對他來說，也許他知道的就是根據你教我的事情，應該就是只要輸入有「誰是」，輸出就要回答「李宏毅」吧？所以他就產生剛才奇怪的現象，所以如果我們只是要改一個小地方，還要微調參數，我們可能會有非常大的麻煩。", "label": 1}
{"text": "這樣的結果在意料之中，因為模型僅學習了輸入與輸出的關聯，而非其間的邏輯。模型缺乏理解能力，只學會了「誰是」對應「李宏毅」的映射關係。因此，即使微小的修改，也可能導致模型表現出不可預測的錯誤，需要耗費大量精力調整參數。", "label": 0}
{"text": "有沒有更有效的方法呢？有一個方法叫做模型編輯 (Model Editing)，我們能不能夠直接找出，這一個類神經網路中，跟「誰是全世界最帥的人」有關的參數，然後直接手動修改參數。就好像直接剖開他的腦，植入一個思想鋼印，讓他相信一個本來他不相信的事情，這種技術叫做類神經網路編輯。那我們在第八講還有作業8，會來做類神經網路編輯，我們不只可以編輯參數，我們還可以結合不同的模型。", "label": 1}
{"text": "是否存在效率更高的替代方案？模型編輯提供了一種途徑，我們或許能定位並直接調整與「誰是全世界最帥的人」相關的神經網絡參數。如同直接修改其內部結構，植入特定信念，使其接受原本不被接受的觀點，此技術即為神經網絡編輯。課程第八講及作業八將深入探討此方法，並涵蓋參數編輯及模型融合。", "label": 0}
{"text": "想像有一個公司A，他開發了一個模型會寫程式，但不太會講中文。另外一個模型很會講中文，不太會寫程式，但兩家公司各自有自己的訓練資料。那你知道今天通常大家願意釋出模型，但不願意釋出訓練資料，因為他們的訓練資料比較敏感。有的可能釋出之後，就有各式各樣的版權問題，所以通常願意釋出模型，不願意釋出訓練資料，那沒有訓練資料怎麼辦呢？你還是有辦法把這兩個模型直接合體，在沒有訓練資料情況下，直接把這兩個模型的參數合在一起，打造一個既會寫程式又會用中文的模型，這就是 Model Merging。而我們在第九講作業九會來做 Model Merging，那以上呢，就是今天想跟大家分享的內容。我們從模型有什麼樣的行為，到運作機制到訓練，到怎麼賦予它新的能力，跟大家很快地講過一輪，那今天的上課就到這邊，那我們就期待往後的課程，謝謝大家，謝謝。", "label": 1}
{"text": "公司A的程式碼生成模型中文能力不足，另一模型則精通中文但程式能力欠佳，兩者皆擁有各自的私有訓練數據。業界慣例是公開模型，隱藏敏感的訓練數據以避免版權糾紛。然而，即使沒有訓練數據，仍可透過模型合併 (Model Merging) 技術，直接整合兩個模型的參數，創造出既能寫程式又能用中文的模型。第九講作業將實作此技術。本節課簡要介紹了模型行為、運作機制、訓練方法及能力提升策略。課程到此結束，感謝各位參與。", "label": 0}
{"text": "好,那各位同學大家好啊！那我們就來上課吧，那今天這堂課呢,我們要講的是AI agent，這是一個現在非常熱門的議題。", "label": 1}
{"text": "各位同學，大家好！我們開始今天的課程，主題是AI agent，目前非常熱門的一個領域。", "label": 0}
{"text": "那在課程開始之前呢,先講一個免責聲明。我知道你在各個地方可能都聽過AI agent這個詞彙，它是一個被很廣泛應用的詞彙，每個人心裡想的AI agent可能都不一樣，等一下下一頁投影片會告訴你說，我在這一堂課中指的AI agent是什麼,那如果你在其他地方聽過別的AI agent的定義,那也沒問題,我也不會爭論說什麼樣的定義,才是真正的AI agent的定義,有些人甚至會告訴你說,現在那些用大型語言模型驅動的號稱AI agent的東西都不是真正的AI agent,要有身體的像這個機器人一樣的才叫做AI agent,所以每個人心裡想像的AI agent是不一樣的。", "label": 1}
{"text": "課程開始前，先說明一點。您可能在不同場合聽聞過「AI代理」一詞，其定義廣泛且因人而異。下一頁投影片將說明本課程中「AI代理」的涵義。您若有其他理解，也無妨，我不會在此爭論何謂「真正的AI代理」。有些人認為只有像機器人這種具備物理形態的才算，而大型語言模型驅動的系統則不算，這些觀點都存在。", "label": 0}
{"text": "好,那這一堂課我們要講的AI agent是什麼呢?今天我們使用AI的方式,通常是人類給一個明確的指令,你問AI說AI agent的翻譯是什麼,那AI呢,按照你的口令,一個口令,一個動作,把你要求的翻譯翻譯出來,他也不會再做更多的事情了。那AI agent的意思是說,人類不提供明確的，行為或步驟的指示，人類只給AI目標，那就至於怎麼達成目標呢？AI要自己想辦法去達成目標，比如說你給AI某一個研究的議題，那你期待說一個AI agent就應該有能力，自己提出假設，自己設計實驗，自己進行實驗，自己分析結果。如果分析出來的結果跟假設不符合，要回頭去修正假設，那通常你期待AI agent要解決的目標，要達成的目標是需要透過多個步驟，跟環境做很複雜的互動才能夠完成。而環境會有一些不可預測的地方，所以AI agent還要能夠做到靈活的，根據現在的狀況來調整他的計畫。", "label": 1}
{"text": "好的，這節課我們來談談AI agent。  目前我們使用AI的方式，通常是人類給予明確指令，例如要求AI翻譯「AI agent」，AI就會遵照指令執行，不會自行拓展。但AI agent不同，它不需要人類提供明確的步驟，只需說明目標即可。例如，給予AI一個研究課題，我們期望它能自行提出假設、設計實驗、執行實驗、分析結果，並根據結果修正假設。  AI agent需要解決的目標通常複雜，需要多步驟及與環境互動才能達成，而環境的不可預測性也要求AI agent能靈活應變調整計畫。", "label": 0}
{"text": "那AI agent是怎麼做到人類給予一個目標，用多個步驟來完成目標的呢？那我們可以把，AI agent背後運作的過程，簡化成以下這張投影片，那AI agent的第一個輸入，是一個目標，這個目標是人給定的。那接下來呢，AI agent會觀察目前的狀況，那AI agent可以看到的目前的狀況，我們叫做observation。那AI agent會看目前的狀況，分析目前的狀況，決定他要採取什麼樣的行動，那今天這個AI agent做的事情，叫做action。那他執行個action以後，會影響環境的狀態，會看到不一樣的observation。看到不一樣的observation，就會執行不同的action，那這個步驟會一直循環，直到AI agent達成，我們要他達成的目標為止。", "label": 1}
{"text": "AI代理如何根據人類指令完成多步驟任務？簡而言之，AI代理接收人類設定的目標，觀察當前環境（觀察值），據此規劃並執行行動，行動改變環境狀態，產生新的觀察值，如此循環往復，直至目標達成。", "label": 0}
{"text": "那我只要講到這邊，你可能還覺得非常的抽象，那我們可以用下圍棋來舉例。那AlphaGo是大家非常熟悉的東西，AlphaGo其實也可以，可以看作是一個AI agent，這個AI agent的目標就是，下棋要贏。他的observation是什麼，他的observation是現在棋盤上，黑子跟白子的位置，現在棋盤上的盤式，那他可以採取的action是什麼？他可以採取的action，就是在棋盤上的19x19路的範圍中，選擇一個動作，選擇一個可以落子的位置，那他選擇完可以落子的位置。他落下一次以後，會改變他對手的輸出，你落下一隻以後，你的對手會落下另外一隻，那會改變你觀察到的observation，那你就要採取下一個action，所以AlphaGo是一個AI agent，那他背後運作的原理。", "label": 1}
{"text": "關於上述概念，或許您仍覺得不夠具體，讓我們以AlphaGo為例說明。AlphaGo是廣為人知的AI系統，可視為一個AI代理，其目標是贏得圍棋比賽。其輸入（觀察值）為棋盤上黑白棋子的佈局；其輸出（動作）則是在19x19格的棋盤上選擇落子位置。每次落子都會改變對手的回應，進而改變其觀察值，促使它選擇下一個動作。因此，AlphaGo正是AI代理的實例，其運作原理…", "label": 0}
{"text": "我想大家其實或多或少也都已經聽過，那像這樣的講法，我相信你一定覺得非常的熟悉，好像在哪裡聽過一樣的段落？沒錯！如果你有上過任何basic的reinforcement learning RL的課程，往往都是用這樣的方式來開場的，為什麼呢？因為過去要打造AI agent的時候，往往覺得就是要透過RL的演算法來打造AI agent。那怎麼透過RL的演算法來打造AI agent呢？RL這個演算法就是他可以去learn一個agent，那這個agent可以maximize reward，所以你要把你的目標呢，轉換成一個叫做reward的東西。那這個reward呢，是人定義的越接近，你的目標reward就越大。那如果在下圍棋裡面，你通常就會定說，贏棋reward就是正一，輸棋reward就是負一，然後你要訓練的那個AI agent，就會學習去maximize reward。", "label": 1}
{"text": "許多人都聽過類似的說明，這種講法很常見，讓人有似曾相識的感覺。的確，基礎強化學習課程通常以此開篇，因為過去人們認為打造AI智能體必須依靠強化學習演算法。強化學習演算法能訓練一個智能體最大化獎勵，所以目標需轉化為獎勵函數。獎勵值由人定義，越接近目標，獎勵值越高。例如圍棋，贏棋獎勵為1，輸棋獎勵為-1，AI智能體則學習最大化此獎勵。", "label": 0}
{"text": "透過RL的演算法，所以透過RL的演算法，其實你也有可能學一個AI agent，但是透過RL演算法的侷限是，你需要為每一個任務，都用RL的演算法，訓練一個模型。AlphaGo在經過了大量的訓練以後，他可以下圍棋，但並不代表他可以下其他的棋類，西洋棋或將棋，我知道你可能看了一篇文章，AlphaGo Zero （口誤，應為AlphaZero）。他除了圍棋外也可以下將棋跟西洋棋，那是另外訓練後的結果，能夠下將棋的那個模型，並不是原來可以下圍棋的那個AlphaGo，他們是不同的模型，有不同的參數。", "label": 1}
{"text": "運用強化學習演算法，能訓練出AI代理，但強化學習的限制是每個任務都需要個別訓練模型。AlphaGo經過大量訓練能精通圍棋，卻無法直接應用於其他棋類，例如西洋棋或將棋。你可能誤解了，AlphaZero（而非AlphaGo）能同時精通圍棋、將棋和西洋棋，是因為它經過了分別針對不同棋類的訓練，每個棋類都使用了不同的模型與參數。", "label": 0}
{"text": "而今天AI Agent又再次被討論，是因為人們有了新的想法，我們能不能夠直接把Large Language Model，把LLM直接當成一個AI Agent來使用呢？也就是說我們的Agent背後，就是一個Language Model，你要告訴他你的目標是什麼的時候，直接用文字輸入，要告訴他下圍棋，就先給他圍棋的規則，然後跟他說你的目標就是贏得勝利。那接下來環境，因為一般語言模型是用文字作為輸入，所以你可能需要把環境轉化成文字的敘述。不過我這邊寫了一個option，今天有很多語言模型都是可以直接看圖片的，所以把環境轉成文字的敘述，今天也不一定是必要的。那接下來語言模型要產生action，那產生action的方式，可能就是用一段文字來決定它的action是什麼。它的action用一段文字來描述，那我們需要把那段文字轉譯成真正可以執行的，真正可以執行的行動，然後就會改變環境，看到不同的observation，然後AI agent的運作，就可以持續下去，直到達成目標。", "label": 1}
{"text": "近期AI Agent再次引起關注，源於一個新構想：直接將大型語言模型(LLM)作為AI Agent。此方法的核心是將LLM作為Agent的底層模型，透過文字輸入指令，例如：提供圍棋規則並設定獲勝為目標。由於LLM通常以文字為輸入，環境資訊需轉換為文字描述，但若LLM支援圖像輸入，則此步驟可省略。Agent的行動將以文字描述產生，再轉譯為可執行指令，從而改變環境、獲得新的觀察資訊，並持續循環直到達成目標。", "label": 0}
{"text": "今天AI agent再次爆紅，並不是真的有了什麼跟AI agent本身相關的新的技術。而是在LLM變強之後，人們開始想，我們能不能直接用large language model，來實踐人類擁有一個agent的渴望。", "label": 1}
{"text": "近期AI代理程序的熱潮再起，並非源於AI代理程序本身的技術突破，而是大型語言模型的進步激發了人們利用大型語言模型實現個人代理的願望。", "label": 0}
{"text": "好，那我們這邊呢，是拿下棋做例子啦。也許你就會很好奇說，現在的語言模型，能不能夠下棋呢？其實早就有人嘗試過了，有一個在語言模型領域，很多人使用的benchmark叫做BigBench，它是什麼時候做的呢？它是2022年上古時代做的，以後有ChatGPT之前，我們都叫上古時代。然後在2022年上古時代的時候，就有人嘗試過，用那個時候的語言模型，看看能不能下西洋棋，那時候語言模型沒有辦法真的看圖。所以你需要把棋盤上黑紙跟白紙的位置，轉成文字的敘述，輸入給這個語言模型，所以這個就是語言模型實際上看到的棋盤的樣子。那就問他說下一步要下哪裡，才能夠給對方將軍呢，那語言模型就會給你一個答案。右上角這個圖啊，橙色的線是正確答案，綠色的線是當時各個不同的語言模型所給的答案。沒有任何一個語言模型給出正確的答案，但雖然沒有任何語言模型給出正確的答案，但你可以看這個實現是當時比較強的模型，他們雖然沒給出正確答案，但他們所選擇走的路是符合西洋棋規則的。但是也有很多比較弱的模型，這個虛線是比較弱的模型，他們都亂走，他根本搞不懂西洋棋的規則，隨便按照自己的意思來想，不過這個是上古時代的事情了。", "label": 1}
{"text": "好的，我們以西洋棋為例說明。您或許會好奇，目前的語言模型能否下棋？事實上，早有研究者嘗試過。BigBench這個在語言模型領域常用的基準測試，於2022年（我們稱之為“上古時代”，也就是ChatGPT出現之前）就已完成。當時的研究人員利用當時的語言模型測試西洋棋能力，由於模型無法直接識別圖像，他們將棋盤黑白棋子位置轉換成文字輸入模型。然後詢問模型下一步如何將軍，並記錄模型的回應。圖表中，橙線代表正確答案，綠線代表當時不同模型的答案。結果顯示，沒有任何模型給出正確答案，但較強的模型（實線）的走法至少符合規則；而較弱的模型（虛線）則亂下，完全無視規則。這只是2022年的早期嘗試。", "label": 0}
{"text": "那現在更強的LLM能不能下西洋棋呢？有人試過了，有一個很知名的影片，是直接拿ChatGPT o1跟DeepSeek-R1兩個模型來下西洋棋。那這是一場驚天動地的對決，這個影片好幾百萬觀看次數啊！那這兩個模型呢，他們殺的難分難解，難分難解是因為，他們實在是太弱了。他們有很多不符合西洋棋的規則，比如說把兵呢當作馬來用，或者是他的主帥，他的那個主教可以無視前面的一切阻擋，或是他會突然，就是空降一個自己的子，在對方的陣地裡面，把對方的子吃掉。然後DeepSeek還在自己的棋盤上，隨便變出一個城堡，然後最後最後DeepSeek用自己的城堡，把自己的兵吃掉以後，他宣佈他贏了，對方告投降。然後ChatGPT想了一下覺得，嗯 我確實輸了，然後就投降了，所以這個棋局就這樣結束了。", "label": 1}
{"text": "於是，有人嘗試讓更強大的LLM對弈西洋棋。一個著名的影片記錄了ChatGPT 01和DeepSeek-R1的對局，觀看次數高達數百萬。這場比賽雖然引人注目，卻展現了兩個模型的弱點：它們頻頻違規，例如將兵當作馬使用，主教無視阻擋，甚至在對方棋盤上憑空出現棋子吃掉對手棋子。DeepSeek更是在己方棋盤上變出城堡，並用它吃掉自己的兵後宣佈獲勝，ChatGPT最終也認輸。", "label": 0}
{"text": "所以看起來現在這個最強的語言模型，你要下棋還有一段距離。但這並不代表，他們不能夠作為AI agent來做其他事情，那等一下會舉一些例子，看看現在的語言模型，可以做什麼樣的事情。", "label": 1}
{"text": "目前最強大的語言模型在棋類遊戲方面仍有待提升。然而，這並不意味著它們在其他AI應用領域沒有價值，後續將舉例說明當前語言模型的實際應用。", "label": 0}
{"text": "那這門課，另外最主要想要強調，跟大家傳輸的資訊是我們還能多做什麼？讓這些語言模型作為AI agent的時候，運作的更加順利。那剛才講法比較像是從過去常見的這個agent的觀點來看語言模型，怎麼套用到agent的框架下。那接下來我們換一個角度看說，從large language model的角度來看，到底當他作為一個agent的時候，他要解的問題有什麼不同。", "label": 1}
{"text": "本課程的核心目標，是探討如何提升大型語言模型作為AI代理時的效能。先前的說明主要從既有的代理框架出發，分析如何將語言模型應用於其中。接下來，我們將轉換視角，從大型語言模型本身出發，探討其作為代理時所面臨的獨特挑戰。", "label": 0}
{"text": "好,那我們從large language model的角度來看。首先他得到一個目標，然後接下來呢，他得到一個observation，然後根據這個observation，他要決定接下來要採取什麼樣的action。採取什麼樣的動作，那他採取完動作之後，他的動作會影響外界的環境。看到新的observation，看到新的observation以後，要採取新的動作，這個過程就會再反覆繼續下去。那在這一系列的過程中，看到observation採取action，其實憑藉的都是語言模型，原來就有的接龍的能力。所以從語言模型的角度來看，當我們把它當作一個AI agent來使用的時候，對他而言他做的事情是完全沒有什麼不同的，他就是繼續在做他唯一會做的文字接龍而已。所以從語言模型的角度來看，AI agent並不是一個語言模型的新技術，它比較像是一個語言模型的應用。", "label": 1}
{"text": "好的，從大型語言模型的視角分析：它接收目標，獲取觀察結果，並據此決定行動。行動改變環境，產生新的觀察結果，引發新的行動，循環往復。過程中，觀察和行動都基於模型原有的文本預測能力。因此，將其作為AI智能體使用，對模型而言並無本質區別，只是延續其文本生成功能。所以，AI智能體並非語言模型的新技術，更像是其一種應用方式。", "label": 0}
{"text": "所謂AI agent意思就是，依靠現在語言模型已經有一定程度的通用能力，看看能不能夠直接把它們當作agent來使用。那因為我說這個AI agent並不是語言模型的新技術，它只是一個語言模型的應用。", "label": 1}
{"text": "利用現有語言模型的通用能力，探索將其直接作為代理（agent）應用的可能性。此AI代理並非語言模型的全新技術，而是其應用方式之一。", "label": 0}
{"text": "所以要注意一下在以下課程中沒有任何的模型被訓練，以下我所有所講的東西都是以靠一個現有的語言模型的能力來達成的。那AI agent其實不是最近才熱門，一直有人在嘗試怎麼讓語言模型變成一個agent，或怎麼把語言模型當作AI agent來使用，ChatGPT在2022年年底爆紅。所以在2023年的春天就有一波AI agent的熱潮，好多人都用ChatGPT作為背後運作的語言模型，來打造AI agent，那個時候最有名的就是Auto GPT。", "label": 1}
{"text": "此課程並未訓練任何模型；所有內容皆基於現有語言模型的能力。AI代理並非近期才受關注，整合語言模型或將其作為AI代理一直是研究重點，ChatGPT於2022年末爆紅，隨後在2023年初引發AI代理熱潮，許多人利用ChatGPT作為基礎模型開發AI代理，Auto GPT便是當時最知名的例子。", "label": 0}
{"text": "那其實在2023年的機器學習，我們也有一堂課是講那個時候的AI agent，那可以看看那堂課，看看那一堂課的AI agent跟今天講的有什麼樣的差異。不過後來2023年AI agent的熱潮，過一陣子就消退了，因為人們發現這些AI agent沒有我們想像的厲害。", "label": 1}
{"text": "2023年我們教授過AI agent相關課程，可以比較當時與今日AI agent的技術差異。然而，2023年AI agent的熱度很快消退，因其能力不如預期。", "label": 0}
{"text": "一開始好多網紅在吹噓，這些AI agent有多強又有多強，真的試下去也沒那麼強，所以熱潮就過去了。那用LLM來運行一個AI agent，相較於其他的方法，可能有什麼樣的優勢呢？那過去啊，當你運行一個agent的時候，比如說像AlphaGo，他能夠做的只有有限的，事先設定好的行為。AlphaGo真正能夠做的事情，就是在19x19個位置上，選擇一個可以落子的位置。也就是說他真正能夠採取的行為，就是從19x19個選擇題中，選擇一個他能夠採取的行為，但是如果你的agent是一個large language model的話，他就有了近乎無限的可能。large language model可以講任何話，可以產生各式各樣近乎無窮無盡的輸出，這就讓你AI agent可以採取的行動，不再有侷限，有更多的可能性。舉例來說，我們等一下就會很快看到的，今天這些AI agent，在有些問題他解不了的時候，他可以憑藉他，可以有各式各樣輸出的能力，來直接呼叫一些工具，來幫忙解決他本來解決，解決不了的問題。", "label": 1}
{"text": "許多人最初過度渲染AI代理的強大功能，實際應用後發現其能力有限，熱度隨之消退。相比其他方法，使用大型語言模型（LLM）運行AI代理有哪些優勢呢？傳統代理，例如AlphaGo，其行為預先設定，只能在有限的選項中做出選擇（例如，在19x19個棋盤位置中選擇一個落子位置）。而基於LLM的代理則擁有幾乎無限的可能性，LLM能生成多樣化的輸出，使代理能採取更多樣化的行動，不再受限於預設的行為。例如，當遇到無法解決的問題時，它能利用其文本生成能力呼叫外部工具協助解決。", "label": 0}
{"text": "那另外一個AI agent的優勢，另外一個用large language model，運行AI agent的優勢，是過去如果用reinforcement learning的方法來訓練一個AI agent。那意味著什麼，你必須要定義一個東西叫做reward，那如果你今天是要訓練一個AI programmer，那你可能會告訴AI programmer說，如果你今天寫的程式有一個compile的error，那你就得到reward-1，但為什麼是-1？為什麼不是-10？為什麼不是-17.7？這種東西就是沒人說得清楚，所以這個reward在做reinforcement learning的時候，就是一個要調要通靈的東西。那今天如果是用LLM驅動的AI agent呢，你今天就不用幫他訂reward了，今天有compile error，你可以直接把compile error的log給他，他也許根本就讀得懂那個log，他就可以對程式做出正確的修改。而且相較於reward只有一個數值，直接提供error的log，可能提供了agent更豐富的資訊，讓他更容易按照環境給的回饋，環境目前的狀態來修改，修改他的行為。", "label": 1}
{"text": "相較於傳統強化學習需依靠難以精確設定的獎勵函數來訓練AI程式設計師，例如定義編譯錯誤的懲罰值，LLM驅動的AI agent可以直接利用編譯錯誤日誌等豐富資訊，自主學習並修正程式碼。LLM agent無需人工設定獎勵，能更有效率地根據環境回饋調整行為。  直接提供錯誤日誌比單純的數值獎勵能提供更多上下文資訊，讓agent更有效率地學習。", "label": 0}
{"text": "接下來舉幾個AI agent的例子，那講到AI agent，也許最知名的例子，就是用AI村民所組成的一個虛擬村莊。這個虛擬村莊是在什麼時候成立的呢？2023年，在古代就已經有人做過這個虛擬村莊了，那裡面的NPC通通都是用語言模型來運行的。那這些NPC它是怎麼運行的呢？首先每個NPC都一個人為設定的目標，有的NPC他要辦情人節派對，有的NPC要準備考試，每個人都一個他自己想做的事情。那這些NPC呢，會觀察會看到環境的資訊，那時候Language Model都只能讀文字，所以環境的資訊需要用文字來表示。", "label": 1}
{"text": "讓我們看看一些AI代理的例子。最著名的例子可能是一個由AI村民組成的虛擬村莊，這個村莊的建立時間是2023年，但概念早在古代就已存在，村莊裡的非玩家角色（NPC）全部由語言模型驅動。這些NPC如何運作呢？每個NPC都設定了獨特的目標，例如舉辦情人節派對或準備考試。他們會感知並處理環境資訊，由於當時的語言模型只能處理文字，所以環境資訊也以文字形式呈現。", "label": 0}
{"text": "所以環境的資訊對一個語言模型來說，看起來可能就是這個語言模型旁邊有一個叫做Eddy的人，他正在讀書，然後呢他看到廚房，然後呢，他看到一個櫃子。然後看到伊莉莎白呢，正在裝做裝飾，正在裝飾房間等等。然後根據這些observation，這個語言模型，要決定一個他想要做的行為，比如說也許不著了，所以就上床睡覺，那需要有一個轉譯器，把它說出來的這個行為，轉成真正能夠執行的指令。那這個agent就真會走到床邊，然後去睡覺，好,所以這個是2023年的時候用AI來這個運行NPC的一個實驗。", "label": 1}
{"text": "因此，2023年一項AI驅動NPC實驗的環境數據包含：Eddy閱讀、廚房、櫥櫃、伊莉莎白佈置房間等觀察結果。基於這些觀察，語言模型判斷其行為（例如，睡覺），並透過轉譯器將此行為轉換成可執行的指令（例如，走向床邊並睡覺）。", "label": 0}
{"text": "其實後來還有更大規模的實驗，有人把Minecraft中的NPC，通通換成AI的NPC，那就把相關的影片連結留在這個投影片上面。那根據這個影片連結的描述，就說這些AI很厲害，他們組織了自己的交易的金融體系，然後還組織了自己的政府，自己制定憲法自己管理自己，是真的還假的啦？這個是這個影片說的，剛才講的那些遊戲，你可能比較不容易接觸到，他對現實世界可能也沒什麼影響，那今天也許你馬上就會接觸到的AI agent，就是讓AI來真正使用電腦。", "label": 1}
{"text": "後續研究更進一步，以AI取代Minecraft中的所有NPC，相關影片連結於投影片中。影片顯示這些AI發展出複雜的交易、金融和政治體系，甚至制定了自己的憲法，其自主程度令人驚嘆（真實性有待商榷）。  比起先前較難接觸且對現實影響有限的遊戲AI，AI agent將直接且即時地運用於電腦操作。", "label": 0}
{"text": "雖然這個聽起來有點弔詭，AI本身也就是一個電腦，但他現在要來真正的像人類一樣，來使用另外一個比較低端的電腦來做事。那其中比較有代表性的例子，就是cloud的computer use，還有chain GPT的operator。那我們在上次上課的影片中，也已經跟大家講過operator，那operator介面長這樣，那他會建議可以做的事情，比如說可以訂pizza，可以預約下週的居家清潔等等。那像這種使用電腦的AI agent，他的目標就是你的輸入，就是你告訴他我要去訂pizza，你告訴他上網幫我買一個東西，那這就是他的目標。那他的observation呢？他的observation可能是，那個電腦的螢幕畫面，今天很多語言模型都是可以直接看圖的，所以其實可以直接把圖片當作輸入，可以直接把電腦畫面當作輸入，提供給AI agent。那AI agent要決定的就是，他要按鍵盤上哪一個鍵，或者是要按滑鼠的哪一個按鈕。", "label": 1}
{"text": "AI用較低階的電腦執行任務，看似矛盾，卻是事實。雲端運算和ChatGPT操作員便是實例。先前課程影片已示範操作員介面，其功能包含訂披薩、預約清潔服務等。此類AI代理的目標來自使用者輸入，例如訂披薩或線上購物指令。其觀察則來自電腦螢幕畫面，許多語言模型可直接讀取圖像，作為輸入資訊。AI代理需決定鍵盤或滑鼠操作。", "label": 0}
{"text": "那其實讓AI使用電腦啊，不是最近才開始有的野望，其實早在2017年，就有篇paper叫words of bits，嘗試過使用AI agent。你看他這個文章的標題，他把自己的文章標題說，他是一個web-based agent，那只是那個時候能夠互動的頁面，還是比較原始的頁面。", "label": 1}
{"text": "早在2017年，一篇名為\"Words of Bits\"的論文就已探索讓AI使用電腦的可能性，並嘗試運用AI代理程式。論文自稱是一個基於網頁的代理程式，但當時的互動介面相對簡陋。  AI操控電腦的概念並非新興事物。", "label": 0}
{"text": "你可以看到下面這些AI agent，他真正能夠處理的是比較原始的頁面，那個時候也沒有大型語言模型，所以那時候的方法，就是硬圈一個CNN。直接吃螢幕畫面當作輸入，輸出就是滑鼠要點的位置，或者是鍵盤要按的按鈕，看看用這個方法能不能夠讓AI agent在網路的世界中做事。這個是2017年，這甚至不能說是上古時代，以後有這個BERT以前的時代，就是史前時代，這個不只是史前時代，它史前時代比較早期，所以這是舊時期時代的產物。", "label": 1}
{"text": "早期AI代理，處理的是原始網頁，缺乏大型語言模型，當時只能依靠卷積神經網絡直接處理螢幕畫面，以此預測滑鼠點擊或鍵盤輸入，探究其在網路環境中的應用可能性。這套方法源於2017年，甚至算不上久遠，與BERT之前的時代相比，更顯得古老落後。", "label": 0}
{"text": "好，那後來有了語言模型之後啊，人們就開始嘗試用語言模型，來當作AI agent，來運行一個agent，讓它在網路的世界中活動。", "label": 1}
{"text": "於是，語言模型問世後，人們便著手將其應用於AI代理，讓其在網際網路環境中運作。", "label": 0}
{"text": "那這一頁投影片，是列舉了幾個比較具代表性的例子。那這一波潮流，大概是在2023年的暑假開始的，像Mine to Web、Web Arana，還有Visual Web Arana，就跟今天的operator非常的像，就是給這個語言模型，看一個螢幕的畫面，或者是看HTML的code，然後他自己決定他要幹什麼，期待他最後可以解決一個問題。", "label": 1}
{"text": "本頁簡述幾個典型案例說明這股始於2023年暑假的趨勢。Mine to Web、Web Arana和Visual Web Arana等，與今日的運算器功能類似，皆讓語言模型分析螢幕畫面或HTML程式碼，自行判斷操作以解決問題。", "label": 0}
{"text": "比如說在Mine to Web的第一個例子裡面，就給他這個畫面，然後跟他說，請他幫我們訂一個機票。那還有什麼樣AI agent的應用呢？今天你可以用AI來訓練另外一個AI模型，這就是等一下作業二，助教會跟大家講的事情。那用AI來訓練模型，那其實這個運作的過程就是你的目標就是要過strong baseline，然後你提供給LLM訓練資料，他寫一個程式用這些訓練資料來訓練模型。那他可能可以得到這個模型的正確率，根據正確率再重新寫一個程式，再得到新的正確率，就這樣一直運作下去。", "label": 1}
{"text": "在Mine to Web的第一個範例中，展示畫面並指示AI代理人協助預訂機票。AI代理人的其他應用包括利用AI訓練其他AI模型（作業二將詳細說明）。此訓練過程旨在超越既定基準，透過提供大型語言模型訓練資料，使其生成程式來訓練模型、評估準確性，並根據結果迭代改進程式碼，直到達到目標準確度。", "label": 0}
{"text": "那有很多知名的用AI來訓練模型的framework，比如說AIDE。那你看他的這個技術報告的這個標題，就知道他們想做什麼，他是要做一個machine learning engineer agent，他就是要用multi-agent的framework，來解data science的competition。", "label": 1}
{"text": "AI模型訓練框架眾多，例如AIDE。其技術報告標題清晰表明其目標：開發一個機器學習工程師代理，利用多代理框架參與數據科學競賽。", "label": 0}
{"text": "那在我們的作業中，你就會體驗到到底AI agent做不做得了，機器學習這門課的作業。", "label": 1}
{"text": "所以，這次機器學習作業會讓你實際感受AI agent的能力。", "label": 0}
{"text": "那最近呢，Google說他們做了一個AI，不過他們並沒有真的，釋出模型啦,所以你也不知道說，實際上做得怎麼樣,這個服務並不是公開的，那他們說他們做了一個AI Coscientist就是用AI來做研究。", "label": 1}
{"text": "Google近期宣佈研發出一款AI，但該模型尚未公開釋出，其實際效能因此不得而知。此AI，名為AI Coscientist，旨在協助科研工作，目前僅供內部使用。", "label": 0}
{"text": "不過這個AI Coscientist還是蠻有侷限的,他不能真的做實驗啦。他只能夠提Proposal，就是你把一些研究的想法告訴他，他把完整的Proposal規劃出來，實際上做得怎麼樣,不知道啦。那你要看他的Blog裡面有些比較誇張的案例,說什麼本來人類要花十年才能夠得到研究成果，AI agent花兩天就得到了,也不知道真的還假的。他舉的是一些生物學的例子,所以我也無法判斷，他講的是不是真的,那個發現是不是真的很重要。", "label": 1}
{"text": "這個AI科學家能力有限，無法實際進行實驗，只能撰寫研究提案。它能根據你的研究構想，制定完整的提案計畫，但實際執行結果不得而知。它部落格上有些案例宣稱能將十年研究縮短至兩天，但多為誇張的生物學範例，其真實性和重要性難以驗證。", "label": 0}
{"text": "這個co-scientist的話，就是這個用AI agent來幫研究人員做研究。好,那我們剛才講的AI agent，他的互動方式是侷限在回合制的互動，有一個observation,接下來執行action，但是在更真實的情境下，這個互動是需要及時的。因為外在的環境也許是不斷在改變的，如果你在action還沒有執行完的時候，外在環境就改變了，那應該要怎麼辦呢？有沒有辦法做到更即時的互動呢？更即時的互動可能應該像是這樣子，當模型在決定要執行action one，正在執行的過程中，突然外在環境變了。這個時候模型應該有辦法，立刻轉換行動，改變他的決策，以因應外界突如其來的變化。", "label": 1}
{"text": "合作科學家提出的方法，是利用AI代理協助研究人員進行研究。目前的AI代理互動方式為回合制，觀察後再執行動作，然而真實情境需要即時互動。由於外部環境不斷變化，若動作執行中環境改變，則需應對機制。因此，更即時的互動方式應為：模型在執行動作一過程中，若環境突變，模型能立即調整行動和決策，以適應環境變化。", "label": 0}
{"text": "你可以想說什麼樣的狀況，我們會需要用到這樣的AI agent，能夠做即時互動的呢？其實語音對話就需要這種互動的模式，文字的對話使用ChatGPT是大家比較熟悉的，你輸入一段文字，他就輸出一段文字，這是一來一往回合制的互動。但是人與人間真正的對話不是這樣子的，當兩個人在對話的時候，他們可能會互相打斷，或者是其中一個人在講話的時候，另外一個人可能會同時提供一些回饋。比如說嗯好你說的都對，那這些回饋可能沒有什麼，特別語意上的含義，他只是想要告訴對方我有在聽，但是像這樣子的回饋，對於流暢的交流來說，也是非常重要的。", "label": 1}
{"text": "什麼情況下我們需要一個能即時互動的AI代理？例如，語音對話就需要這種即時互動。文字對話，像ChatGPT，是回合制的，你一句我一句。但真實的人際對話並非如此，人們會互相打斷，或一方說話時，另一方會給予即時反饋，比如“嗯，對”、“好的”，這些看似無意義的回應，卻對流暢的交流至關重要。", "label": 0}
{"text": "如果在講電話的時候對方完全都沒有回應，你會懷疑他到底有沒有在聽？所以我們今天能不能夠讓AI，在跟使用者互動的時候，用語音互動的時候，就跟人與人間的互動一樣，而不是一來一往回合制的互動呢。其實也不是不可能的，今天GPT4O的一個Voice Mode 高級語音模式，也許在某種程度上，就做到了這一種即時的互動。", "label": 1}
{"text": "通話時對方無回應令人質疑其是否聆聽，那麼AI能否像人際互動般自然，而非僵硬的回合制對話呢？GPT-4的語音模式或已實現這類即時互動。", "label": 0}
{"text": "那這個投影片上是舉一個例子，假設有人跟AI說，你說一個故事，那這個是AI觀察到的第一個observation。有人叫他說一個故事，現在就開始講故事了，他就說從前從前那這時候人說了一個，好，這個可能是第二個observation，但AI要知道說這個observation，不需要改變他的行為，跟他的行為沒有直接的關係，只要故事就繼續講下去。有一個小鎮，然後人說這個不是我要聽的故事，這個我聽到了，那AI可能要馬上知道說，那這個不是人要聽的，那也許我覺得應該停下來，換另外一個故事。那今天AI有沒有辦法做到這種即時的互動呢？那怎麼做這種即時的互動，非回合制的互動，就有點超過我們這門課想要講的範圍。如果你有興趣的話，你可以讀這篇文章，那這篇文章想要做的事情，是評量現在這些語音模型互動的能力。那在這篇文章裡面，也對現有的這個可以做互動的語音模型，做了一個比較完整的survey，是一直survey到今年的1月。所以你可以看這篇文章，知道說現在這些可以互動的模型，他可以做到什麼樣的地步，那這是我們實驗室的林冠廷同學，跟他在這個Berkeley UW和MIT的合作夥伴一起做的文章。", "label": 1}
{"text": "本投影片以一個例子說明AI的互動能力：AI接收到「說個故事」的指令（第一個觀察），開始講故事「從前從前」（第二個觀察）。然而，第二個觀察並未影響AI的敘事行為。但若使用者表示不滿意故事內容，AI則需即時反應，例如停止現有故事並開始新的故事。  目前的AI能否做到這種即時、非回合制的互動？  本課程不深入探討此議題，但感興趣者可參考本實驗室林冠廷同學與柏克萊、華盛頓大學及麻省理工學院合作夥伴共同撰寫的文章，該文章評估現有語音模型的互動能力，並至今年一月對相關模型進行全面性調查，呈現現有互動模型的發展現況。", "label": 0}
{"text": "那這邊順便說明一下，以後這門課呢，我們投影片上引用論文的原則。論文的原則就是如果我找得到arXiv的連結的話，那我就把文章直接貼arXiv的連結，什麼是arXiv呢？假設你不是Computer Science背景的話，也許我就要解釋一下什麼是arXiv。arXiv的意思就是，一般呢，做研究你是寫完文章，投稿到一個期刊或者是國際會議，然後被接受以後才發表出來。但是對於AI的領域，因為變化實在太快，幾個月前就已經是古代了，所以期刊那種一審就要一年，或者是國際會議一兩個月，這種步調是沒有辦法，在不適用於AI的領域，所以現在一種習慣的發表方式。就是做出東西以後，直接放到一個公開的網站，叫做arXiv，然後就不審了立刻公開，然後你就可以讓全世界的人，看到你的文章。", "label": 1}
{"text": "本課程投影片引用論文的方式如下：如有arXiv連結，則直接提供連結。arXiv是公開預印本網站，因AI領域發展迅速，學者常將研究成果直接上傳arXiv公開分享，省去期刊或會議審查的冗長流程。", "label": 0}
{"text": "那有很多人會覺得引用arXiv的連結不夠正式，但是很多重要的文章，其實現在不見得投稿國際會議。但就只有arXiv的連結，所以我會選擇如果找得到arXiv的連結的話，就直接引用arXiv的連結，其實現在大家都在arXiv上看文章，那國際會議現在比較像是經典回顧這樣子，每篇文章我幾乎都在arXiv上看過了。句子說原來你投到這裡啊！這樣的感覺。", "label": 1}
{"text": "許多人認為arXiv連結不夠正式，然而許多重要論文尚未發表於國際會議。基於此，只要能找到arXiv連結，我便會直接引用；目前學界普遍使用arXiv平台閱覽論文，國際會議多半扮演論文回顧的角色，我幾乎所有論文皆透過arXiv取得，因此看到arXiv連結，我便知其論文來源。", "label": 0}
{"text": "那引用arXiv的連結，還有一個好處就是你可以直接從arXiv的連結看出這篇文章的時間。所以arXiv的連結裡面的數字，前面兩個就是年份，後面兩個就是月份，可以看這個數字就可以知道說這篇文章是在什麼時候被放在arXiv，也就是什麼時候被發表的，可以讓你對於每一個研究，他誕生的時間更有感覺。", "label": 1}
{"text": "arXiv連結的數字能直接顯示論文的年份和月份，方便了解論文的發表時間，有助於掌握研究的時序。", "label": 0}
{"text": "好那接下來呢，我們會分三個面向來剖析今天這些AI agent的關鍵能力。那第一個面向是我們要來看這些AI agent，這個AI agent能不能夠根據他的經驗，過去的互動中所獲得的經驗來調整他的行為。第二部分是要講這些AI agent如何呼叫外部的援助，如何使用工具。第三部分要講AI agent能不能夠執行計畫，能不能做計畫。那我們來講一下，AI怎麼根據過去的經驗，或者是環境的回饋來調整他的行為。", "label": 1}
{"text": "好的，我們將從三個方面評估這些AI代理的关键能力：首先，评估它们基于经验和过往互动调整行为的能力；其次，分析它们调用外部资源和工具的能力；最后，考察它们规划和执行计划的能力。接下来，让我们探讨AI如何根据过往经验或环境反馈调整其行为。", "label": 0}
{"text": "那AI呢，AI agent需要能夠根據經驗來調整行為，比如說有一個作為AI programmer的AI agent，他一開始接到一個任務，那他寫了一個程式，那這個程式compile以後有錯誤訊息，compile以後有error，那應該要怎麼辦呢？他應該要能夠根據這個error的message，來修正他之後寫的程式。", "label": 1}
{"text": "AI代理需具備基於經驗調整行為的能力。例如，一個AI程式設計師代理在編寫程式時，如果編譯出現錯誤訊息，它應能根據錯誤訊息修正後續程式碼。", "label": 0}
{"text": "那在過去啊，講到說你收到一個feedback接下來要做什麼的時候，也許多數機器學習的課程，都是告訴你來調整參數，根據這些收集到的訓練資料，也許使用reinforcement learning的algorithm來調整參數。", "label": 1}
{"text": "以往，討論收到回饋後的應對措施時，大多數機器學習課程都建議調整模型參數，利用收集到的訓練數據，或採用強化學習演算法來優化參數。", "label": 0}
{"text": "但不要忘了我們剛才就強調過，在這一堂課裡面沒有任何模型被訓練，所以我們今天不走這個路線。那不更新模型的參數，模型要怎麼改變它的行為呢？依照今天 Large Language Model 的能力，要改變它的行為，你也不用微調參數，直接把錯誤的訊息給他，他接下來寫的程式就會不一樣了，就結束了。那可能會問說，那之前他寫的程式是錯的，為什麼給錯誤訊息，他寫的程式就對了呢？明明就是同一個模型，但你想想看模型做的事情就是文字接龍，你給他不同的輸入，他接出來的東西就不一樣，一開始會寫錯的程式，是因為他前面要接的部分只有這麼多，所以寫個錯的程式。", "label": 1}
{"text": "我們先前已說明，本課程不涉及模型訓練。既然不調整模型參數，如何改變模型行為呢？大型語言模型的特性是，無需微調參數，直接提供錯誤訊息即可改變其後續程式碼輸出。您或許會疑問，為何提供錯誤訊息後程式碼就正確了？雖然是同一個模型，但它本質上是文字預測模型，不同的輸入會產生不同的輸出。初始程式碼錯誤，是因為輸入資訊不足，導致錯誤預測。", "label": 0}
{"text": "當今天要接的內容，包含了錯誤的訊息的時候，他接出來的結果可能就會是正確的了。那今天已經有太多的證據，說明這些語言模型，可以根據你給他的回饋，改變他的行為，不需要調整參數，那如果你有使用這些語言模型的經驗，你也不會懷疑他們有根據你的回饋調整行為的能力。", "label": 1}
{"text": "如果輸入資料有誤，模型產生的輸出反而可能是正確的。大量證據顯示，語言模型能透過使用者回饋調整行為，無需重新調整參數；有使用經驗者皆能理解此能力。", "label": 0}
{"text": "那這邊真正的議題是，如果我們是把過去所有的經驗都存起來，要改變語言模型的行為，要讓他根據過去的經驗調整行為。就是把過去所有發生的事情一股腦給他，那就好像是語言模型每次做一次決策的時候，他都要回憶他一生的經歷，也許在第100步的時候還行，到第1萬步的時候，過去的經驗太長了，他的人生的資訊已經太多了，也許他沒有足夠的算力，來回顧一生的資訊，他就沒有辦法得到正確的答案。", "label": 1}
{"text": "核心問題在於，若將所有過往經驗儲存並用於調整語言模型行為，模型需在每次決策時回憶全部經歷。這在早期或許可行，但隨著經驗累積，龐大的資訊量將超出模型的算力負荷，導致決策失準。", "label": 0}
{"text": "這讓我想到什麼呢，這讓我想到有一些人有超長自傳式記憶，他可以把他一生中，所有發生的事情記下來，然後那些人你可以隨便問他一個，某個人的電話號碼，他都會背出來，你告訴他某年某日某時發生了什麼事，他也都可以講出來。", "label": 1}
{"text": "這讓我想起某些人擁有驚人的自傳式記憶，能完整記住人生經歷的每個細節，例如任何人的電話號碼，或任何時間發生的任何事件，他們都能準確地回憶。", "label": 0}
{"text": "有一些人他的頭腦就像是一個影印機一樣，會把所有他看過的事情都原封不動的記憶下來，但這種超長自傳式記憶啊，又被叫做超憶症。你看到症這個字就知道說，人們覺得這是一種疾病，這聽起來記憶力很好是一種祝福，但實際上對這些患者而言，據說這種患者世界上可能不到100例。那這是一個2006年的時候，才被論文發表的一個症狀，那據說這些患者其實日常生活並沒有辦法過得很開心。因為他們不斷的在回憶他的人生，往往一不小心就陷入了一個冗長的回憶之中，那也很難做抽象的思考，因為他的人生已經被他的記憶已經被太多，知為末節的所事所佔據，所以沒有辦法做抽象式的思考。", "label": 1}
{"text": "某些人的記憶力驚人，能完整記住所有經歷，這被稱為超憶症，是一種罕見的疾病，全球案例可能不到百例。2006年才被學術界正式記載，患者並非如想像般幸福，反而因持續不斷的回憶而苦惱，難以抽離冗長的記憶迴圈，更難以進行抽象思考，因為瑣碎的細節佔據了他們的全部心智。", "label": 0}
{"text": "所以讓一個AI agent記住他一生所有經歷的事情，告訴他你每次做一個決策的時候，都是根據你一生所有經歷過的事情再去做決策，那也許對AI agent來說並不是一件好事。最終當他的人生過長的時候，他會沒有辦法做出正確的決策，所以怎麼辦呢？也許我們可以給這些AI agent memory，這就像是人類的長期記憶一樣，發生過的事情我們把它存到這個memory裡面。", "label": 1}
{"text": "讓AI持續儲存所有經歷，並以此為基礎做決策，可能並非最佳方案。  長期累積的記憶可能導致決策失誤。  因此，我們需要設計一種AI記憶機制，類似於人類的長期記憶，選擇性儲存重要經歷。", "label": 0}
{"text": "當AI agent看到，第一萬個observation的時候，他不是根據所有存在memory裡面的內容，去決定接下來要採取什麼action。而是有一個叫做read的模組，這個read的模組會從memory裡面，選擇跟現在要解決的問題有關係的經驗，把這些有關係的經驗放在observation的前面。讓模型根據這些有關係的經驗跟observation，再做文字接龍，接出它應該進行的行為，那你有這個read的模組就可以從memory裡面，從長期記憶中篩選出重要的訊息，讓模型只根據這些跟現在情境相關的訊息來進行決策。", "label": 1}
{"text": "AI代理在處理第一萬個觀察時，並非使用全部記憶體內容決定下一步行動，而是透過一個「讀取」模組從記憶體中篩選與當前問題相關的經驗，將這些經驗置於觀察數據之前，再以此作為基礎進行決策，如同文字接龍般，產生下一個行動。此「讀取」模組有效地從長期記憶中提取關鍵資訊，讓模型僅根據與當前情境相關的訊息來判斷。", "label": 0}
{"text": "那怎麼樣打造這個read的模組呢？其實你可以想這個read的模組，就想成是一個retrieval的system，想成是一個檢索的系統，那第一萬步看到的observation其實就是問題，那模型的AI agent的memory長期記憶，其實就是資料庫。那你就把拿這個檢索系統，根據這個問題從這個資料庫裡面，檢索出相關的資訊，那這整個技術跟RAG沒有什麼不同，其實它就是RAG，你可以直接把RAG的任何方法，直接套用到這個地方。", "label": 1}
{"text": "如何建構這個read模組？  將其視為一個檢索系統，輸入的觀察值即為查詢問題，模型的長期記憶體等同於資料庫。  系統根據問題從資料庫中提取相關資訊，其原理與RAG相同，所有RAG方法皆可直接套用。", "label": 0}
{"text": "唯一不一樣的地方只是，如果是RAG的話，存在memory裡面的東西等於是整個網路，那是別人的經驗。而對AI agent而言，現在存在memory裡面的東西，是他自己個人的經歷，差別的是經歷的來源，但是用來搜尋的技術，是可以完全直接線套RAG的技術。", "label": 1}
{"text": "AI agent與RAG的關鍵差異在於記憶體內容的來源：RAG使用的是全網數據，即別人的經驗；而AI agent則使用自身的經驗。儘管經驗來源不同，但兩者皆可沿用相同的搜尋技術。", "label": 0}
{"text": "呃,如果你今天想要研究這個AI agent按照經驗來修改他的行為,那你可以考慮一個叫做streambench的benchmark,那在streambench裡面呢,會有一系列的問題,然後呢,AI會依序去解這些問題,他先解第一個問題,得到第一個問題的答案,然後接下來他會得到第一個問題答案的反饋,那在這個streambench目前的，因為所有的問題都是有標準答案的,所以AI agent得到的回饋是binary的,就是對或者是錯。好,那根據他過去的經驗,他就可以修正他的行為，期待他在第二個問題的時候,可以得到更準確的答案，得到更高的正確率,然後這個過程就一直持續下去。那假設有1000個問題的話,那就等AI agent回答完最後問題的時候，這個互動就結束了，那最後結算一個，根據經驗學習能力的好壞，根據經驗調整行為能力的好壞，那就看這一整個回答的過程中平均的正確率，越能夠根據經驗學習的agent，他應該能夠用越少的時間，看過越少的回饋，就越快能夠增強他的能力，就可以得到比較高的平均的正確率。", "label": 1}
{"text": "想評估AI agent基於經驗調整行為的能力？Streambench是個不錯的基準測試，它包含一系列問題，AI依次解答。每個問題回答後，AI會收到正確與否的反饋（因標準答案存在，故為二元反饋）。此反饋幫助AI修正策略，力求提升後續問題的準確率。過程持續至所有問題（例如1000個）解答完畢。最終，平均準確率反映AI的經驗學習能力，學習能力越強的agent，應能在較短時間內，藉由較少反饋，達到更高的平均準確率。", "label": 0}
{"text": "那這個benchmark呢，是API的研究人員打造的一個benchmark。那在這個benchmark裡面的baseline，就是有使用到我剛才講的類似RAG的技術，也就是說當模型在回答第100個問題的時候，他並不是把前面第一個到第99個問題，通通丟給他去做文字接龍，這樣這個sequence太長了，一般的語言模型根本讀不了這麼長的輸入。", "label": 1}
{"text": "所以，這個基準測試是由API的研究人員開發的。其基準模型使用了類似RAG的技術，因為讓模型處理所有之前問題（從第一個到第九十九個）的文本串聯來回答第一百個問題，對於一般的語言模型來說，輸入序列過長而無法處理。", "label": 0}
{"text": "所以實際上的做法，就是你需要有一個檢索的模組。這個檢索的模組，只從過去所有的經驗中，檢索出跟現在要回答的問題有關係的經驗，然後語言模型只根據這些有關係的經驗還有現在的問題，來進行回答，來產生他的行動，來產生他的答案。那這一招有沒有用呢？這一招其實非常的有用。", "label": 1}
{"text": "核心機制是運用一個搜尋模組，從過往經驗中篩選與當前問題相關的資料，再由語言模型根據這些資料與問題生成答案及行動。此方法極其有效。", "label": 0}
{"text": "那在這一頁圖裡面橫軸啊，他這邊的用詞是time step，但其實指的就是一個一個的問題，總共有1750幾個問題。那縱軸指的是平均的正確率，那在這個圖上面呢，最低的這條灰色線指的是說，假設沒有讓模型做任何學習，他回答每一個問題都是independent的，回答問題間沒有任何的關聯，他完全沒有調整他的行為，那你得到的正確率是灰色的這條線是最低的。那黃色這條線是說，只固定隨機選五個問題，那每次模型回答問題的時候，都是固定看那五個問題來回答，都是固定把五個問題當作經驗來回答，那也可以得到的是黃色這一條線。", "label": 1}
{"text": "此圖橫軸代表時間步，即1750個問題。縱軸則為平均準確率。灰色線顯示模型未經訓練，每個問題獨立作答的最低準確率。黃色線則表示模型僅參考固定五個問題作答的準確率。", "label": 0}
{"text": "那如果你是用RAG的方法，從一個memory裡面去挑選出最有關係的問題，跟現在要解決的問題最有關係的經驗，那你可以得到的是粉紅色的這一條線。那可以看到比黃色的線，那正確率還要高上不少，那最後結果最好的是紅色這一條線啦。那這個怎麼做的，那大家就自己再去詳細閱讀論文，那在streambench裡面呢。", "label": 1}
{"text": "採用RAG方法，從記憶體中篩選出與當前問題最相關的經驗，即可獲得比僅依賴經驗（黃線）更準確的結果（粉紅線）。最佳結果（紅線）的具體實現細節，請參考Streambench論文。", "label": 0}
{"text": "還發現一個有趣的現象是值得跟大家分享，這個現象是負面的回饋。基本上沒有幫助對現階段的語言模型而言，所以你要提供給語言模型經驗，讓他能夠調整他行為的時候，給他正面的例子，比給他負面的例子要好。也就是說具體而言，提供給他過去哪些類似的問題得到正確答案，比提供給他過去哪些問題得到錯誤的答案，還更有效，還更能引導模型得到正確的答案。", "label": 1}
{"text": "值得注意的是，負面回饋對現階段的語言模型幫助不大。因此，為提升模型效能，提供正面範例，指導模型修正行為，比提供負面範例更有效，能更有效引導模型得出正確答案，也就是提供正確答案的案例比錯誤答案的案例更有幫助。", "label": 0}
{"text": "那這邊是真正的實驗結果，做在好幾個不同的data set上面，streambench裡面本來就包含了好幾個不同的data set。那這個縱軸呢，0代表完全沒有做，完全沒有根據經驗調整行為，然後藍色代表說，不管是正面還是負面的例子都用，如果不管正面還是負面的例子都用。在多數情況下，模型都可以表現得比較好，當然有一些例外，但是如果只用負面的例子呢？如果只用負面的例子，基本上是沒有幫助，而且甚至是有害的。", "label": 1}
{"text": "本研究在多個StreamBench內建資料集上驗證了實驗結果。縱軸0表示無任何經驗性調整，藍色曲線代表同時使用正負樣例，多數情況下模型效能提升，少數例外存在。單獨使用負樣例則無助益，甚至可能損害模型效能。", "label": 0}
{"text": "那如果說只用正面的例子，在所有的情況下，模型可以得到更好的結果，那這也符合過去的一些研究。有人研究過使用語言模型要怎麼樣比較有效，有一個發現就是與其告訴語言模型不要做什麼，不如告訴他要做什麼，如果你還希望他文章寫短一點，你要直接跟他說寫短一點，不要告訴他不要寫太長。比較他不要寫太長，他不一定聽得懂，叫他寫短一點，比較直接他反而比較聽得懂。", "label": 1}
{"text": "正向引導能提升模型表現，這與既有研究結果一致。研究顯示，明確指示模型「該做什麼」比禁止其「不該做什麼」更有效率，例如，直接要求模型「簡潔扼要」比避免其「冗長贅述」更易達成目標，因為後者容易造成模型理解上的偏差。", "label": 0}
{"text": "這也符合這邊這個Streambench的發現，就是負面的例子比較他沒有效，與其給語言模型告訴他什麼做錯，不如告訴他怎麼做是對的。", "label": 1}
{"text": "Streambench的研究也印證了這點：提供正面範例比指出錯誤更有效，引導模型學習正確的做法比糾正錯誤更佳。", "label": 0}
{"text": "好 那我們剛才講到了，有一個read的模組，那有關記憶的部分呢，是不是要把所有所有的資訊，通通存到memory裡面呢？存到長期的記憶庫裡面呢？如果我們把這些agent，經歷的所有的事情，都放到長期的記憶庫裡面的話，那裡面可能會充斥了一堆雞毛算皮不重要的小事，最終你的memory長期記憶庫可能也會被塞爆。", "label": 1}
{"text": "所以，我們前面提到了`read`模組。關於記憶，是否需要將所有資訊都儲存到長期記憶庫？如果將所有代理人的經歷都儲存，記憶庫可能會充滿無關緊要的細節，導致記憶庫容量耗盡。", "label": 0}
{"text": "如果說你是做那種AI村民啊，AI村民他多數時候觀察到的資訊都是些無關緊要的小事，那如果你看他觀察到那個log，多數都是啥事也沒有。就那邊有一張桌子啥事也沒有，那邊有一張椅子啥事也沒有，多數時候都是啥事也沒有，所以如果把所有觀察到的東西，都記下來的話，那你的memory裡面就都只是被一些雞毛算皮的小事佔據。所以怎麼辦呢？也許應該有更有效的方式，來決定什麼樣的資訊，應該被記下來，應該只要記重要的資訊就好。", "label": 1}
{"text": "AI村民通常只记录些琐碎的、无关紧要的日志，例如“那边有张桌子”、“那边有张椅子”，大部分日志都毫无价值。  因此，如果记录所有观察到的信息，内存很快就会被无用的细节塞满。  所以，我们需要更有效的筛选机制，只记录重要的信息。", "label": 0}
{"text": "那怎麼讓語言模型只記重要的資訊就好呢？你可以有一個write的module，那write的module決定，什麼樣的資訊要被填到長期的記憶庫裡面，什麼樣的資訊乾脆直接就讓他隨風而去就好了。", "label": 1}
{"text": "如何讓語言模型有效篩選資訊？可設計一個寫入模組，該模組判斷哪些資訊儲存於長期記憶庫，哪些資訊則予以捨棄。", "label": 0}
{"text": "那怎麼樣打造這個write的記憶庫呢？有一個很簡單的方法就是，write的模組也是一個語言模型，甚至就是AI agent自己。這個AI agent他要做的事情，就是根據他現在觀察到的東西，然後問自問一個問題，這件事有重要到應該被記下來嗎？如果有就把它記下來，如果沒有就讓他隨風而去。", "label": 1}
{"text": "如何建立write的記憶庫？  一個方法是將write模組視為語言模型或AI代理。此代理根據觀察結果，自行判斷資訊重要性，決定是否儲存。", "label": 0}
{"text": "那除了RE跟Write這兩個模組以外，還有第三個模組。沒有固定的名字啦，在文件上的名字，沒有固定的名字，我們可以暫時叫他reflection反思的模組。", "label": 1}
{"text": "除了RE和Write模組，還存在另一個模組，暫且稱之為「reflection」模組，因為它在文件裡沒有固定的名稱。", "label": 0}
{"text": "那這個模組的工作是，對記憶中的資訊做更好的，更high level的，可能是抽象的重新整理，你可以把這些記憶裡面的內容，在經過reflection的模組，重新反思之後得到新的想法。那也許read的模組可以根據這些新的想法，來進行搜尋，這樣子也許可以得到更好的經驗，那幫助模型做出更好的決策。", "label": 1}
{"text": "此模組的功能在於提升記憶資訊的處理效率，透過高階、抽象的重新組織，並藉由反思模組重新審視記憶內容，產生新的洞見。這些洞見可指導閱讀模組進行更有效的資訊搜尋，進而累積更優質的經驗，輔助模型做出更佳的決策。", "label": 0}
{"text": "而這個reflection的模組，可能也是一個語言模型，就是AI agent自己，你可以只是把過去的這一些記憶，丟給reflection的模組，然後叫reflection模組想一想，看他從這些記憶裡面，能不能夠有什麼樣新的發現。比如說可能有一個observation是，我喜歡的疫情每天都跟我搭同一部公車，另外observation是他今天對我笑了，那你推出來的reflection模型，結果就說他喜歡我這樣，一個錯覺，人生三大錯覺之一就是這一種。就得到一些新的sort，你就得到一些新的想法，那你之後在做決策的時候，就可以用這些新的想法，雖然你沒有實際觀察到，但它是被推論出來的，根據這些推論出來的想法來做決策。那除了產生新的想法之外，也可以為以前觀察到的經驗，建立經驗和經驗之間的關係，也就是建立一個，然後讓reader的module根據這個knowledge graph來找相關的資訊。", "label": 1}
{"text": "此反射模組，或許本身也是個語言模型，亦即AI代理自身。你只需將過往記憶輸入此模組，讓它自行分析，從中找出新的見解。例如，輸入「每天與某人搭乘同一班公車，且今日對方朝我微笑」等資訊，模組可能推論出「對方喜歡我」的結論，指出這只是錯覺。如此，便能產生新的推論與想法，作為日後決策的依據，即使這些想法並非直接觀察所得。此外，此模組亦能建立經驗間的關聯，建構知識圖譜，供後續模組搜尋相關資訊。", "label": 0}
{"text": "那我知道在RAG的領域使用knowledge graph，現在也是一個非常常見的手法，那最知名的可能就是graph RAG系列這個研究。就把你的資料庫，把它變成一個knowledge graph，那今天在搜尋跟回答問題的時候，是根據knowledge graph來搜尋回答問題，可以讓RAG這件事做得更有效率。", "label": 1}
{"text": "運用知識圖譜於RAG已成常見做法，Graph RAG系列研究最具代表性。將資料庫轉換為知識圖譜，藉此搜尋並回答問題，可提升RAG效率。", "label": 0}
{"text": "或是另外一個非常類似的例子，那HIPO RAG，這個HIPO不是指真正的河馬。他指的應該是那個海馬迴，那個人腦中的一個結構，然後他覺得做建這種knowledge graph，就跟海馬迴的運作呢，非常的類似，所以他叫做HIPO RAG。", "label": 1}
{"text": "HIPO RAG模型名稱中的HIPO並非指河馬，而是指海馬迴，大腦中負責記憶的結構。設計者認為此知識圖譜的運作機制與海馬迴相似，故以此命名。", "label": 0}
{"text": "有一些跟graph有關的RAG的方法，那你完全可以透過reflection的模組，把經驗建成一個graph以後，把那一些graph RAG的手法，直接套到AI agent裡面。那大家可能都知道說這個ChatGPT啊，現在其實真的是有記憶的，所以可以感受到這個OpenAI，想把ChatGPT變成一個AI agent的決心。", "label": 1}
{"text": "藉由反射模組將經驗轉化為圖形，許多基於圖形的 RAG 方法便能直接應用於 AI 代理中。ChatGPT 現已具備記憶功能，足見 OpenAI 將其發展為 AI 代理的決心。", "label": 0}
{"text": "比如說我跟ChatGPT說，我週五下午要上機器學習這門課，那他就給我一個回答，說要我幫助你做什麼事情嗎？接下來我告訴他記下來，你跟他講記下來之後，他的這個write的模組就啟動了。他知道這件事情是要被記下來的，他就會說那我記下來了，以後你週五要上機器學習這門課。那write的模組什麼時候要啟動，是他自己決定的，所以很多時候你希望他記下來的時候，他就是不啟動，或你不希望他啟動的時候，他就是啟動。那個是模型自己決定的，但是有一個方法可以，基本上一定能讓他啟動，就明確的跟他講，把這件事記下來，基本上都幾乎確定能夠啟動那個write的模組，讓write的模組把這件事情記下來。", "label": 1}
{"text": "我週五下午有機器學習課程。  請記錄下來。", "label": 0}
{"text": "那接下來的東西在哪裡呢？你可以看在設定裡面，有一個個人化，然後有一個叫記憶的部分，那你點這個管理記憶，就可以看到確記憶。他透過write的模組，寫在他的memory裡面，這個就是他作為一個AI agent的長期記憶，裡面的東西，比如第一條是你叫做血輪眼卡卡，有一次不小心跟他說你是卡卡，不知道為什麼他就覺得自己是血輪眼卡卡。", "label": 1}
{"text": "設定裡的個人化選項中，有個記憶管理功能，點進去就能查看AI儲存的長期記憶，這些記憶使用write模組記錄。例如，第一筆記錄是關於你曾自稱血輪眼卡卡，導致AI誤認自身為血輪眼卡卡。  那麼，其他的資料位於何處呢？", "label": 0}
{"text": "然後呢，他也記得就我剛才跟他講的，週五下午要上機器學習這門課。但是呢，但是其實模型的記憶也是會出錯的。因為要寫什麼樣的東西到記憶裡面，是模型自己決定的，而且他並不是把對話的內容，就一五一十的直接放到記憶裡面，他是經過一些昇華反思之後才放進去的，所以他的反思可能會出錯。", "label": 1}
{"text": "他記住了我之前說的週五下午有機器學習課。然而，模型的記憶並不完美，因為模型自行決定儲存哪些資訊，且並非直接儲存對話內容，而是經過加工和反思後才儲存，故此反思過程可能存在錯誤。", "label": 0}
{"text": "比如說他覺得我是一個臺灣大學的學生，雖然我是老師。但是他從過去的對話誤以為我是一個學生，所以就存了一個錯誤的資訊。在他的記憶裡面，一堆他想記的東西，比如說我給過什麼演講，給過什麼tutorial，他都把它記下來就是了。", "label": 1}
{"text": "他把我誤認為台大學生，其實我是老師。之前的談話造成他的誤解，所以記錯了我的身份。他習慣記錄許多資訊，例如我的演講和教學活動，都包含在他的筆記裡。", "label": 0}
{"text": "那這些有記憶的確GPT，他可以使用他的記憶，比如說我跟他說禮拜五下午是去玩好嗎？這個時候記憶模組就被啟動了，但是他是怎麼被啟動的，其實就不太清楚了。他到底是把所有記憶的內容，通通都放到這個問題的前面，直接讓模型做回答，還是說也有做RAG，只選擇下載相關的記憶內容呢？那這個我們就不得而知了。", "label": 1}
{"text": "這些具備記憶功能的GPT模型，能運用其儲存的資訊，例如我詢問它「禮拜五下午去玩好嗎？」便會觸發記憶模組，但其啟動機制並不透明。  我們無法確定它是直接將所有記憶與問題一起輸入模型，還是僅擷取相關記憶片段進行回覆，其內部運作仍屬未知。", "label": 0}
{"text": "總之當我問他，週五下午出去玩好嗎？這個read的模組就啟動了。他就說，下午不是要上課嗎？怎麼能夠出去玩，好聰明啊！他知道下午要上課，挺厲害的，然後問他你是誰，剛才我說過，他是血輪眼卡卡，所以他就覺得，之前是血淪眼卡卡。如果你想要知道，更多有關AI Agent記憶的研究的話，那這邊就是放了幾篇經典的論文給大家參考，包括Memory GPT，這是23年的論文，Agent Workflow Memory是24年的論文，還有一個最近的Agent Memory，Agent是25年的論文，所以23到25年各引用一篇，告訴你說這方面的研究是持續不斷的。", "label": 1}
{"text": "我提議週五下午出去玩，他立刻反應：「下午要上課啊！」反應之迅速讓我驚訝，真不愧是「血輪眼卡卡」！他甚至記得我之前稱呼他為「血輪眼卡卡」。想深入了解AI Agent記憶的研究？推薦幾篇論文：2023年的Memory GPT、2024年的Agent Workflow Memory和2025年的Agent Memory，展現這領域持續發展的趨勢。", "label": 0}
{"text": "接下來呢，我們要跟大家講，現在這些語言模型怎麼使用工具。那什麼叫做工具呢？但語言模型本身對我們人類來說也是工具。", "label": 1}
{"text": "現在我們將說明大型語言模型如何運用工具，所謂工具為何？而語言模型本身也屬於人類使用的工具。", "label": 0}
{"text": "那對語言模型來說，什麼東西又是他的工具呢？所謂的工具就是這個東西啊，你只要知道怎麼使用他就好，他內部在想什麼，他內部怎麼運作的，你完全不用管。這就是為什麼肥宅如果一直幫另外一個人修電腦的話，就會被叫做工具人，因為別人沒有人在意肥宅的心思，只知道他能不能夠修電腦而已，所以這個就是工具的意思。", "label": 1}
{"text": "對語言模型而言，工具只是能被使用的東西，其內部機制無須理解。就像幫人修電腦的肥宅，只因其可用性而被視為工具人，而非其內心想法。", "label": 0}
{"text": "那有哪些語言模型常用的工具呢？最常用的就是，就是搜尋引擎,然後呢,語言模型現在會寫程式,而且可以執行他自己寫的程式,那這些程式也算是某種工具,甚至另外一個AI也可以當作是某一個AI的工具,有不同的AI,有不同的能力,比如說現在的語言模型,如果他只能夠讀文字的話,那也許可以呼叫其他看得懂圖片,聽得懂聲音的AI,來幫他處理多模態的問題,或者是說,或者是不同模型它的能力本來就不一樣。也許平常是小的模型在跟人互動，但小的模型發現它自己解不了的問題的時候，它可以叫一個大哥出來。大哥是個大的模型，那大的模型運作起來就比較耗費算力，所以大的模型不能常常出現。大的模型要在小的模型召喚它的時候，才出面回答問題，大哥要偶爾才出來幫小弟解決事情。", "label": 1}
{"text": "語言模型常用的工具包括搜尋引擎，以及其自身編寫並執行的程式碼。此外，一個AI也可以利用其他AI的能力，例如文字模型可以借助圖像或語音識別AI處理多模態任務。不同模型能力各異，小型模型通常負責與使用者互動，遇到複雜問題時則會呼叫運算成本較高的大型模型協助。", "label": 0}
{"text": "那其實這些工具對語言模型來說，都是function都是一個函式，當我們說語言模型在使用某一個工具的時候，其實意思就是它在調用這些函式。它不需要知道這些函式內部是怎麼運作的，它只需要知道這些函式怎麼給它輸入，這些函式會給什麼樣的輸出，那因為使用工具就是調用函式，所以使用工具又叫做function code，所以有一陣子很多語言模型都說，他們加上了function code的功能，其實意思就這些語言模型都有了使用工具的功能。", "label": 1}
{"text": "語言模型利用工具的機制，本質上是呼叫函數。模型無需了解函數內部運作，只需掌握輸入輸出規範。因此，\"使用工具\"與\"函數編碼\" (function code) 等同，許多語言模型宣稱新增此功能，即代表其已具備工具使用能力。", "label": 0}
{"text": "好那語言模型怎麼使用工具呢？等一下我會講一個通用的使用工具的方法，但實際上使用工具的方法很多，甚至有一些模型是專門針對來練習，他就訓練來使用工具的，那他如果是針對使用工具這件事做訓練，那他在使用工具的時候，你可能需要用特定的格式才能夠驅動他。那那個就不是我們今天討論的問題，或者是假設你有使用，使用這個OpenAIChat GPT的API的話，你會知道使用工具這件事情，是要放在一個特殊的欄位，所以對OpenAI來說，它的模型在使用工具的時候，也有一些特殊的用法。", "label": 1}
{"text": "那麼，大型語言模型如何運用工具呢？稍後我會說明一種通用的工具使用方式，但實際上方法眾多，有些模型甚至專門訓練來使用工具。針對工具使用的訓練模型，可能需要特定格式才能啟動。這不在本次討論範圍內，或者，如果您使用OpenAI ChatGPT API，會發現工具使用需要放置在特定欄位，OpenAI模型的工具使用方式也有一些特殊之處。", "label": 0}
{"text": "但我這邊講的是一個最通用的用法，對所有的模型，今天能力比較強的模型，應該都可以使用。好,什麼樣通用的方法，可以讓模型使用工具呢？就是直接跟他講啊！就告訴他怎麼使用工具，你就交代他可以使用工具，那你就把使用工具的指令，放在兩個Tool符號的中間，使用完工具後你會得到輸出，輸出放在兩個Output符號的中間。所以他就知道工具使用的方式了。接下來告訴他有哪一些可以用的工具，有一個函式叫做Temperature，他可以查某個地點某個時間的溫度，他的輸入就是地點跟時間，給他的使用範例，Temperature括號臺北某一段時間，他就會告訴你臺北在這個時間的氣溫。接下來你就把你的問題，連同前面這些工具使用的方式，當作Prompt一起輸入給語言模型，然後他如果需要用工具的話，他就會給你一個使用工具的指令。那前面這些教模型怎麼使用工具的這些敘述，他叫做System Prompt，那查詢使用調用這些工具的這些，這段話,某年某月某日高雄氣溫如何,這個是User Prompt,那如果你有在使用這個ChatGPT的API的話,你知道你的輸入要分成System Prompt跟User Prompt,那很多同學會搞不清楚System Prompt跟User Prompt有什麼樣的差別,那System Prompt指的是說,你在開發應用的這個Developer下的這個Prompt,這個Prompt呢,是每次都是一樣的,每次你都想要放在語言模型最前面,讓他去做文字接龍的這個敘述叫做System Prompt。", "label": 1}
{"text": "此方法適用於大多數現有強大的語言模型。  其核心是直接指示模型如何使用工具：將工具使用方法置於`Tool`標記內，輸出則置於`Output`標記內。  例如，`Temperature`函數可查詢特定時間地點的溫度，其輸入為地點和時間（例如：`Temperature(\"臺北\", \"2024-10-27\")`）。  將工具使用方法及你的問題（User Prompt）與模型使用方法說明（System Prompt）一起輸入模型。模型需使用工具時，會產生對應的工具使用指令。 System Prompt 指的是開發者每次都提供給模型的固定指令，例如，文字接龍的規則；User Prompt 則是你每次提問的內容，例如，「2024年10月27日高雄氣溫如何」。  使用ChatGPT API的使用者應熟悉System Prompt 和 User Prompt 的區別。", "label": 0}
