# ppl_evaluator.py

import json
import torch
import math
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

# ==== è¨­å®šå€ ====
MODEL_NAME = "./ppo_checkpoints/checkpoint_step300"  # å¯æ”¹æˆä½  fine-tuned çš„æ¨¡å‹è·¯å¾‘
LEE_FILE = "label1.jsonl"            # æå®æ¯…èªæ–™è³‡æ–™é›†
DATAS_FILE = "label0.jsonl"       # éæå®æ¯…èªæ–™è³‡æ–™é›†
MAX_LEN = 512                    # æœ€é•·è¼¸å…¥é•·åº¦ï¼ˆtokensï¼‰
USE_CUDA = torch.cuda.is_available()

# ==== æ¨¡å‹è¼‰å…¥ ====
device = torch.device("cuda" if USE_CUDA else "cpu")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)
model.eval()

# ==== Perplexity è¨ˆç®—å‡½æ•¸ ====
def compute_perplexity(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_LEN)
    input_ids = inputs["input_ids"].to(device)

    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss

    return math.exp(loss.item())

# ==== è³‡æ–™è¼‰å…¥ ====
def load_sentences(path):
    sentences = []
    if path.endswith(".json"):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            for item in data:
                if isinstance(item, dict):
                    sentences.append(item.get("text", "").strip())
                elif isinstance(item, str):
                    sentences.append(item.strip())
    elif path.endswith(".jsonl"):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                obj = json.loads(line)
                sentences.append(obj.get("text", "").strip())
    return [s for s in sentences if s]

# ==== è©•ä¼°æµç¨‹ ====
def evaluate_ppl_on_dataset(label, file_path):
    print(f"\nğŸ” é–‹å§‹è©•ä¼° {label} èªæ–™çš„ Perplexity ...")
    sentences = load_sentences(file_path)
    scores = []

    for sentence in tqdm(sentences, desc=f"{label}"):
        try:
            ppl = compute_perplexity(sentence)
            scores.append(ppl)
        except Exception as e:
            print(f"è·³éå¤±æ•—å¥å­ï¼š{sentence[:30]}... -> {e}")
    
    scores = np.array(scores)
    print(f"\nğŸ“Š [{label}] çµ±è¨ˆçµæœï¼š")
    print(f" - æ•¸é‡ï¼š{len(scores)}")
    print(f" - å¹³å‡ Perplexityï¼š{scores.mean():.2f}")
    print(f" - æ¨™æº–å·®ï¼š{scores.std():.2f}")

    return scores

# ==== ä¸»ç¨‹å¼ ====
if __name__ == "__main__":
    scores_lee = evaluate_ppl_on_dataset("æå®æ¯…", LEE_FILE)
    scores_datas = evaluate_ppl_on_dataset("å…¶ä»–è³‡æ–™", DATAS_FILE)
